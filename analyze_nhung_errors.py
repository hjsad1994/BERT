"""
PhÃ¢n tÃ­ch cÃ¡c errors cÃ³ tá»« "nhÆ°ng" (adversative conjunction)
TÃ¬m patterns vÃ  Ä‘á» xuáº¥t giáº£i phÃ¡p
"""

import sys
import io
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

import pandas as pd
import os

def analyze_nhung_errors():
    # Change to BERT directory
    os.chdir('D:/BERT')
    
    print("="*80)
    print("ğŸ” PHÃ‚N TÃCH ERRORS CÃ“ Tá»ª 'NHÆ¯NG' (Adversative Conjunction)")
    print("="*80)
    
    # Load all errors
    errors_file = "error_analysis_results/all_errors_detailed.csv"
    
    if not os.path.exists(errors_file):
        print(f"âŒ File khÃ´ng tá»“n táº¡i: {errors_file}")
        print("Vui lÃ²ng cháº¡y: python tests/error_analysis.py")
        return
    
    df = pd.read_csv(errors_file, encoding='utf-8-sig')
    
    print(f"\nğŸ“Š Tá»•ng sá»‘ errors: {len(df)}")
    
    # Find errors with "nhÆ°ng"
    nhung_errors = df[df['sentence'].str.contains('nhÆ°ng', case=False, na=False)]
    
    print(f"ğŸ“Š Sá»‘ errors cÃ³ tá»« 'nhÆ°ng': {len(nhung_errors)} ({len(nhung_errors)/len(df)*100:.1f}%)")
    
    if len(nhung_errors) == 0:
        print("\nâœ“ KhÃ´ng cÃ³ errors nÃ o chá»©a tá»« 'nhÆ°ng'")
        return
    
    # Analyze confusion patterns
    print(f"\n{'='*80}")
    print("ğŸ“ˆ CONFUSION PATTERNS CHO ERRORS CÃ“ 'NHÆ¯NG':")
    print(f"{'='*80}")
    
    confusion_stats = nhung_errors.groupby('confusion_type').size().reset_index(name='count')
    confusion_stats = confusion_stats.sort_values('count', ascending=False)
    
    for _, row in confusion_stats.iterrows():
        pct = row['count'] / len(nhung_errors) * 100
        print(f"   â€¢ {row['confusion_type']:<30} {row['count']:>3} cases ({pct:>5.1f}%)")
    
    # Analyze by aspect
    print(f"\n{'='*80}")
    print("ğŸ“ˆ ERRORS CÃ“ 'NHÆ¯NG' BY ASPECT:")
    print(f"{'='*80}")
    
    aspect_stats = nhung_errors.groupby('aspect').size().reset_index(name='count')
    aspect_stats = aspect_stats.sort_values('count', ascending=False)
    
    for _, row in aspect_stats.iterrows():
        pct = row['count'] / len(nhung_errors) * 100
        print(f"   â€¢ {row['aspect']:<20} {row['count']:>3} cases ({pct:>5.1f}%)")
    
    # Show examples
    print(f"\n{'='*80}")
    print("ğŸ“ TOP 10 ERRORS CÃ“ 'NHÆ¯NG' (Examples):")
    print(f"{'='*80}\n")
    
    for i, row in nhung_errors.head(10).iterrows():
        # Highlight "nhÆ°ng" in sentence
        sentence = row['sentence']
        # Find position of "nhÆ°ng"
        import re
        nhung_match = re.search(r'nhÆ°ng', sentence, re.IGNORECASE)
        if nhung_match:
            pos = nhung_match.start()
            # Show context around "nhÆ°ng"
            start = max(0, pos - 40)
            end = min(len(sentence), pos + 60)
            context = sentence[start:end]
            if start > 0:
                context = "..." + context
            if end < len(sentence):
                context = context + "..."
        else:
            context = sentence[:100] + "..." if len(sentence) > 100 else sentence
        
        print(f"{i+1}. Aspect: {row['aspect']}")
        print(f"   True: {row['sentiment']:>8} â†’ Predicted: {row['predicted_sentiment']:<8}")
        print(f"   Text: {context}")
        print()
    
    # Save nhung errors to separate file
    output_file = "error_analysis_results/nhung_errors_detailed.csv"
    nhung_errors.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"âœ“ Saved {len(nhung_errors)} errors cÃ³ 'nhÆ°ng' to: {output_file}")
    
    # Also check all data (not just errors)
    print(f"\n{'='*80}")
    print("ğŸ“Š PHÃ‚N TÃCH TOÃ€N Bá»˜ DATASET (bao gá»“m cáº£ Ä‘Ãºng + sai):")
    print(f"{'='*80}")
    
    # Load test set
    test_file = "data/test.csv"
    pred_file = "test_predictions.csv"
    
    if os.path.exists(test_file) and os.path.exists(pred_file):
        test_df = pd.read_csv(test_file, encoding='utf-8-sig')
        pred_df = pd.read_csv(pred_file, encoding='utf-8-sig')
        
        # Merge
        full_df = test_df.copy()
        full_df['predicted_sentiment'] = pred_df['predicted_sentiment']
        full_df['correct'] = full_df['sentiment'] == full_df['predicted_sentiment']
        
        # Find all "nhÆ°ng" sentences
        nhung_all = full_df[full_df['sentence'].str.contains('nhÆ°ng', case=False, na=False)]
        
        print(f"\nğŸ“Š Tá»•ng sá»‘ samples cÃ³ 'nhÆ°ng' trong test set: {len(nhung_all)}")
        print(f"ğŸ“Š Sá»‘ samples Ä‘Ãºng: {nhung_all['correct'].sum()} ({nhung_all['correct'].sum()/len(nhung_all)*100:.1f}%)")
        print(f"ğŸ“Š Sá»‘ samples sai: {(~nhung_all['correct']).sum()} ({(~nhung_all['correct']).sum()/len(nhung_all)*100:.1f}%)")
        
        print(f"\nğŸ’¡ So sÃ¡nh vá»›i overall performance:")
        overall_acc = full_df['correct'].mean()
        nhung_acc = nhung_all['correct'].mean()
        print(f"   â€¢ Overall accuracy: {overall_acc:.2%}")
        print(f"   â€¢ Accuracy on 'nhÆ°ng' sentences: {nhung_acc:.2%}")
        print(f"   â€¢ Difference: {(nhung_acc - overall_acc)*100:+.2f}%")
        
        if nhung_acc < overall_acc:
            print(f"\nâš ï¸  Model performs WORSE on sentences with 'nhÆ°ng'!")
        else:
            print(f"\nâœ“ Model performs OK on sentences with 'nhÆ°ng'")
    
    # Generate solutions
    print(f"\n{'='*80}")
    print("ğŸ’¡ GIáº¢I PHÃP Äá»€ XUáº¤T:")
    print(f"{'='*80}\n")
    
    solutions = [
        "1. DATA AUGMENTATION vá»›i tá»« chuyá»ƒn Ã½:",
        "   â€¢ Táº¡o thÃªm samples cÃ³ 'nhÆ°ng', 'tuy nhiÃªn', 'máº·c dÃ¹', 'song'",
        "   â€¢ Oversampling cÃ¡c samples cÃ³ tá»« chuyá»ƒn Ã½ bá»‹ sai",
        "   â€¢ Synthetic data: Ä‘áº£o ngÆ°á»£c cÃ¢u cÃ³ 'nhÆ°ng' Ä‘á»ƒ táº¡o thÃªm data",
        "",
        "2. FEATURE ENGINEERING:",
        "   â€¢ ThÃªm special token [ADV] trÆ°á»›c 'nhÆ°ng' khi tokenize",
        "   â€¢ VÃ­ dá»¥: 'pin tá»‘t nhÆ°ng camera xáº¥u' â†’ 'pin tá»‘t [ADV] nhÆ°ng camera xáº¥u'",
        "   â€¢ Model sáº½ há»c Ä‘Æ°á»£c ráº±ng [ADV] lÃ  signal quan trá»ng",
        "",
        "3. ATTENTION MECHANISM:",
        "   â€¢ Fine-tune thÃªm epochs vá»›i focus vÃ o adversative conjunctions",
        "   â€¢ TÄƒng weight cho tokens xung quanh 'nhÆ°ng' trong loss function",
        "",
        "4. RULE-BASED POST-PROCESSING:",
        "   â€¢ Detect cÃ¢u cÃ³ 'nhÆ°ng' â†’ split thÃ nh 2 pháº§n",
        "   â€¢ Pháº§n SAU 'nhÆ°ng' thÆ°á»ng quan trá»ng hÆ¡n",
        "   â€¢ VÃ­ dá»¥: 'Pin tá»‘t nhÆ°ng camera tá»‡' â†’ Focus vÃ o 'camera tá»‡'",
        "",
        "5. ENSEMBLE METHOD:",
        "   â€¢ Train model riÃªng cho sentences cÃ³ adversative conjunctions",
        "   â€¢ Combine predictions vá»›i main model",
        "",
        "6. CONTEXT WINDOW:",
        "   â€¢ TÄƒng max_length Ä‘á»ƒ model tháº¥y Ä‘Æ°á»£c full context",
        "   â€¢ Hiá»‡n táº¡i: 256 tokens â†’ cÃ³ thá»ƒ tÄƒng lÃªn 384",
        "",
        "7. HARD NEGATIVE MINING:",
        "   â€¢ Táº­p trung train láº¡i trÃªn nhá»¯ng samples cÃ³ 'nhÆ°ng' bá»‹ sai",
        "   â€¢ TÄƒng weight cá»§a nhá»¯ng samples nÃ y trong training",
        "",
        "8. PREPROCESSING:",
        "   â€¢ TÃ¡ch cÃ¢u phá»©c thÃ nh cÃ¢u Ä‘Æ¡n táº¡i vá»‹ trÃ­ 'nhÆ°ng'",
        "   â€¢ Má»—i pháº§n Ä‘Æ°á»£c phÃ¢n tÃ­ch riÃªng",
        "   â€¢ VÃ­ dá»¥: 'Pin tá»‘t nhÆ°ng camera tá»‡' â†’",
        "     - 'Pin tá»‘t' (positive)",
        "     - 'camera tá»‡' (negative)",
    ]
    
    for solution in solutions:
        print(solution)
    
    print(f"\n{'='*80}")
    print("ğŸ¯ HÃ€NH Äá»˜NG Äá»€ XUáº¤T NGAY:")
    print(f"{'='*80}\n")
    
    print("OPTION 1: DATA AUGMENTATION (Dá»… nháº¥t, hiá»‡u quáº£ cao)")
    print("   â†’ Oversampling cÃ¡c errors cÃ³ 'nhÆ°ng' trong training data")
    print("   â†’ Táº¡o script Ä‘á»ƒ duplicate vÃ  augment nhá»¯ng samples nÃ y")
    print()
    print("OPTION 2: SPECIAL TOKEN (Cáº§n retrain)")
    print("   â†’ ThÃªm [ADV] token vÃ o tokenizer")
    print("   â†’ Retrain model vá»›i special token nÃ y")
    print()
    print("OPTION 3: RULE-BASED (KhÃ´ng cáº§n retrain)")
    print("   â†’ Post-processing: split cÃ¢u táº¡i 'nhÆ°ng'")
    print("   â†’ PhÃ¢n tÃ­ch má»—i pháº§n riÃªng, Æ°u tiÃªn pháº§n SAU 'nhÆ°ng'")
    print()
    
    print(f"{'='*80}\n")


if __name__ == '__main__':
    analyze_nhung_errors()
