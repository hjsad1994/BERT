# Focal Loss + Contrastive Learning Analysis

## üéØ Why Current Result is 95.44% (Not Improving)

### Current Approach (Contrastive Only):
```
Loss = 0.3 √ó Classification + 0.7 √ó Contrastive
     = 0.3 √ó 0.0764 + 0.7 √ó 0.5482
     = 0.023 + 0.384 = 0.407
```

**Problem:**
- Classification loss = 0.0764 (VERY LOW - already saturated!)
- Focus 70% on contrastive, only 30% on classification
- Classification already good ‚Üí no more improvement possible

---

## ‚úÖ Solution: Add Focal Loss

### New Approach (Focal + Contrastive):
```
Loss = 0.7 √ó Focal + 0.3 √ó Contrastive
```

**Why Focal Loss:**
- **Focal Loss ‚â† Cross-Entropy**
- Focuses on HARD examples (misclassified samples)
- Better handles class imbalance
- Expected focal loss: 0.2-0.4 (vs 0.076 for CE)

---

## üìä Comparison: Cross-Entropy vs Focal Loss

### Cross-Entropy Loss (Current):
```python
loss = -log(prob_correct)
```

**Problem:**
```
Sample A: pos, pos, pos, pos, pos  (easy)
loss = -log(0.95) = 0.05

Sample B: pos, neg, neu, pos, neg  (hard)
loss = -log(0.60) = 0.51

‚Üí Treated equally! Hard examples not getting special attention
```

### Focal Loss (Proposed):
```python
loss = -Œ±(1-pt)^Œ≥ * log(pt)
```

**With Œ≥=2.0:**
```
Sample A: pos, pos, pos, pos, pos  (easy)
pt = 0.95
(1-pt)^2 = (0.05)^2 = 0.0025
loss = 0.0025 √ó 0.05 = 0.000125 (almost zero!)

Sample B: pos, neg, neu, pos, neg  (hard)
pt = 0.60
(1-pt)^2 = (0.40)^2 = 0.16
loss = 0.16 √ó 0.51 = 0.0816 (64x more attention!)

‚Üí Hard examples get 64x more attention!
```

---

## üî• Why Focal + Contrastive Should Work

### **Problem Analysis:**

**Current training shows:**
```
Epoch 8: loss=0.4066, cls=0.0764, contr=0.5482
```

**Interpretation:**
- ‚úÖ Contrastive learning working (contr=0.548)
- ‚ùå Classification saturated (cls=0.076 - too low!)
- ‚ùå Model already good at easy examples
- ‚ùå Cannot improve on hard examples

---

### **Solution with Focal Loss:**

**Expected with Focal + Contrastive:**
```
Epoch 8: loss=0.4500, focal=0.3000, contr=0.4500
```

**Why better:**
- ‚úÖ Focal loss = 0.300 (vs 0.076 CE) ‚Üí More room for improvement
- ‚úÖ Hard examples get more attention (64x more!)
- ‚úÖ Better handle class imbalance
- ‚úÖ Still keep contrastive learning (representations)

---

## üìà Expected Training Progress

### **Contrastive Only (Current):**
```
Epoch 1: cls=0.400, contr=1.200
Epoch 2: cls=0.200, contr=0.800
Epoch 3: cls=0.100, contr=0.600
Epoch 4: cls=0.080, contr=0.550
Epoch 5: cls=0.076, contr=0.548  ‚Üê Saturated!
```

**Problem:** Classification loss reaches minimum early ‚Üí No improvement

---

### **Focal + Contrastive (Expected):**
```
Epoch 1: focal=0.800, contr=1.200
Epoch 2: focal=0.600, contr=0.800
Epoch 3: focal=0.450, contr=0.600
Epoch 4: focal=0.350, contr=0.550
Epoch 5: focal=0.300, contr=0.500  ‚Üê Still improving!
Epoch 6: focal=0.280, contr=0.480
Epoch 7: focal=0.260, contr=0.460
Epoch 8: focal=0.250, contr=0.450  ‚Üê Better final!
```

**Benefit:** Both losses keep improving ‚Üí Better final performance

---

## üéØ Why This Should Reach 96%+

### **Current Bottleneck:**
```
Test F1: 95.44%
Validation F1: 94.96%
Training loss: cls=0.076 (saturated)
```

**Problem:** Classification cannot improve further with CE

---

### **Focal Loss Advantage:**
```
Expected:
Test F1: 96.0-96.5%
Validation F1: 95.5-96.0%
Training loss: focal=0.250 (still learning!)
```

**Why better:**
1. **Hard examples focus:** 64x more attention to difficult cases
2. **Class imbalance handling:** Automatic weighting
3. **More training signal:** Focal loss higher than CE (0.25 vs 0.076)
4. **Combination benefits:** Better classification + good representations

---

## üìä Detailed Loss Function Analysis

### **Current Cross-Entropy per Aspect:**

For a sample with Battery=pos, Camera=neg, others=neutral:

```python
# Cross-Entropy
battery_ce = -log(P_battery_pos) = -log(0.90) = 0.105
camera_ce = -log(P_camera_neg) = -log(0.40) = 0.916
others_ce = -log(P_neutral) = -log(0.95) = 0.051 (10√ó)

total_ce = 0.105 + 0.916 + 10√ó0.051 = 1.531
```

**Problem:** 10 neutral aspects dominate loss (0.51) vs important aspects (0.4)

---

### **New Focal Loss per Aspect:**

```python
# Focal Loss with gamma=2.0, alpha=class_weights
battery_focal = Œ±_pos √ó (1-0.90)^2 √ó 0.105 = 1.0 √ó 0.01 √ó 0.105 = 0.00105
camera_focal = Œ±_neg √ó (1-0.40)^2 √ó 0.916 = 2.0 √ó 0.36 √ó 0.916 = 0.6595
others_focal = Œ±_neu √ó (1-0.95)^2 √ó 0.051 = 0.5 √ó 0.0025 √ó 0.051 = 0.000064 (10√ó)

total_focal = 0.00105 + 0.6595 + 10√ó0.000064 = 0.665
```

**Benefits:**
- ‚úÖ Camera (hard) gets 600x more weight than Battery (easy)
- ‚úÖ Neutral aspects negligible (0.000064 vs 0.051)
- ‚úÖ Focus on what matters: hard examples, non-neutral aspects

---

## üî¨ Technical Details

### **Loss Function Implementation:**

```python
def combined_loss(logits, embeddings, labels):
    # Focal Loss (for classification)
    focal_loss = 0
    for each aspect:
        aspect_logits = logits[:, aspect]
        aspect_labels = labels[:, aspect]
        
        # Focal Loss formula
        pt = torch.softmax(aspect_logits, dim=1)[range(batch), aspect_labels]
        alpha = class_weights[aspect_labels]
        focal_loss += (-alpha * (1-pt)**2 * torch.log(pt)).mean()
    
    focal_loss = focal_loss / num_aspects
    
    # Contrastive Loss (for representations)
    contr_loss = contrastive_loss_fn(embeddings, labels)
    
    # Combined
    total_loss = 0.7 * focal_loss + 0.3 * contr_loss
    
    return total_loss, focal_loss, contr_loss
```

### **Class Weights Calculation:**

```python
# From balanced data: 15,921 samples
battery_pos: 1500 samples ‚Üí weight = 15921/1500 = 10.6
battery_neg: 300 samples  ‚Üí weight = 15921/300 = 53.1
battery_neu: 14121 samples ‚Üí weight = 15921/14121 = 1.13

# Apply to focal loss alpha parameter
```

---

## üéØ Expected Results

### **Training Log (Expected):**

```
Epoch 1: loss=1.2500, focal=0.9000, contr=0.6500
Epoch 2: loss=0.9500, focal=0.6500, contr=0.5500
Epoch 3: loss=0.7500, focal=0.5000, contr=0.5000
Epoch 4: loss=0.6000, focal=0.4000, contr=0.4700
Epoch 5: loss=0.5000, focal=0.3500, contr=0.4500
Epoch 6: loss=0.4500, focal=0.3000, contr=0.4300
Epoch 7: loss=0.4250, focal=0.2750, contr=0.4200
Epoch 8: loss=0.4100, focal=0.2600, contr=0.4100

Test F1: 96.0-96.5% ‚Üê TARGET!
```

### **Per-Aspect Improvement:**

Current problematic aspects:
- General: 85.07% F1
- Design: 90.18% F1
- Performance: 91.93% F1

Expected with Focal:
- General: 88-90% F1 (+3-5%)
- Design: 92-94% F1 (+2-4%)
- Performance: 94-95% F1 (+2-3%)

**Overall: 95.44% ‚Üí 96.0-96.5%**

---

## ‚úÖ Action Plan

### **Step 1: Test Focal + Contrastive**

```bash
train_focal_contrastive.bat
```

**Expected:**
- Focal loss: 0.2-0.4 (vs 0.076 CE)
- Contrastive loss: 0.3-0.6 (same)
- Test F1: 96.0-96.5%

---

### **Step 2: If Still < 96%**

Option A: Adjust focal weight
```bash
python train_multilabel_focal_contrastive.py \
    --focal-weight 0.8 --contrastive-weight 0.2
```

Option B: Increase epochs
```bash
python train_multilabel_focal_contrastive.py --epochs 12
```

Option C: Ensemble (guaranteed 96%+)
```bash
# Train 2 models with different focal weights
# Ensemble them
```

---

## üéØ Conclusion

**Current Problem:** Classification saturated with CE, contrastive learning not enough

**Solution:** Add Focal Loss to focus on hard examples

**Expected Improvement:** 95.44% ‚Üí 96.0-96.5% F1

**Why it should work:**
1. ‚úÖ Hard examples get 64x more attention
2. ‚úÖ Better class imbalance handling
3. ‚úÖ Higher training signal (focal loss 0.25 vs CE 0.076)
4. ‚úÖ Combination: Better classification + good representations

**Run training now!** üöÄ
