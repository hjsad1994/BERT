# BiLSTM Multi-Task Learning Configuration
# ========================================================================
# Multi-task approach: Train AD and SC simultaneously with shared backbone
# 
# Model: BiLSTM + CNN with 2 task-specific heads
#   - Shared: Embedding → BiLSTM → Conv1D → Pooling → Dense
#   - Head 1: AD (binary classification, 11 outputs)
#   - Head 2: SC (multi-class, 11 × 3 outputs)
# 
# Loss: Combined loss = α * Loss_AD + β * Loss_SC
# ========================================================================

paths:
  data_dir: "BILSTM-MTL/data"
  train_file: "BILSTM-MTL/data/train_multilabel.csv"
  validation_file: "BILSTM-MTL/data/validation_multilabel.csv"
  test_file: "BILSTM-MTL/data/test_multilabel.csv"
  output_dir: "BILSTM-MTL/models/mtl"
  final_results_dir: "BILSTM-MTL/results"

model:
  # Tokenizer
  tokenizer_name: "5CD-AI/visobert-14gb-corpus"
  
  # Trainable embeddings
  vocab_size: 30000
  embedding_dim: 300
  padding_idx: 0
  
  # Shared BiLSTM backbone
  lstm_hidden_size: 256
  lstm_num_layers: 2
  lstm_dropout: 0.3
  
  # Spatial Dropout
  spatial_dropout: 0.2
  
  # Conv1D
  conv_filters: 128
  conv_kernel_size: 3
  
  # Shared dense
  dense_hidden_size: 256
  dense_dropout: 0.3
  
  # Task settings
  num_aspects: 11
  num_sentiments: 3
  max_length: 256

aspect_names:
  - Battery
  - Camera
  - Performance
  - Display
  - Design
  - Packaging
  - Price
  - Shop_Service
  - Shipping
  - General
  - Others

sentiment_labels:
  positive: 0
  negative: 1
  neutral: 2

training:
  # Batch sizes
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 64
  gradient_accumulation_steps: 2
  
  # Optimizer
  learning_rate: 3.0e-4
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Scheduler
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  
  # Training duration
  num_train_epochs: 30
  
  # Mixed precision
  fp16: true
  
  # DataLoader
  dataloader_num_workers: 2
  dataloader_pin_memory: true
  
  # Evaluation
  evaluation_strategy: "epoch"
  save_strategy: "epoch"
  save_total_limit: 3
  
  # Early stopping
  early_stopping_patience: 7
  early_stopping_threshold: 0.001
  
  # Logging
  logging_steps: 50
  
  # Device
  device: "cuda"

# Multi-task learning settings
multi_task:
  # Loss weights (combined loss = alpha * L_AD + beta * L_SC)
  loss_weight_ad: 1.0       # Weight for AD loss
  loss_weight_sc: 1.0       # Weight for SC loss
  
  # AD task settings
  aspect_detection:
    use_pos_weight: true    # Use pos_weight for BCE loss
    pos_weight_auto: true   # Auto-calculate from data
    metric: "f1_macro"      # Metric for evaluation (standard in ABSA research)
  
  # SC task settings
  sentiment_classification:
    use_class_weight: true  # Use class weights for CE loss
    class_weight_auto: true # Auto-calculate from data
    metric: "f1_macro"      # Changed to F1 Macro for consistency with ABSA standards
  
  # Best model selection
  # Options: "ad_f1_macro", "sc_f1_macro", "combined" (average of both)
  best_model_metric: "combined"

# Reproducibility
reproducibility:
  seed: 42
  deterministic: true

general:
  log_level: "info"
  save_predictions: true
