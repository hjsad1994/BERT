# OPTIMIZED Config for RTX 3070 8GB - HuggingFace Best Practices 2024
# ========================================================================
# Reference: https://huggingface.co/docs/transformers/perf_train_gpu_one
#
# Key Optimizations:
# 1. Batch size = 64 (power of 2, optimal for GPU)
# 2. Mixed precision FP16 + TF32 (Ampere optimization)
# 3. Gradient accumulation = 2 (effective batch = 128)
# 4. AdamW 8-bit optimizer (50% memory reduction)
# 5. Cosine scheduler with 6% warmup (optimal for convergence)
# 6. DataLoader: 4 workers, prefetch 2 (balanced throughput)
# ========================================================================

paths:
  data_dir: "data"
  # train_file: "data/train.csv"
  # train_file: "data/train_augmented_nhung.csv"
  # train_file: "data/train_augmented_neutral_nhung.csv"
  # Multi-label files:
  train_file: "data/train_multilabel_balanced.csv"
  
  validation_file: "data/validation_multilabel.csv"
  test_file: "data/test_multilabel.csv"
  output_dir: "finetuned_visobert_absa_model"
  evaluation_report: "evaluation_report.txt"
  predictions_file: "test_predictions.csv"

model:
  name: "5CD-AI/Vietnamese-Sentiment-visobert"
  num_labels: 3
  max_length: 256  # Covers 97.7% of sequences

valid_aspects:
  - Battery
  - Camera
  - Performance
  - Display
  - Design
  - Software
  - Packaging
  - Price
  - Audio
  - Warranty
  - Shop_Service
  - Shipping
  - General
  - Others

sentiment_labels:
  positive: 0
  negative: 1
  neutral: 2

training:
  # ========================================================
  # BATCH SIZE - MAXIMIZED for 99-100% GPU Utilization
  # ========================================================
  # Pushing to 96 to max out RTX 3070 8GB VRAM
  # This will achieve 99-100% GPU utilization
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 64  # 2x (no gradients in eval)
  gradient_accumulation_steps: 2   # Effective batch = 96 * 1 = 96

  # per_device_train_batch_size: 16
  # per_device_eval_batch_size: 32         # FIX!
  # gradient_accumulation_steps: 4         # FIX!
  
  # ========================================================
  # OPTIMIZER - AdamW 8-bit (Memory Efficient)
  # ========================================================
  # 8-bit optimizer reduces memory by ~50% with minimal impact
  optim: "adamw_bnb_8bit"  # Requires bitsandbytes
  learning_rate: 2.0e-5  # Standard for BERT (2e-5 to 5e-5)
  weight_decay: 0.01     # L2 regularization
  adam_beta1: 0.9        # First moment decay
  adam_beta2: 0.999      # Second moment decay
  adam_epsilon: 1.0e-8   # Numerical stability
  max_grad_norm: 1.0     # Gradient clipping
  
  # ========================================================
  # LEARNING RATE SCHEDULER - Cosine with Warmup
  # ========================================================
  # Cosine decay: better than linear for final convergence
  # Warmup: prevents early instability with large LR
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.06  # 6% warmup (HF recommendation for small datasets)
  
  # ========================================================
  # TRAINING DURATION
  # ========================================================
  num_train_epochs: 3
  
  # ========================================================
  # MIXED PRECISION - FP16 + TF32
  # ========================================================
  # RTX 3070 = Ampere architecture
  # FP16: 2x speed, 50% memory reduction
  # TF32: additional speed boost (Ampere only)
  fp16: true
  fp16_opt_level: "O2"  # Aggressive mixed precision (O2 > O1)
  fp16_full_eval: false # Keep eval in FP32 for accuracy
  tf32: true            # Enable TensorFloat-32 (Ampere GPUs)
  
  # ========================================================
  # DATALOADER - MAXIMIZED for GPU Saturation
  # ========================================================
  # Reduced workers to avoid CPU bottleneck with large batch
  # Increased prefetch to keep GPU fed
  dataloader_num_workers: 2
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 4  # Prefetch more with large batch
  dataloader_persistent_workers: true
  
  # ========================================================
  # MEMORY OPTIMIZATION
  # ========================================================
  # Gradient checkpointing: Disabled (8GB sufficient)
  # Only enable if OOM occurs (trades compute for memory)
  gradient_checkpointing: false
  
  # Advanced memory settings
  auto_find_batch_size: false  # We set batch size manually
  group_by_length: false       # Not beneficial for BERT
  
  # ========================================================
  # EVALUATION & CHECKPOINTING
  # ========================================================
  evaluation_strategy: "epoch"  # Eval after each epoch
  save_strategy: "epoch"        # Save after each epoch
  save_total_limit: 3          # Keep best 3 checkpoints
  load_best_model_at_end: true  # Load best at end
  metric_for_best_model: "eval_f1"
  greater_is_better: true
  
  # ========================================================
  # EARLY STOPPING
  # ========================================================
  early_stopping_patience: 3      # Stop if no improvement for 3 epochs
  early_stopping_threshold: 0.001 # Min improvement threshold
  
  # ========================================================
  # LOGGING
  # ========================================================
  logging_strategy: "steps"
  logging_steps: 50
  logging_first_step: true
  report_to: []  # Disable WandB/TensorBoard
  
  # ========================================================
  # REPRODUCIBILITY
  # ========================================================
  seed: 42
  data_seed: 42
  
  # ========================================================
  # MISC
  # ========================================================
  disable_tqdm: false
  prediction_loss_only: false
  remove_unused_columns: true
  label_names: ["labels"]
  include_inputs_for_metrics: false

general:
  seed: 42
  device: "auto"
  log_level: "info"
