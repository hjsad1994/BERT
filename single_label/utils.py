"""
Module Ti·ªán √çch cho ABSA Fine-tuning
====================================
Ch·ª©a c√°c h√†m v√† class ti·ªán √≠ch ƒë·ªÉ h·ªó tr·ª£ qu√° tr√¨nh hu·∫•n luy·ªán m√¥ h√¨nh ABSA

Bao g·ªìm:
    - load_config: ƒê·ªçc file c·∫•u h√¨nh YAML
    - set_seed: Thi·∫øt l·∫≠p seed cho reproducibility
    - load_and_preprocess_data: T·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu
    - ABSADataset: Custom PyTorch Dataset cho ABSA
    - compute_metrics: T√≠nh to√°n c√°c ch·ªâ s·ªë ƒë√°nh gi√°
    - save_predictions: L∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n
"""

import os
import random
import yaml
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset
from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support


class FocalLoss(nn.Module):
    """
    Focal Loss ƒë·ªÉ x·ª≠ l√Ω class imbalance
    
    Focal Loss = -Œ±(1-pt)^Œ≥ * log(pt)
    
    Args:
        alpha: Tr·ªçng s·ªë cho t·ª´ng class (list ho·∫∑c tensor)
        gamma: Focusing parameter (default=2.0). TƒÉng gamma tƒÉng focus v√†o hard examples
        reduction: 'mean' ho·∫∑c 'sum'
    
    Reference:
        Lin et al. "Focal Loss for Dense Object Detection" (2017)
        https://arxiv.org/abs/1708.02002
    """
    
    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        """
        Args:
            inputs: Predicted logits (batch_size, num_classes)
            targets: Ground truth labels (batch_size)
        
        Returns:
            loss: Focal loss value
        """
        # Get probabilities
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)  # pt = probability of true class
        
        # Apply focal term: (1 - pt)^gamma
        focal_term = (1 - pt) ** self.gamma
        
        # Apply alpha weighting if provided
        if self.alpha is not None:
            if isinstance(self.alpha, (list, np.ndarray)):
                alpha = torch.tensor(self.alpha, device=inputs.device)
            else:
                alpha = self.alpha
            
            # Get alpha for each sample based on target class
            alpha_t = alpha[targets]
            loss = alpha_t * focal_term * ce_loss
        else:
            loss = focal_term * ce_loss
        
        # Reduction
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss


def load_config(config_path):
    """
    ƒê·ªçc file c·∫•u h√¨nh YAML
    
    Args:
        config_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file config.yaml
        
    Returns:
        dict: Dictionary ch·ª©a c·∫•u h√¨nh
        
    Raises:
        FileNotFoundError: N·∫øu file kh√¥ng t·ªìn t·∫°i
        yaml.YAMLError: N·∫øu file YAML kh√¥ng h·ª£p l·ªá
    """
    print(f"üìñ ƒêang t·∫£i c·∫•u h√¨nh t·ª´: {config_path}")
    
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file c·∫•u h√¨nh: {config_path}")
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        print(f"‚úì ƒê√£ t·∫£i c·∫•u h√¨nh th√†nh c√¥ng")
        return config
    except yaml.YAMLError as e:
        raise yaml.YAMLError(f"L·ªói khi ƒë·ªçc file YAML: {str(e)}")


def set_seed(seed):
    """
    Thi·∫øt l·∫≠p seed cho random, numpy, v√† torch ƒë·ªÉ ƒë·∫£m b·∫£o reproducibility
    
    Args:
        seed: Gi√° tr·ªã seed (integer)
    """
    print(f"üé≤ ƒêang thi·∫øt l·∫≠p seed = {seed} cho reproducibility")
    
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    
    print(f"‚úì ƒê√£ thi·∫øt l·∫≠p seed th√†nh c√¥ng")


def load_and_preprocess_data(config):
    """
    T·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu t·ª´ c√°c file CSV
    
    Args:
        config: Dictionary ch·ª©a c·∫•u h√¨nh
        
    Returns:
        tuple: (train_df, val_df, test_df, label_map, id2label)
        
    Raises:
        FileNotFoundError: N·∫øu file d·ªØ li·ªáu kh√¥ng t·ªìn t·∫°i
        ValueError: N·∫øu d·ªØ li·ªáu kh√¥ng h·ª£p l·ªá
    """
    print(f"\n{'='*70}")
    print("üìä ƒêang t·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu...")
    print(f"{'='*70}")
    
    # L·∫•y ƒë∆∞·ªùng d·∫´n t·ª´ config
    train_path = config['paths']['train_file']
    val_path = config['paths']['validation_file']
    test_path = config['paths']['test_file']
    
    # Ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa c√°c file
    for path in [train_path, val_path, test_path]:
        if not os.path.exists(path):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu: {path}")
    
    # ƒê·ªçc c√°c file CSV
    print(f"\n‚úì ƒêang ƒë·ªçc file train: {train_path}")
    train_df = pd.read_csv(train_path, encoding='utf-8-sig')
    
    print(f"‚úì ƒêang ƒë·ªçc file validation: {val_path}")
    val_df = pd.read_csv(val_path, encoding='utf-8-sig')
    
    print(f"‚úì ƒêang ƒë·ªçc file test: {test_path}")
    test_df = pd.read_csv(test_path, encoding='utf-8-sig')
    
    # Data ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi prepare_data.py (underscores removed)
    # No additional preprocessing needed
    
    # Ki·ªÉm tra c√°c c·ªôt b·∫Øt bu·ªôc
    required_columns = ['sentence', 'aspect', 'sentiment']
    for df_name, df in [('train', train_df), ('validation', val_df), ('test', test_df)]:
        missing_cols = [col for col in required_columns if col not in df.columns]
        if missing_cols:
            raise ValueError(f"File {df_name} thi·∫øu c√°c c·ªôt: {', '.join(missing_cols)}")
    
    print(f"\n‚úì K√≠ch th∆∞·ªõc d·ªØ li·ªáu:")
    print(f"   Train:      {len(train_df):>6} m·∫´u")
    print(f"   Validation: {len(val_df):>6} m·∫´u")
    print(f"   Test:       {len(test_df):>6} m·∫´u")
    print(f"   T·ªïng:       {len(train_df) + len(val_df) + len(test_df):>6} m·∫´u")
    
    # Ki·ªÉm tra c√°c kh√≠a c·∫°nh h·ª£p l·ªá (n·∫øu c√≥ trong config)
    if 'valid_aspects' in config:
        valid_aspects = set(config['valid_aspects'])
        
        for df_name, df in [('train', train_df), ('validation', val_df), ('test', test_df)]:
            invalid_aspects = set(df['aspect'].unique()) - valid_aspects
            if invalid_aspects:
                print(f"\n‚ö†Ô∏è  C·∫£nh b√°o: File {df_name} ch·ª©a c√°c aspect kh√¥ng h·ª£p l·ªá: {invalid_aspects}")
    
    # L·∫•y label mapping t·ª´ config
    if 'sentiment_labels' in config:
        label_map = config['sentiment_labels']
    else:
        # T·∫°o label mapping t·ª± ƒë·ªông
        unique_sentiments = sorted(set(train_df['sentiment'].unique()) | 
                                   set(val_df['sentiment'].unique()) | 
                                   set(test_df['sentiment'].unique()))
        label_map = {sentiment: idx for idx, sentiment in enumerate(unique_sentiments)}
    
    # T·∫°o reverse mapping
    id2label = {idx: label for label, idx in label_map.items()}
    
    print(f"\n‚úì Label mapping:")
    for label, idx in label_map.items():
        print(f"   {label:>10} -> {idx}")
    
    # M√£ h√≥a sentiment th√†nh label_id
    for df in [train_df, val_df, test_df]:
        df['label_id'] = df['sentiment'].map(label_map)
        
        # Ki·ªÉm tra c√°c sentiment kh√¥ng h·ª£p l·ªá
        invalid_mask = df['label_id'].isna()
        if invalid_mask.any():
            invalid_sentiments = df[invalid_mask]['sentiment'].unique()
            raise ValueError(f"Ph√°t hi·ªán sentiment kh√¥ng h·ª£p l·ªá: {invalid_sentiments}")
    
    # Ph√¢n t√≠ch ph√¢n b·ªë nh√£n
    print(f"\n‚úì Ph√¢n b·ªë nh√£n:")
    for df_name, df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:
        print(f"\n   {df_name}:")
        sentiment_counts = df['sentiment'].value_counts()
        for sentiment, count in sentiment_counts.items():
            percentage = count / len(df) * 100
            print(f"      {sentiment:>10}: {count:>5} ({percentage:>5.1f}%)")
    
    print(f"\n‚úì Ho√†n t·∫•t vi·ªác t·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu")
    
    return train_df, val_df, test_df, label_map, id2label


class ABSADataset(Dataset):
    """
    Custom PyTorch Dataset cho ABSA (Aspect-Based Sentiment Analysis)
    
    Format input cho BERT: [CLS] sentence [SEP] aspect [SEP]
    """
    
    def __init__(self, dataframe, tokenizer, max_length=256):
        """
        Kh·ªüi t·∫°o ABSADataset
        
        Args:
            dataframe: pandas DataFrame ch·ª©a columns: sentence, aspect, label_id
            tokenizer: Tokenizer t·ª´ transformers
            max_length: ƒê·ªô d√†i t·ªëi ƒëa c·ªßa sequence
        """
        self.dataframe = dataframe.reset_index(drop=True)
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # Validate required columns
        required_cols = ['sentence', 'aspect', 'label_id']
        missing = [col for col in required_cols if col not in self.dataframe.columns]
        if missing:
            raise ValueError(f"DataFrame thi·∫øu c√°c c·ªôt: {', '.join(missing)}")
    
    def __len__(self):
        """Tr·∫£ v·ªÅ s·ªë l∆∞·ª£ng m·∫´u trong dataset"""
        return len(self.dataframe)
    
    def __getitem__(self, idx):
        """
        L·∫•y m·ªôt m·∫´u t·ª´ dataset
        
        Args:
            idx: Index c·ªßa m·∫´u
            
        Returns:
            dict: Dictionary ch·ª©a input_ids, attention_mask, token_type_ids, labels
        """
        row = self.dataframe.iloc[idx]
        
        sentence = str(row['sentence'])
        aspect = str(row['aspect'])
        label = int(row['label_id'])
        
        # Format: [CLS] sentence [SEP] aspect [SEP]
        # S·ª≠ d·ª•ng tokenizer v·ªõi sentence pair ƒë·ªÉ t·ª± ƒë·ªông th√™m [CLS], [SEP]
        encoding = self.tokenizer(
            sentence,
            aspect,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_token_type_ids=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'token_type_ids': encoding['token_type_ids'].squeeze(0),
            'labels': torch.tensor(label, dtype=torch.long)
        }


def compute_metrics(eval_preds):
    """
    T√≠nh to√°n c√°c ch·ªâ s·ªë ƒë√°nh gi√° cho m√¥ h√¨nh
    
    Args:
        eval_preds: Tuple (predictions, labels) t·ª´ Trainer
        
    Returns:
        dict: Dictionary ch·ª©a accuracy, precision, recall, f1
    """
    predictions, labels = eval_preds
    
    # L·∫•y class c√≥ x√°c su·∫•t cao nh·∫•t
    if len(predictions.shape) > 1:
        preds = np.argmax(predictions, axis=1)
    else:
        preds = predictions
    
    # T√≠nh accuracy
    accuracy = accuracy_score(labels, preds)
    
    # T√≠nh precision, recall, f1 v·ªõi weighted average
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, 
        preds, 
        average='weighted',
        zero_division=0
    )
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }


def get_detailed_metrics(predictions, labels, label_names):
    """
    T√≠nh to√°n metrics chi ti·∫øt cho t·ª´ng class
    
    Args:
        predictions: Array c√°c predictions
        labels: Array c√°c true labels
        label_names: List t√™n c√°c label
        
    Returns:
        str: Classification report d·∫°ng string
    """
    if len(predictions.shape) > 1:
        preds = np.argmax(predictions, axis=1)
    else:
        preds = predictions
    
    # T·∫°o classification report
    report = classification_report(
        labels,
        preds,
        target_names=label_names,
        digits=4,
        zero_division=0
    )
    
    return report


def save_predictions(trainer, test_dataset, test_df, config, id2label):
    """
    D·ª± ƒëo√°n tr√™n test set v√† l∆∞u k·∫øt qu·∫£ v√†o CSV
    
    Args:
        trainer: Hugging Face Trainer object
        test_dataset: Test Dataset object
        test_df: Test DataFrame g·ªëc
        config: Dictionary ch·ª©a c·∫•u h√¨nh
        id2label: Dictionary mapping t·ª´ label_id sang t√™n sentiment
    """
    print(f"\n{'='*70}")
    print("üîÆ ƒêang d·ª± ƒëo√°n tr√™n t·∫≠p test...")
    print(f"{'='*70}")
    
    # D·ª± ƒëo√°n
    predictions_output = trainer.predict(test_dataset)
    predictions = predictions_output.predictions
    
    # L·∫•y predicted class
    if len(predictions.shape) > 1:
        pred_labels = np.argmax(predictions, axis=1)
    else:
        pred_labels = predictions
    
    # T·∫°o DataFrame v·ªõi k·∫øt qu·∫£
    results_df = test_df[['sentence', 'aspect', 'sentiment']].copy()
    results_df['predicted_sentiment'] = [id2label[pred] for pred in pred_labels]
    results_df.rename(columns={'sentiment': 'true_sentiment'}, inplace=True)
    
    # L∆∞u v√†o file
    output_path = config['paths']['predictions_file']
    results_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    
    print(f"‚úì ƒê√£ l∆∞u predictions v√†o: {output_path}")
    print(f"‚úì S·ªë l∆∞·ª£ng predictions: {len(results_df)}")
    
    # T√≠nh accuracy tr√™n test set
    correct = (results_df['true_sentiment'] == results_df['predicted_sentiment']).sum()
    total = len(results_df)
    accuracy = correct / total * 100
    
    print(f"‚úì Accuracy tr√™n test set: {accuracy:.2f}% ({correct}/{total})")
    
    # In m·ªôt s·ªë v√≠ d·ª•
    print(f"\n‚úì M·ªôt s·ªë v√≠ d·ª• d·ª± ƒëo√°n:")
    sample_size = min(5, len(results_df))
    for idx in range(sample_size):
        row = results_df.iloc[idx]
        status = "‚úì" if row['true_sentiment'] == row['predicted_sentiment'] else "‚úó"
        print(f"\n   {status} M·∫´u {idx + 1}:")
        print(f"      C√¢u:    {row['sentence'][:60]}...")
        print(f"      Aspect: {row['aspect']}")
        print(f"      Th·ª±c t·∫ø: {row['true_sentiment']:>10} | D·ª± ƒëo√°n: {row['predicted_sentiment']:>10}")
    
    return results_df


def save_predictions_from_output(predictions_output, test_df, config, id2label):
    """
    L∆∞u predictions v√†o CSV t·ª´ predictions_output ƒë√£ c√≥
    (Tr√°nh predict 2 l·∫ßn - ti·∫øt ki·ªám memory v√† th·ªùi gian)
    
    Args:
        predictions_output: Output t·ª´ trainer.predict() ƒë√£ c√≥ s·∫µn
        test_df: Test DataFrame g·ªëc
        config: Dictionary ch·ª©a c·∫•u h√¨nh
        id2label: Dictionary mapping t·ª´ label_id sang t√™n sentiment
    """
    print(f"\n{'='*70}")
    print("üíæ ƒêang l∆∞u predictions v√†o file...")
    print(f"{'='*70}")
    
    predictions = predictions_output.predictions
    
    # L·∫•y predicted class
    if len(predictions.shape) > 1:
        pred_labels = np.argmax(predictions, axis=1)
    else:
        pred_labels = predictions
    
    # T·∫°o DataFrame v·ªõi k·∫øt qu·∫£
    results_df = test_df[['sentence', 'aspect', 'sentiment']].copy()
    results_df['predicted_sentiment'] = [id2label[pred] for pred in pred_labels]
    results_df.rename(columns={'sentiment': 'true_sentiment'}, inplace=True)
    
    # L∆∞u v√†o file
    output_path = config['paths']['predictions_file']
    results_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    
    print(f"‚úì ƒê√£ l∆∞u predictions v√†o: {output_path}")
    print(f"‚úì S·ªë l∆∞·ª£ng predictions: {len(results_df)}")
    
    # T√≠nh accuracy tr√™n test set
    correct = (results_df['true_sentiment'] == results_df['predicted_sentiment']).sum()
    total = len(results_df)
    accuracy = correct / total * 100
    
    print(f"‚úì Accuracy tr√™n test set: {accuracy:.2f}% ({correct}/{total})")
    
    # In m·ªôt s·ªë v√≠ d·ª•
    print(f"\n‚úì M·ªôt s·ªë v√≠ d·ª• d·ª± ƒëo√°n:")
    sample_size = min(5, len(results_df))
    for idx in range(sample_size):
        row = results_df.iloc[idx]
        status = "‚úì" if row['true_sentiment'] == row['predicted_sentiment'] else "‚úó"
        print(f"\n   {status} M·∫´u {idx + 1}:")
        print(f"      C√¢u:    {row['sentence'][:60]}...")
        print(f"      Aspect: {row['aspect']}")
        print(f"      Th·ª±c t·∫ø: {row['true_sentiment']:>10} | D·ª± ƒëo√°n: {row['predicted_sentiment']:>10}")
    
    return results_df


def print_system_info():
    """In th√¥ng tin v·ªÅ h·ªá th·ªëng v√† c√°c th∆∞ vi·ªán"""
    print(f"\n{'='*70}")
    print("üíª TH√îNG TIN H·ªÜ TH·ªêNG")
    print(f"{'='*70}")
    
    # Python version
    import sys
    print(f"Python version: {sys.version.split()[0]}")
    
    # PyTorch version
    print(f"PyTorch version: {torch.__version__}")
    
    # CUDA availability
    if torch.cuda.is_available():
        print(f"‚úì CUDA available: True")
        print(f"   CUDA version: {torch.version.cuda}")
        print(f"   GPU device: {torch.cuda.get_device_name(0)}")
        print(f"   GPU count: {torch.cuda.device_count()}")
    else:
        print(f"‚úó CUDA available: False (s·∫Ω s·ª≠ d·ª•ng CPU)")
    
    # Transformers version
    try:
        import transformers
        print(f"Transformers version: {transformers.__version__}")
    except:
        pass
    
    print(f"{'='*70}\n")

