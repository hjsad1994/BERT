"""
Script Chu·∫©n B·ªã D·ªØ Li·ªáu ABSA
============================
Chuy·ªÉn ƒë·ªïi dataset t·ª´ format multi-label (nhi·ªÅu aspect tr√™n m·ªôt d√≤ng)
sang format single-label (m·ªôt m·∫´u cho m·ªói c·∫∑p sentence-aspect)

Input: dataset.csv (format g·ªëc v·ªõi nhi·ªÅu c·ªôt aspect)
Output: 
    - data/train.csv
    - data/validation.csv  
    - data/test.csv
    
Format output: sentence, aspect, sentiment
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import os
from collections import Counter


class ABSADataPreparator:
    """Class ƒë·ªÉ chu·∫©n b·ªã d·ªØ li·ªáu ABSA"""
    
    # Danh s√°ch c√°c kh√≠a c·∫°nh h·ª£p l·ªá t·ª´ dataset g·ªëc
    VALID_ASPECTS = [
        'Battery', 'Camera', 'Performance', 'Display', 'Design',
        'Software', 'Packaging', 'Price', 'Audio', 'Warranty', 'Shop_Service',
        'Shipping', 'General', 'Others'
    ]
    
    # Mapping sentiment t·ª´ format g·ªëc sang chu·∫©n h√≥a
    SENTIMENT_MAPPING = {
        'Positive': 'positive',
        'Negative': 'negative',
        'Neutral': 'neutral',
        'positive': 'positive',
        'negative': 'negative',
        'neutral': 'neutral'
    }
    
    def __init__(self, input_file, output_dir='data', train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_seed=42):
        """
        Kh·ªüi t·∫°o ABSADataPreparator
        
        Args:
            input_file: ƒê∆∞·ªùng d·∫´n file CSV input (multi-label format)
            output_dir: Th∆∞ m·ª•c l∆∞u output files
            train_ratio: T·ª∑ l·ªá t·∫≠p train
            val_ratio: T·ª∑ l·ªá t·∫≠p validation
            test_ratio: T·ª∑ l·ªá t·∫≠p test
            random_seed: Random seed cho reproducibility
        """
        self.input_file = input_file
        self.output_dir = output_dir
        self.train_ratio = train_ratio
        self.val_ratio = val_ratio
        self.test_ratio = test_ratio
        self.random_seed = random_seed
        
        # Validate ratios
        if not np.isclose(train_ratio + val_ratio + test_ratio, 1.0):
            raise ValueError(f"T·ª∑ l·ªá ph·∫£i t·ªïng b·∫±ng 1.0, nh·∫≠n ƒë∆∞·ª£c {train_ratio + val_ratio + test_ratio}")
        
        # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a t·ªìn t·∫°i
        os.makedirs(output_dir, exist_ok=True)
    
    def load_data(self):
        """T·∫£i dataset t·ª´ file CSV"""
        print(f"\n{'='*70}")
        print(f"üìÅ ƒêang t·∫£i dataset t·ª´: {self.input_file}")
        print(f"{'='*70}")
        
        if not os.path.exists(self.input_file):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file: {self.input_file}")
        
        # ƒê·ªçc CSV v·ªõi encoding UTF-8-sig ƒë·ªÉ x·ª≠ l√Ω BOM
        self.df = pd.read_csv(self.input_file, encoding='utf-8-sig')
        
        print(f"‚úì K√≠ch th∆∞·ªõc dataset: {self.df.shape}")
        print(f"‚úì C√°c c·ªôt: {', '.join(self.df.columns)}")
        print(f"‚úì T·ªïng s·ªë d√≤ng: {len(self.df)}")
        
        # Ki·ªÉm tra c·ªôt 'data' (ch·ª©a c√¢u vƒÉn)
        if 'data' not in self.df.columns:
            raise ValueError("Dataset ph·∫£i c√≥ c·ªôt 'data' ch·ª©a c√¢u vƒÉn")
        
        # X·ª≠ l√Ω VNCoreNLP segmentation: remove underscores cho BERT tokenizer
        underscore_count = self.df['data'].astype(str).str.count('_').sum()
        if underscore_count > 0:
            print(f"\nüîß Ph√°t hi·ªán {underscore_count:,} underscores (VNCoreNLP segmentation)")
            print(f"üîß ƒêang chuy·ªÉn ƒë·ªïi ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi BERT tokenizer: 'ChƒÉm_s√≥c' ‚Üí 'ChƒÉm s√≥c'")
            self.df['data'] = self.df['data'].astype(str).str.replace('_', ' ', regex=False)
            print(f"‚úì ƒê√£ x·ª≠ l√Ω VNCoreNLP segmentation (BERT-friendly format)")
        
        # Ki·ªÉm tra c√°c c·ªôt aspect
        found_aspects = [col for col in self.VALID_ASPECTS if col in self.df.columns]
        print(f"‚úì T√¨m th·∫•y {len(found_aspects)} kh√≠a c·∫°nh: {', '.join(found_aspects)}")
        
        if len(found_aspects) == 0:
            raise ValueError("Kh√¥ng t√¨m th·∫•y c·ªôt aspect n√†o trong dataset")
        
        self.aspect_columns = found_aspects
        
        return self
    
    def clean_data(self):
        """L√†m s·∫°ch d·ªØ li·ªáu"""
        print(f"\n{'='*70}")
        print("üßπ ƒêang l√†m s·∫°ch d·ªØ li·ªáu...")
        print(f"{'='*70}")
        
        initial_size = len(self.df)
        
        # Lo·∫°i b·ªè c√°c d√≤ng c√≥ c√¢u vƒÉn r·ªóng
        self.df = self.df[self.df['data'].notna() & (self.df['data'].str.strip() != '')]
        
        # L√†m s·∫°ch kho·∫£ng tr·∫Øng trong c√¢u vƒÉn
        self.df['data'] = self.df['data'].str.strip()
        
        final_size = len(self.df)
        removed = initial_size - final_size
        
        print(f"‚úì D√≤ng ban ƒë·∫ßu: {initial_size}")
        print(f"‚úì D√≤ng sau khi l√†m s·∫°ch: {final_size}")
        if removed > 0:
            print(f"‚úì ƒê√£ lo·∫°i b·ªè: {removed} d√≤ng c√≥ c√¢u vƒÉn r·ªóng")
        
        return self
    
    def convert_to_single_label(self):
        """Chuy·ªÉn ƒë·ªïi t·ª´ multi-label sang single-label format"""
        print(f"\n{'='*70}")
        print("üîÑ ƒêang chuy·ªÉn ƒë·ªïi sang format ABSA single-label...")
        print(f"{'='*70}")
        
        absa_samples = []
        skipped_count = 0
        
        # L·∫∑p qua t·ª´ng d√≤ng trong dataset
        for idx, row in self.df.iterrows():
            sentence = row['data']
            
            # L·∫∑p qua t·ª´ng aspect column
            for aspect in self.aspect_columns:
                sentiment_value = row[aspect]
                
                # B·ªè qua n·∫øu aspect kh√¥ng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p (NaN ho·∫∑c empty)
                if pd.isna(sentiment_value) or str(sentiment_value).strip() == '':
                    continue
                
                sentiment_str = str(sentiment_value).strip()
                
                # Chu·∫©n h√≥a sentiment
                if sentiment_str in self.SENTIMENT_MAPPING:
                    normalized_sentiment = self.SENTIMENT_MAPPING[sentiment_str]
                else:
                    # B·ªè qua c√°c gi√° tr·ªã sentiment kh√¥ng h·ª£p l·ªá
                    skipped_count += 1
                    continue
                
                # Th√™m m·∫´u ABSA
                absa_samples.append({
                    'sentence': sentence,
                    'aspect': aspect,
                    'sentiment': normalized_sentiment
                })
        
        # T·∫°o DataFrame m·ªõi
        self.absa_df = pd.DataFrame(absa_samples)
        
        print(f"‚úì S·ªë d√≤ng g·ªëc: {len(self.df)}")
        print(f"‚úì S·ªë m·∫´u ABSA ƒë∆∞·ª£c t·∫°o: {len(self.absa_df)}")
        print(f"‚úì Trung b√¨nh aspects/c√¢u: {len(self.absa_df)/len(self.df):.2f}")
        if skipped_count > 0:
            print(f"‚úì B·ªè qua {skipped_count} gi√° tr·ªã sentiment kh√¥ng h·ª£p l·ªá")
        
        return self
    
    def analyze_distribution(self):
        """Ph√¢n t√≠ch ph√¢n b·ªë d·ªØ li·ªáu"""
        print(f"\n{'='*70}")
        print("üìä Ph√¢n t√≠ch ph√¢n b·ªë d·ªØ li·ªáu...")
        print(f"{'='*70}")
        
        # Ph√¢n b·ªë theo sentiment
        print("\n1. Ph√¢n b·ªë theo Sentiment:")
        sentiment_counts = self.absa_df['sentiment'].value_counts()
        for sentiment, count in sentiment_counts.items():
            percentage = count / len(self.absa_df) * 100
            print(f"   {sentiment:>10}: {count:>6} m·∫´u ({percentage:>5.2f}%)")
        
        # Ph√¢n b·ªë theo aspect
        print("\n2. Ph√¢n b·ªë theo Aspect:")
        aspect_counts = self.absa_df['aspect'].value_counts()
        for aspect, count in aspect_counts.items():
            percentage = count / len(self.absa_df) * 100
            print(f"   {aspect:>15}: {count:>6} m·∫´u ({percentage:>5.2f}%)")
        
        # Ph√¢n b·ªë k·∫øt h·ª£p
        print("\n3. Ph√¢n b·ªë Sentiment theo t·ª´ng Aspect:")
        for aspect in self.aspect_columns:
            aspect_data = self.absa_df[self.absa_df['aspect'] == aspect]
            if len(aspect_data) > 0:
                print(f"\n   {aspect}:")
                sentiment_dist = aspect_data['sentiment'].value_counts()
                for sentiment, count in sentiment_dist.items():
                    percentage = count / len(aspect_data) * 100
                    print(f"      {sentiment:>10}: {count:>5} ({percentage:>5.1f}%)")
        
        return self
    
    def stratified_split(self):
        """Chia d·ªØ li·ªáu v·ªõi stratified sampling ƒë·ªÉ ƒë·∫£m b·∫£o ph√¢n b·ªë c√¢n b·∫±ng"""
        print(f"\n{'='*70}")
        print("‚úÇÔ∏è  ƒêang chia d·ªØ li·ªáu v·ªõi stratified sampling...")
        print(f"{'='*70}")
        
        # Set random seed
        np.random.seed(self.random_seed)
        
        # T·∫°o stratify label b·∫±ng c√°ch k·∫øt h·ª£p aspect v√† sentiment
        self.absa_df['stratify_label'] = self.absa_df['aspect'] + '_' + self.absa_df['sentiment']
        
        # Split 1: train+val vs test
        train_val_df, test_df = train_test_split(
            self.absa_df,
            test_size=self.test_ratio,
            random_state=self.random_seed,
            stratify=self.absa_df['stratify_label'],
            shuffle=True
        )
        
        # Split 2: train vs val
        val_ratio_adjusted = self.val_ratio / (self.train_ratio + self.val_ratio)
        train_df, val_df = train_test_split(
            train_val_df,
            test_size=val_ratio_adjusted,
            random_state=self.random_seed,
            stratify=train_val_df['stratify_label'],
            shuffle=True
        )
        
        # X√≥a c·ªôt stratify_label kh√¥ng c·∫ßn thi·∫øt
        train_df = train_df[['sentence', 'aspect', 'sentiment']].reset_index(drop=True)
        val_df = val_df[['sentence', 'aspect', 'sentiment']].reset_index(drop=True)
        test_df = test_df[['sentence', 'aspect', 'sentiment']].reset_index(drop=True)
        
        # L∆∞u tr·ªØ splits
        self.train_df = train_df
        self.val_df = val_df
        self.test_df = test_df
        
        # In th·ªëng k√™
        total = len(self.absa_df)
        print(f"\n‚úì Th·ªëng k√™ ph√¢n chia:")
        print(f"   Train:      {len(self.train_df):>6} m·∫´u ({len(self.train_df)/total*100:>5.1f}%)")
        print(f"   Validation: {len(self.val_df):>6} m·∫´u ({len(self.val_df)/total*100:>5.1f}%)")
        print(f"   Test:       {len(self.test_df):>6} m·∫´u ({len(self.test_df)/total*100:>5.1f}%)")
        print(f"   T·ªïng:       {total:>6} m·∫´u")
        
        return self
    
    def validate_splits(self):
        """Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa c√°c splits"""
        print(f"\n{'='*70}")
        print("‚úÖ ƒêang ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa c√°c splits...")
        print(f"{'='*70}")
        
        # Ki·ªÉm tra sentiment distribution trong t·ª´ng split
        print("\nPh√¢n b·ªë Sentiment trong c√°c splits:")
        for split_name, split_df in [('Train', self.train_df), ('Val', self.val_df), ('Test', self.test_df)]:
            print(f"\n{split_name}:")
            sentiment_dist = split_df['sentiment'].value_counts(normalize=True) * 100
            for sentiment, percentage in sentiment_dist.items():
                print(f"   {sentiment:>10}: {percentage:>5.1f}%")
        
        # Ki·ªÉm tra aspect distribution trong t·ª´ng split
        print("\nPh√¢n b·ªë Aspect trong c√°c splits:")
        for split_name, split_df in [('Train', self.train_df), ('Val', self.val_df), ('Test', self.test_df)]:
            print(f"\n{split_name}:")
            aspect_dist = split_df['aspect'].value_counts(normalize=True) * 100
            for aspect, percentage in aspect_dist.items():
                print(f"   {aspect:>15}: {percentage:>5.1f}%")
        
        return self
    
    def save_splits(self):
        """L∆∞u c√°c splits th√†nh CSV files"""
        print(f"\n{'='*70}")
        print("üíæ ƒêang l∆∞u c√°c splits...")
        print(f"{'='*70}")
        
        # Define output paths
        train_path = os.path.join(self.output_dir, 'train.csv')
        val_path = os.path.join(self.output_dir, 'validation.csv')
        test_path = os.path.join(self.output_dir, 'test.csv')
        
        # L∆∞u th√†nh CSV v·ªõi UTF-8 encoding
        self.train_df.to_csv(train_path, index=False, encoding='utf-8-sig')
        self.val_df.to_csv(val_path, index=False, encoding='utf-8-sig')
        self.test_df.to_csv(test_path, index=False, encoding='utf-8-sig')
        
        # In th√¥ng tin file
        print(f"\n‚úì Files ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh c√¥ng:")
        print(f"\n   üìÑ Train:      {train_path}")
        print(f"      K√≠ch th∆∞·ªõc: {os.path.getsize(train_path) / 1024:.2f} KB")
        print(f"      S·ªë m·∫´u:     {len(self.train_df)}")
        
        print(f"\n   üìÑ Validation: {val_path}")
        print(f"      K√≠ch th∆∞·ªõc: {os.path.getsize(val_path) / 1024:.2f} KB")
        print(f"      S·ªë m·∫´u:     {len(self.val_df)}")
        
        print(f"\n   üìÑ Test:       {test_path}")
        print(f"      K√≠ch th∆∞·ªõc: {os.path.getsize(test_path) / 1024:.2f} KB")
        print(f"      S·ªë m·∫´u:     {len(self.test_df)}")
        
        return self
    
    def save_metadata(self):
        """L∆∞u metadata v·ªÅ qu√° tr√¨nh chu·∫©n b·ªã d·ªØ li·ªáu"""
        import json
        from datetime import datetime
        
        metadata = {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'input_file': self.input_file,
            'output_directory': self.output_dir,
            'random_seed': self.random_seed,
            'split_ratios': {
                'train': self.train_ratio,
                'validation': self.val_ratio,
                'test': self.test_ratio
            },
            'split_sizes': {
                'train': len(self.train_df),
                'validation': len(self.val_df),
                'test': len(self.test_df),
                'total': len(self.absa_df)
            },
            'aspects': self.aspect_columns,
            'sentiments': ['positive', 'negative', 'neutral'],
            'format': {
                'columns': ['sentence', 'aspect', 'sentiment'],
                'description': 'Single-label ABSA format: m·ªói d√≤ng ch·ª©a m·ªôt c·∫∑p (sentence, aspect) v√† sentiment t∆∞∆°ng ·ª©ng'
            }
        }
        
        metadata_path = os.path.join(self.output_dir, 'data_metadata.json')
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"\n‚úì Metadata ƒë√£ ƒë∆∞·ª£c l∆∞u: {metadata_path}")
        
        return self
    
    def run(self):
        """Th·ª±c thi to√†n b·ªô pipeline chu·∫©n b·ªã d·ªØ li·ªáu"""
        try:
            self.load_data()
            self.clean_data()
            self.convert_to_single_label()
            self.analyze_distribution()
            self.stratified_split()
            self.validate_splits()
            self.save_splits()
            self.save_metadata()
            
            print(f"\n{'='*70}")
            print("üéâ [TH√ÄNH C√îNG] Chu·∫©n b·ªã d·ªØ li·ªáu ho√†n t·∫•t!")
            print(f"{'='*70}")
            print(f"\n‚úì Output files ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {os.path.abspath(self.output_dir)}/")
            print(f"\n‚úì B·∫°n c√≥ th·ªÉ b·∫Øt ƒë·∫ßu hu·∫•n luy·ªán b·∫±ng l·ªánh:")
            print(f"   python train.py --config config.yaml")
            
            return self
            
        except Exception as e:
            print(f"\n{'='*70}")
            print(f"‚ùå [L·ªñI] ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh chu·∫©n b·ªã d·ªØ li·ªáu!")
            print(f"{'='*70}")
            print(f"Chi ti·∫øt l·ªói: {str(e)}")
            raise


def main():
    """H√†m main"""
    import sys
    import io
    
    # Thi·∫øt l·∫≠p UTF-8 cho console output tr√™n Windows
    if sys.platform == 'win32':
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
    
    # C·∫•u h√¨nh
    INPUT_FILE = 'dataset.csv'
    OUTPUT_DIR = 'data'
    TRAIN_RATIO = 0.7
    VAL_RATIO = 0.15
    TEST_RATIO = 0.15
    RANDOM_SEED = 42
    
    print("\n" + "="*70)
    print("üöÄ ABSA DATA PREPARATION PIPELINE")
    print("="*70)
    print(f"\nC·∫•u h√¨nh:")
    print(f"  Input file:     {INPUT_FILE}")
    print(f"  Output dir:     {OUTPUT_DIR}")
    print(f"  Train ratio:    {TRAIN_RATIO:.0%}")
    print(f"  Val ratio:      {VAL_RATIO:.0%}")
    print(f"  Test ratio:     {TEST_RATIO:.0%}")
    print(f"  Random seed:    {RANDOM_SEED}")
    
    # T·∫°o preparator v√† ch·∫°y
    preparator = ABSADataPreparator(
        input_file=INPUT_FILE,
        output_dir=OUTPUT_DIR,
        train_ratio=TRAIN_RATIO,
        val_ratio=VAL_RATIO,
        test_ratio=TEST_RATIO,
        random_seed=RANDOM_SEED
    )
    
    preparator.run()


if __name__ == '__main__':
    main()
