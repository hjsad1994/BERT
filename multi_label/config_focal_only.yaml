# Multi-Label ABSA Configuration - FOCAL LOSS ONLY TEST
# ========================================================================
# For multi-label approach (all 11 aspects predicted simultaneously)
# Model: ViSoBERT with multi-label classification head
# Method: Focal Loss ONLY (no contrastive)
# Baseline: Compare with Focal+Contrastive (95.99%) and GHM variants
# ========================================================================

paths:
  data_dir: "multi_label/data"
  train_file: "multi_label/data/train_multilabel_balanced.csv"
  validation_file: "multi_label/data/validation_multilabel.csv"
  test_file: "multi_label/data/test_multilabel.csv"
  output_dir: "multi_label/models/multilabel_focal_only"  # Focal Loss only (no contrastive)
  evaluation_report: "multi_label/results/evaluation_report_focal_only.txt"
  predictions_file: "multi_label/results/test_predictions_focal_only.csv"

model:
  name: "5CD-AI/Vietnamese-Sentiment-visobert"
  num_labels: 33  # 11 aspects Ã— 3 sentiments = 33 outputs
  num_aspects: 11
  num_sentiments: 3
  max_length: 256
  
  # Model architecture
  hidden_size: 512
  projection_dim: 256  # Not used for focal only
  dropout: 0.3

valid_aspects:
  - Battery
  - Camera
  - Performance
  - Display
  - Design
  - Packaging
  - Price
  - Shop_Service
  - Shipping
  - General
  - Others

sentiment_labels:
  positive: 0
  negative: 1
  neutral: 2

training:
  # Batch size
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 64
  gradient_accumulation_steps: 2
  
  # Optimizer settings
  optim: "adamw_bnb_8bit"
  learning_rate: 2.0e-5  # Slightly higher for focal only
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8
  max_grad_norm: 1.0
  
  # Learning rate scheduler
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.06
  
  # Training duration
  num_train_epochs: 15
  
  # Mixed precision
  fp16: true
  fp16_opt_level: "O2"
  fp16_full_eval: false
  tf32: true
  
  # DataLoader settings
  dataloader_num_workers: 2
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 4
  dataloader_persistent_workers: true
  
  # Memory optimization
  gradient_checkpointing: false
  auto_find_batch_size: false
  group_by_length: false
  
  # Evaluation & checkpointing
  evaluation_strategy: "epoch"
  save_strategy: "epoch"
  save_total_limit: 5
  load_best_model_at_end: true
  metric_for_best_model: "eval_f1"
  greater_is_better: true
  
  # Early stopping
  early_stopping_patience: 5
  early_stopping_threshold: 0.001
  
  # Logging
  logging_strategy: "steps"
  logging_steps: 50
  logging_first_step: true
  report_to: []
  
  # Reproducibility
  seed: 42
  data_seed: 42
  
  # Misc
  disable_tqdm: false
  prediction_loss_only: false
  remove_unused_columns: true
  label_names: ["labels"]
  include_inputs_for_metrics: false

general:
  seed: 42
  device: "auto"
  log_level: "info"

# Multi-label specific settings - FOCAL LOSS ONLY
multi_label:
  # LOSS FUNCTION TYPE
  loss_type: "focal"  # Use standard Focal Loss
  
  # Loss function weights (FOCAL ONLY, NO CONTRASTIVE)
  focal_weight: 1.0           # 100% Focal Loss
  contrastive_weight: 0.0     # 0% Contrastive Loss (DISABLED)
  
  # ========== FOCAL LOSS SETTINGS ==========
  focal_gamma: 2.0            # Focusing parameter (1.0-5.0)
                              # Higher = more focus on hard examples
                              # Standard: 2.0
                              # For extreme imbalance: 3.0-5.0
  
  focal_alpha: "auto"         # Class weights
                              # "auto" = calculate from data
                              # Or provide list: [1.0, 1.0, 1.0]
  
  # ========== CONTRASTIVE LEARNING SETTINGS (DISABLED) ==========
  use_contrastive: false      # Disabled - using Focal only
  contrastive_temperature: 0.1   # Not used
  contrastive_base_weight: 0.1   # Not used
  contrastive_type: "improved"   # Not used
  
  # ========== GHM LOSS FALLBACK (if loss_type = "ghm") ==========
  ghm_bins: 10                # Not used
  ghm_momentum: 0.75          # Not used
  
  # ========== DATA SETTINGS ==========
  use_balanced_data: true
  balance_method: "aspect_wise"
  
  # Inference settings
  batch_prediction: true
  use_class_weights: true     # Apply class weights to focal loss
