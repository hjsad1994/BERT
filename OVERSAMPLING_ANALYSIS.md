# Ph√¢n T√≠ch: T·∫°i Sao Oversampling Kh√¥ng Hi·ªáu Qu·∫£

## K·∫øt Qu·∫£ Th·ª±c T·∫ø

### Training v·ªõi Oversampling (20251013_221133)

| Epoch | Accuracy | F1 (Overall) | Eval Loss | V·∫•n ƒê·ªÅ |
|-------|----------|--------------|-----------|---------|
| 1 | 89.63% | 0.9028 | 0.1198 | Baseline |
| 2 | 89.63% | 0.9027 | 0.1422 | Loss ‚Üë 19% |
| 3 | 90.73% | 0.9100 | 0.1874 | Loss ‚Üë 32% |
| 4 | 92.05% | 0.9213 | 0.2410 | **OVERFITTING** |

**üö® D·∫•u hi·ªáu Overfitting r√µ r√†ng:**
- F1 tƒÉng nh∆∞ng Loss tƒÉng li√™n t·ª•c
- Model ƒëang memorize training data

---

## Nguy√™n Nh√¢n Oversampling Th·∫•t B·∫°i

### 1. Random Duplicate - Kh√¥ng T·∫°o Diversity
```python
# C√°ch hi·ªán t·∫°i (oversampling_utils.py)
oversampled = class_df.sample(n=n_samples, replace=True, random_state=random_state)
```

**V·∫•n ƒë·ªÅ:**
- Ch·ªâ copy y nguy√™n samples
- Model th·∫•y c√πng 1 sample nhi·ªÅu l·∫ßn ‚Üí h·ªçc thu·ªôc
- Kh√¥ng c√≥ variation/noise ƒë·ªÉ generalize

### 2. Oversample Qu√° Nhi·ªÅu
```
neutral: 1,069 ‚Üí 2,830 samples (+1,761)
TƒÉng 265% so v·ªõi g·ªëc!
```

**T√°c ƒë·ªông:**
- Model focus qu√° nhi·ªÅu v√†o neutral class
- Training time l√¢u h∆°n
- Overfitting nghi√™m tr·ªçng

### 3. Thi·∫øu Data Augmentation
- Kh√¥ng c√≥ SMOTE (Synthetic Minority Over-sampling)
- Kh√¥ng c√≥ text augmentation (synonym replacement, back-translation)
- Kh√¥ng c√≥ mixup/cutmix

### 4. Kh√¥ng Track Per-Class Metrics
- M·ª•c ti√™u: c·∫£i thi·ªán Neutral class (F1=0.48)
- Nh∆∞ng ch·ªâ track overall F1
- Kh√¥ng bi·∫øt Neutral class c√≥ c·∫£i thi·ªán th·∫≠t kh√¥ng

---

## Gi·∫£i Ph√°p ƒê·ªÅ Xu·∫•t

### ‚úÖ Gi·∫£i ph√°p 1: Smart Oversampling v·ªõi Class Weights (ƒê∆°n gi·∫£n, hi·ªáu qu·∫£)

**Thay v√¨ duplicate samples, tƒÉng loss weight:**

```python
# train.py
from sklearn.utils.class_weight import compute_class_weight

# T√≠nh class weights
class_weights = compute_class_weight(
    'balanced',
    classes=np.unique(train_labels),
    y=train_labels
)

# √Åp d·ª•ng v√†o Focal Loss (ƒë√£ c√≥)
# Alpha weights t·ª± ƒë·ªông balance theo frequency
```

**∆Øu ƒëi·ªÉm:**
- Kh√¥ng duplicate ‚Üí tr√°nh overfitting
- Model focus nhi·ªÅu h∆°n v√†o minority class
- Training nhanh h∆°n (data size kh√¥ng tƒÉng)

---

### ‚úÖ Gi·∫£i ph√°p 2: Moderate Oversampling + Early Stopping

**Gi·∫£m t·ª∑ l·ªá oversample:**

```python
# Thay v√¨ 30% of majority (2,830 samples)
# ‚Üí Ch·ªâ 15-20% (1,698-2,264 samples)

target_counts = {
    'neutral': int(majority_count * 0.15),  # 15% thay v√¨ 30%
    'positive': class_counts['positive'],
    'negative': class_counts['negative']
}
```

**K·∫øt h·ª£p Early Stopping ch·∫∑t ch·∫Ω h∆°n:**

```yaml
# config.yaml
training:
  early_stopping_patience: 1  # Gi·∫£m t·ª´ 2 ‚Üí 1
  num_train_epochs: 3  # Gi·∫£m t·ª´ 4 ‚Üí 3
  metric_for_best_model: "eval_loss"  # D√πng loss thay v√¨ F1
```

---

### ‚úÖ Gi·∫£i ph√°p 3: Text Augmentation (N√¢ng cao)

**T·∫°o synthetic samples thay v√¨ duplicate:**

```python
# augmentation.py
import nlpaug.augmenter.word as naw

# 1. Synonym Replacement (ti·∫øng Vi·ªát)
aug_synonym = naw.SynonymAug(aug_src='ppdb', model_path='vi')

# 2. Back Translation
aug_back_trans = naw.BackTranslationAug(
    from_model_name='VietAI/envit5-translation',
    to_model_name='VietAI/envit5-translation'
)

# 3. Random Insertion/Deletion
def augment_text(text, num_aug=2):
    augmented = []
    for _ in range(num_aug):
        augmented.append(aug_synonym.augment(text))
    return augmented

# √Åp d·ª•ng ch·ªâ cho minority class
if label == 'neutral':
    augmented_texts = augment_text(text, num_aug=2)
```

---

### ‚úÖ Gi·∫£i ph√°p 4: SMOTE cho Text (Embedding-based)

**T·∫°o synthetic samples trong embedding space:**

```python
from imblearn.over_sampling import SMOTE

# 1. Encode texts th√†nh embeddings
embeddings = model.encode(texts)

# 2. Apply SMOTE
smote = SMOTE(sampling_strategy='auto', k_neighbors=5)
X_resampled, y_resampled = smote.fit_resample(embeddings, labels)

# 3. Decode embeddings ‚Üí text (ho·∫∑c train tr·ª±c ti·∫øp tr√™n embeddings)
```

---

### ‚úÖ Gi·∫£i ph√°p 5: Track Per-Class Metrics

**Th√™m custom metric callback:**

```python
# focal_loss_trainer.py
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    preds = np.argmax(predictions, axis=1)
    
    # Overall metrics
    overall_f1 = f1_score(labels, preds, average='weighted')
    
    # Per-class metrics
    per_class_f1 = f1_score(labels, preds, average=None)
    
    return {
        'f1': overall_f1,
        'f1_neutral': per_class_f1[2],  # neutral = class 2
        'f1_positive': per_class_f1[0],
        'f1_negative': per_class_f1[1],
    }
```

---

## Khuy·∫øn Ngh·ªã Th·ª±c Hi·ªán

### üéØ Ph∆∞∆°ng √°n Nhanh (1-2 gi·ªù)

1. **T·∫Øt Oversampling, tƒÉng Class Weights**
   ```yaml
   # config.yaml - T·∫Øt oversampling
   training:
     use_oversampling: false
   ```

2. **Gi·∫£m epochs xu·ªëng 3**
3. **Track per-class F1**
4. **Compare v·ªõi baseline**

---

### üéØ Ph∆∞∆°ng √°n T·ªët Nh·∫•t (1-2 ng√†y)

1. **Gi·ªØ Moderate Oversampling (15% thay v√¨ 30%)**
2. **Th√™m Text Augmentation cho Neutral class**
3. **Early stopping v·ªõi patience=1**
4. **Track per-class metrics**
5. **Test nhi·ªÅu strategies:**
   - No oversampling + class weights
   - Moderate oversampling + augmentation
   - SMOTE (n·∫øu c√≥ th·ªùi gian)

---

## So S√°nh Strategies

| Strategy | Overfitting Risk | Implementation Cost | Expected Improvement |
|----------|------------------|---------------------|----------------------|
| No oversampling + class weights | Low | Low | +2-3% Neutral F1 |
| Moderate oversampling (15%) | Medium | Low | +3-5% Neutral F1 |
| Text augmentation | Low | Medium | +5-7% Neutral F1 |
| SMOTE | Medium | High | +4-6% Neutral F1 |

---

## Next Steps

1. **T·∫Øt oversampling, test baseline v·ªõi class weights only**
2. **So s√°nh F1 per-class**
3. **N·∫øu kh√¥ng c·∫£i thi·ªán, th·ª≠ moderate oversampling (15%)**
4. **N·∫øu v·∫´n overfit, implement text augmentation**

---

## T√†i Li·ªáu Tham Kh·∫£o

- [SMOTE for Text Classification](https://arxiv.org/abs/1812.04718)
- [Text Augmentation Techniques](https://github.com/makcedward/nlpaug)
- [Class Imbalance in NLP](https://aclanthology.org/2020.emnlp-main.438/)
