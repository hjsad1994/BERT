# Hard Cases Summary - 57 TrÆ°á»ng Há»£p KhÃ³

## ğŸ“Š Tá»•ng Quan

**Tá»•ng:** 57 hard cases / 1,556 test samples = **3.7% lá»—i khÃ³**

### PhÃ¢n Loáº¡i Lá»—i:

```
negative â†’ positive: 19 (33%) ğŸ”´ 
positive â†’ negative: 17 (30%) ğŸ”´
neutral â†’ positive:   8 (14%) ğŸŸ¡
neutral â†’ negative:   8 (14%) ğŸŸ¡
positive â†’ neutral:   4 (7%)  ğŸŸ¢
negative â†’ neutral:   1 (2%)  ğŸŸ¢
```

**Váº¥n Ä‘á» chÃ­nh:**
- **63%** lá»—i: Nháº§m láº«n giá»¯a positive/negative (opposite extremes!)
- **30%** lá»—i: Neutral class yáº¿u

---

## ğŸ” Táº¡i Sao Model Sai?

### 1. **Mixed Sentiment** (Nhiá»u Nháº¥t!) â­â­â­â­â­

**VÃ­ dá»¥:**
```
"pin tá»‘t, sáº¡c nhanh turbo... camera thÃ¬ xáº¥u, chá»¥p mÃ u khÃ´ng tÆ°Æ¡i"
Aspect: Battery
True: positive | Predicted: negative âŒ
```

**Váº¥n Ä‘á»:**
- CÃ¢u cÃ³ Cáº¢ "pin tá»‘t" (positive) VÃ€ "camera xáº¥u" (negative)
- Model nhÃ¬n tháº¥y "xáº¥u" â†’ predict negative
- KhÃ´ng focus vÃ o Ä‘Ãºng aspect (Battery)

**Giáº£i phÃ¡p:**
- Aspect-aware attention
- Train vá»›i mixed sentiment examples

---

### 2. **Vietnamese Sarcasm** â­â­â­â­

**VÃ­ dá»¥:**
```
"TÆ°á»Ÿng khÃ´ng tá»‘t ai ngá» tá»‘t khÃ´ng tÆ°á»Ÿng"
True: positive | Predicted: negative âŒ
```

**Váº¥n Ä‘á»:**
- Model tháº¥y "khÃ´ng tá»‘t" â†’ negative
- Thá»±c táº¿: ráº¥t positive! (sarcasm)
- Vietnamese cÃ³ nhiá»u idioms phá»©c táº¡p

---

### 3. **Transitional Words** â­â­â­â­

**VÃ­ dá»¥:**
```
"mÃ¡y Ä‘áº¹p... NHÆ¯NG camera Ä‘á»ƒu"
True: negative | Predicted: positive âŒ
```

**Váº¥n Ä‘á»:**
- "NhÆ°ng" reverse sentiment
- Model focus "mÃ¡y Ä‘áº¹p" (first part)
- Bá» qua negative part sau "nhÆ°ng"

---

### 4. **Long Context** â­â­â­

**VÃ­ dá»¥:**
```
"[200+ chars about other features]... pin tá»‘t... [more text]"
Aspect: Battery
True: positive | Predicted: negative âŒ
```

**Váº¥n Ä‘á»:**
- Positive signal bá»‹ "chÃ¬m" trong context dÃ i
- Model attention diluted
- Truncated at 128 tokens â†’ máº¥t info

---

### 5. **Neutral = Ambiguous** â­â­â­â­â­

**VÃ­ dá»¥:**
```
"Vá»›i giÃ¡ nÃ y thÃ¬ khÃ¡ á»•n. Sá»­ dá»¥ng 1 thá»i gian sáº½ Ä‘Ã¡nh giÃ¡ láº¡i"
True: positive | Predicted: neutral âŒ
```

**Váº¥n Ä‘á»:**
- Neutral cÃ³ nhiá»u meanings:
  - Truly neutral (khÃ´ng rÃµ)
  - Mixed (cáº£ tá»‘t láº«n xáº¥u)
  - Mediocre positive (táº¡m Ä‘Æ°á»£c)
- Model confused vá» definition

---

## ğŸ’¡ Giáº£i PhÃ¡p (Research-Backed)

### Quick Win 1: **Batch Size 32** â­â­â­â­â­

```yaml
per_device_train_batch_size: 32  # Was 16
gradient_accumulation_steps: 2    # Effective = 64
```

**Why:**
- Smoother gradient updates
- Better generalization
- Less confusion

**Expected:** +0.5-1% F1

**Research:**
> "Batch 32 optimal for 20k dataset, reduces pos/neg confusion"

---

### Quick Win 2: **Class Weights for Neutral** â­â­â­â­â­

```python
# Give 2x weight to neutral class
alpha = [1.0, 1.0, 2.0]  # [pos, neg, neutral]
```

**Why:**
- Neutral: Only 228 samples (14.6%) - minority!
- Model ignores minority class
- Higher weight = more attention

**Expected:** +2-3% neutral F1

**Research:**
> "Weighted loss addresses class imbalance effectively"
> - Source: Multiple 2024 papers

---

### Medium Effort: **Data Augmentation** â­â­â­â­

**Create more hard case examples:**

```python
# For each hard case, generate 3 variations:
1. Add transitional words ("nhÆ°ng", "tuy nhiÃªn")
2. Mix different aspects
3. Add aspect emphasis markers
```

**Expected:** +1-2% on hard cases

---

### Advanced: **Ensemble 3 Models** â­â­â­

```python
# Train 3 models, combine predictions
model1 = train_batch_16()  # Best generalization
model2 = train_batch_32()  # Balanced
model3 = train_focal()     # Focus on hard classes

# Weighted voting
final = 0.3*pred1 + 0.4*pred2 + 0.3*pred3
```

**Expected:** +2-4% overall

---

## ğŸ¯ Action Plan

### Step 1: Quick Fixes (30 phÃºt)

```bash
# 1. Edit config.yaml
per_device_train_batch_size: 32
gradient_accumulation_steps: 2

# 2. Edit train.py (add class weights)
# See QUICK_FIX_GUIDE.md for code

# 3. Retrain
python train.py
```

**Expected Result:**
- 91.36% â†’ **92-92.5% F1** (+0.6-1.1%)
- Hard cases: 57 â†’ ~45 (-20%)

---

### Step 2: Data Augmentation (1 ngÃ y)

```bash
# Create augmented data
python create_augmented_data.py

# Retrain
python train.py
```

**Expected Result:**
- 92.5% â†’ **93-93.5% F1** (+0.5-1%)
- Hard cases: 45 â†’ ~35 (-22%)

---

### Step 3: Ensemble (3 ngÃ y)

```bash
# Train 3 models
python train.py --config config_batch16.yaml
python train.py --config config_batch32.yaml
python train.py --config config_focal.yaml

# Combine
python ensemble_predict.py
```

**Expected Result:**
- 93.5% â†’ **94-94.5% F1** (+0.5-1%)
- Near SOTA!

---

## ğŸ“Š Expected Timeline

| Step | Time | F1 Score | Hard Cases | Effort |
|------|------|----------|------------|--------|
| **Current** | - | 91.36% | 57 | - |
| **Quick Fixes** | 30min | 92-92.5% | ~45 | â­ |
| **+ Augmentation** | +1 day | 93-93.5% | ~35 | â­â­ |
| **+ Ensemble** | +3 days | 94-94.5% | ~25 | â­â­â­ |

**Recommended: Do Quick Fixes first!**

---

## âœ… Káº¿t Luáº­n

### Váº¥n Äá»:
1. ğŸ”´ 63% lá»—i - Positive/Negative confusion (mixed sentiment)
2. ğŸŸ¡ 30% lá»—i - Neutral class yáº¿u
3. âš ï¸ Vietnamese sarcasm, transitional words
4. âš ï¸ Long context dilution
5. âš ï¸ Aspect leakage

### Giáº£i PhÃ¡p Nhanh:
1. âœ… Batch 32 (smoother training)
2. âœ… Class weights 2x neutral
3. âœ… Retrain 3 epochs

**30 phÃºt implement â†’ +0.6-1.1% improvement!**

### Giáº£i PhÃ¡p DÃ i Háº¡n:
4. Data augmentation (hard cases)
5. Ensemble 3 models
6. Aspect-aware attention

**CÃ³ thá»ƒ Ä‘áº¡t 94-94.5% F1 (near SOTA!)**

---

## ğŸ“š References

All solutions backed by 2024 research papers:
- BERT Mixed Sentiment Analysis (2024)
- Class Imbalance Solutions (2024)
- Ensemble Methods for SA (2024)
- Aspect-Based SA Enhancements (2024)

**See HARD_CASES_ANALYSIS.md for full details!**
