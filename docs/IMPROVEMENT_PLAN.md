# Káº¿ Hoáº¡ch Cáº£i Thiá»‡n Äá»™ ChÃ­nh XÃ¡c Tá»« 91.06%

Dá»±a trÃªn phÃ¢n tÃ­ch log training: `training_logs/training_log_20251010_225807.txt`

---

## ğŸ“Š PhÃ¢n TÃ­ch Káº¿t Quáº£ Hiá»‡n Táº¡i

### âœ… Äiá»ƒm Máº¡nh:
- **Overall Accuracy**: 91.06% (khÃ¡ tá»‘t)
- **Negative class**: Precision=0.96, Recall=0.94, **F1=0.95** âœ… (1192 samples)
- **Positive class**: Precision=0.89, Recall=0.92, **F1=0.91** âœ… (716 samples)

### âŒ Váº¥n Äá» NghiÃªm Trá»ng:

#### 1. **Neutral Class Performance Ráº¥t Tháº¥p**
```
Neutral: Precision=0.48, Recall=0.48, F1=0.48 âŒ (chá»‰ 106 samples)
```
- Chá»‰ dá»± Ä‘oÃ¡n Ä‘Ãºng ~50% samples neutral
- NguyÃªn nhÃ¢n: **Severe imbalance** (11.08x)
- Neutral chá»‰ chiáº¿m 5.3% trong training data

#### 2. **Overfitting RÃµ RÃ ng**
```
Epoch 1: eval_loss=0.233, acc=91.3%
Epoch 2: eval_loss=0.207, acc=91.7% â† BEST MODEL
Epoch 3: eval_loss=0.214, acc=89.7% â†“ 
Epoch 4: eval_loss=0.356, acc=91.8% â† LOSS TÄ‚NG 70%!
```
- Eval loss tÄƒng tá»« 0.207 â†’ 0.356 (tÄƒng 70%)
- Model Ä‘ang há»c thuá»™c training data sau epoch 2
- Early stopping KHÃ”NG hoáº¡t Ä‘á»™ng (Ä‘Ã¡ng láº½ dá»«ng á»Ÿ epoch 2)

---

## ğŸ¯ CÃ¡c BÆ°á»›c Cáº£i Thiá»‡n (Theo Thá»© Tá»± Æ¯u TiÃªn)

### âœ… **1. Báº¬T Láº I OVERSAMPLING** (Æ¯u tiÃªn CAO NHáº¤T) - ÄÃƒ FIX

**Váº¥n Ä‘á»:** Neutral class quÃ¡ Ã­t (106 samples = 5.3%)

**Giáº£i phÃ¡p Ä‘Ã£ Ã¡p dá»¥ng:**
```yaml
# train.py
- Oversample neutral tá»« 501 â†’ 1,665 samples (30% of majority)
- Imbalance ratio giáº£m tá»« 11.08x â†’ ~3.3x
```

**Káº¿t quáº£ mong Ä‘á»£i:** F1 neutral tÄƒng tá»« 0.48 â†’ 0.65-0.75

---

### âœ… **2. GIáº¢M Sá» EPOCHS** - ÄÃƒ FIX

**Váº¥n Ä‘á»:** Training 4 epochs nhÆ°ng best model á»Ÿ epoch 2

**Giáº£i phÃ¡p Ä‘Ã£ Ã¡p dá»¥ng:**
```yaml
# config.yaml
num_train_epochs: 5 â†’ 3
```

**LÃ½ do:** 
- Epoch 2: eval_loss=0.207 (lowest)
- Epoch 3-4: eval_loss tÄƒng (overfitting)
- Dá»«ng á»Ÿ epoch 3 Ä‘á»ƒ trÃ¡nh lÃ£ng phÃ­ thá»i gian

---

### ğŸ”„ **3. TÄ‚NG DROPOUT** (Náº¿u váº«n overfit)

Náº¿u sau khi Ã¡p dá»¥ng 1+2 mÃ  váº«n overfit:

```yaml
# config.yaml - CHÆ¯A ÃP Dá»¤NG, test xem bÆ°á»›c 1+2 cÃ³ Ä‘á»§ khÃ´ng
model:
  hidden_dropout_prob: 0.2  # TÄƒng tá»« 0.1 (default)
  attention_probs_dropout_prob: 0.2
```

---

### ğŸ“Š **4. ÄIá»€U CHá»ˆNH LEARNING RATE** (TÃ¹y chá»n)

Náº¿u káº¿t quáº£ chÆ°a tá»‘t:

**Option A: Giáº£m thÃªm má»™t chÃºt**
```yaml
learning_rate: 1.5e-5 â†’ 1.2e-5
```
â†’ Há»c cháº­m hÆ¡n, á»•n Ä‘á»‹nh hÆ¡n

**Option B: TÄƒng warmup**
```yaml
warmup_ratio: 0.1 â†’ 0.15
```
â†’ Model há»c á»•n Ä‘á»‹nh hÆ¡n á»Ÿ Ä‘áº§u training

---

### ğŸ”¥ **5. TÄ‚NG FOCAL LOSS GAMMA** (TÃ¹y chá»n)

Náº¿u neutral váº«n tháº¥p sau oversampling:

```python
# utils.py - FocalLoss
gamma = 2.0 â†’ 2.5  # Focus nhiá»u hÆ¡n vÃ o hard examples (neutral)
```

---

## ğŸ“ Checklist CÃ¡c Thay Äá»•i ÄÃ£ Ãp Dá»¥ng

- âœ… Báº­t láº¡i oversampling vá»›i 30% ratio
- âœ… Giáº£m epochs tá»« 5 â†’ 3
- âœ… Giá»¯ nguyÃªn learning_rate = 1.5e-5
- âœ… Giá»¯ nguyÃªn weight_decay = 0.05
- âœ… Early stopping Ä‘Ã£ cÃ³ (patience=2)

---

## ğŸš€ BÆ°á»›c Tiáº¿p Theo

### 1. **Cháº¡y Training Má»›i**
```bash
python train.py
```

### 2. **Theo DÃµi Metrics**

**ChÃº Ã½ cÃ¡c chá»‰ sá»‘ sau:**

```
âœ… Má»¥c tiÃªu cáº£i thiá»‡n:
- Neutral F1: 0.48 â†’ >0.65 (tÄƒng 35%+)
- Overall Accuracy: 91.06% â†’ 92-93%
- Eval loss khÃ´ng tÄƒng sau epoch 2

âš ï¸ Dáº¥u hiá»‡u xáº¥u (cáº§n Ä‘iá»u chá»‰nh tiáº¿p):
- Neutral F1 váº«n <0.60
- Eval loss váº«n tÄƒng á»Ÿ epoch 3
- Accuracy khÃ´ng cáº£i thiá»‡n
```

### 3. **So SÃ¡nh Káº¿t Quáº£**

| Metric | TrÆ°á»›c | Má»¥c tiÃªu | Thá»±c táº¿ |
|--------|-------|----------|---------|
| **Overall Acc** | 91.06% | 92-93% | ___ |
| **Positive F1** | 0.91 | ~0.91 | ___ |
| **Negative F1** | 0.95 | ~0.95 | ___ |
| **Neutral F1** | **0.48** | **>0.65** | ___ |
| **Best Epoch** | 2 | 2-3 | ___ |
| **Eval Loss** | TÄƒng | KhÃ´ng tÄƒng | ___ |

---

## ğŸ“ˆ Dá»± ÄoÃ¡n Káº¿t Quáº£

**Ká»‹ch báº£n láº¡c quan:**
```
- Neutral F1: 0.48 â†’ 0.70 (tÄƒng 46%)
- Overall Accuracy: 91.06% â†’ 92.5%
- KhÃ´ng overfit (eval loss á»•n Ä‘á»‹nh)
```

**Ká»‹ch báº£n thá»±c táº¿:**
```
- Neutral F1: 0.48 â†’ 0.62-0.68 (tÄƒng 30-42%)
- Overall Accuracy: 91.06% â†’ 91.8-92.2%
- Overfit nháº¹ hÆ¡n
```

**Náº¿u káº¿t quáº£ khÃ´ng Ä‘áº¡t â†’ Ãp dá»¥ng bÆ°á»›c 3-5**

---

## âš ï¸ LÆ°u Ã Quan Trá»ng

### 1. **Äá»«ng Ká»³ Vá»ng QuÃ¡ Cao VÃ o Neutral**
- Neutral class vá»‘n khÃ³ phÃ¢n biá»‡t (khÃ´ng positive, khÃ´ng negative)
- Dataset chá»‰ cÃ³ 106 test samples â†’ variance cao
- F1 = 0.65-0.70 lÃ  **ráº¥t tá»‘t** cho class nÃ y

### 2. **Trade-off CÃ³ Thá»ƒ Xáº£y Ra**
- Cáº£i thiá»‡n neutral cÃ³ thá»ƒ lÃ m giáº£m nháº¹ positive/negative
- Overall accuracy cÃ³ thá»ƒ tÄƒng nháº¹ hoáº·c giá»¯ nguyÃªn
- **Má»¥c tiÃªu: CÃ¢n báº±ng 3 classes, khÃ´ng chá»‰ maximize overall accuracy**

### 3. **Overfitting Váº«n LÃ  Má»‘i Quan TÃ¢m**
- Oversampling tÄƒng duplicates â†’ cÃ³ thá»ƒ tÄƒng overfitting
- Cáº§n theo dÃµi eval loss cáº©n tháº­n
- Náº¿u overfit náº·ng â†’ Ã¡p dá»¥ng bÆ°á»›c 3 (tÄƒng dropout)

---

## ğŸ“š TÃ i Liá»‡u Tham Kháº£o

- `OVERSAMPLING_FOCAL_LOSS.md`: Giáº£i thÃ­ch chi tiáº¿t vá» chiáº¿n lÆ°á»£c
- `CHECKPOINT_MANAGEMENT.md`: CÃ¡ch load best model
- `FIX_PREDICT_CRASH.md`: Xá»­ lÃ½ memory issues

---

## ğŸ”„ Náº¿u Váº«n ChÆ°a Äáº¡t Má»¥c TiÃªu

### Plan B: Thu Tháº­p ThÃªm Data
- Annotate thÃªm 200-300 samples neutral
- TÄƒng tá»« 501 â†’ 700-800 samples
- ÄÃ¢y lÃ  giáº£i phÃ¡p **tá»‘t nháº¥t** nhÆ°ng tá»‘n cÃ´ng

### Plan C: Data Augmentation
- Synonym replacement cho neutral samples
- Back-translation (Viâ†’Enâ†’Vi)
- Paraphrasing

### Plan D: Ensemble
- Train nhiá»u models vá»›i random seeds khÃ¡c nhau
- Voting hoáº·c averaging predictions
- ThÆ°á»ng tÄƒng 0.5-1% accuracy

---

## âœ… TÃ³m Táº¯t

**ÄÃ£ lÃ m:**
1. âœ… Báº­t oversampling (neutral 30%)
2. âœ… Giáº£m epochs (5 â†’ 3)

**Cháº¡y ngay:**
```bash
python train.py
```

**Theo dÃµi:**
- Neutral F1 (má»¥c tiÃªu: >0.65)
- Eval loss (khÃ´ng tÄƒng)
- Overall accuracy (má»¥c tiÃªu: >92%)

**Náº¿u chÆ°a Ä‘á»§ â†’ Ãp dá»¥ng bÆ°á»›c 3-5 trong plan**

Good luck! ğŸš€
