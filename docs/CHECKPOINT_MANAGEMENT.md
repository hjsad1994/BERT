# Quáº£n LÃ½ Checkpoints vÃ  Best Model

## CÃ¢u há»i: Checkpoint cÃ³ bá»‹ ghi Ä‘Ã¨ khÃ´ng?

**VÃ­ dá»¥:** Accuracy 91 â†’ 92 â†’ 89, checkpoint-91 vÃ  checkpoint-92 cÃ³ bá»‹ máº¥t khÃ´ng?

## Tráº£ lá»i: CÃ“ vÃ  KHÃ”NG (tÃ¹y config)

---

## Config hiá»‡n táº¡i:

```yaml
save_total_limit: 3  # Giá»¯ 3 checkpoints gáº§n nháº¥t
load_best_model_at_end: true  # QUAN TRá»ŒNG!
metric_for_best_model: "eval_loss"
greater_is_better: false
```

---

## CÆ¡ cháº¿ hoáº¡t Ä‘á»™ng:

### 1. **LÆ°u Checkpoints (save_total_limit: 3)**

Training 4 epochs vá»›i eval_loss:

```
Epoch 1: eval_loss = 0.10 â†’ checkpoint-91 (best) âœ…
Epoch 2: eval_loss = 0.18 â†’ checkpoint-90 âœ…
Epoch 3: eval_loss = 0.23 â†’ checkpoint-89 âœ…

Äang cÃ³ 3 checkpoints â†’ Äáº¡t limit!

Epoch 4: eval_loss = 0.25 â†’ checkpoint-88 âœ…
â†’ XÃ³a checkpoint cÅ© nháº¥t (checkpoint-91) âŒ

Káº¿t quáº£: checkpoint-90, checkpoint-89, checkpoint-88
âŒ Máº¥t checkpoint-91 (best)!
```

**Váº¥n Ä‘á»:** Checkpoint tá»‘t nháº¥t cÃ³ thá»ƒ Bá»Š XÃ“A náº¿u training nhiá»u epochs!

---

### 2. **Load Best Model (load_best_model_at_end: true)** â­

**May máº¯n:** Trainer LUÃ”N track best checkpoint trong memory!

```python
# Trainer tá»± Ä‘á»™ng:
class Trainer:
    def train(self):
        # ... training loop
        
        # Track best checkpoint
        if eval_loss < self.best_metric:
            self.best_metric = eval_loss
            self.best_checkpoint = current_checkpoint  # Nhá»› best checkpoint
        
        # Khi training káº¿t thÃºc:
        if load_best_model_at_end:
            # Load láº¡i best checkpoint (dÃ¹ Ä‘Ã£ bá»‹ xÃ³a khá»i disk!)
            self.load_checkpoint(self.best_checkpoint)
            
            # Save best model vÃ o output_dir chÃ­nh
            self.save_model(output_dir)
```

**Káº¿t quáº£:**
```
Training káº¿t thÃºc â†’
Trainer load láº¡i best model (checkpoint-91) tá»« memory â†’
Save vÃ o finetuned_visobert_absa_model/ (thÆ° má»¥c gá»‘c) â†’
Evaluate vÃ  predict trÃªn best model âœ…
```

---

## So sÃ¡nh cÃ¡c chiáº¿n lÆ°á»£c:

| save_total_limit | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm | Khuyáº¿n nghá»‹ |
|------------------|---------|-----------|-------------|
| **1** | Tiáº¿t kiá»‡m disk | Chá»‰ giá»¯ checkpoint cuá»‘i (cÃ³ thá»ƒ khÃ´ng pháº£i best) | âŒ KhÃ´ng nÃªn |
| **2** | Tiáº¿t kiá»‡m disk | Dá»… máº¥t best checkpoint | âš ï¸ Rá»§i ro cao |
| **3** | CÃ¢n báº±ng | An toÃ n vá»›i early stopping (2 epochs) | âœ… Khuyáº¿n nghá»‹ |
| **5** | An toÃ n | Tá»‘n disk (nhÆ°ng khÃ´ng nhiá»u) | âœ… An toÃ n nháº¥t |
| **None** | Giá»¯ táº¥t cáº£ | Tá»‘n disk nhiá»u | ğŸ’¾ Náº¿u disk Ä‘á»§ |

---

## Cáº¥u trÃºc thÆ° má»¥c:

```
finetuned_visobert_absa_model/
â”œâ”€â”€ checkpoint-89/          # Checkpoint epoch 3
â”œâ”€â”€ checkpoint-90/          # Checkpoint epoch 2
â”œâ”€â”€ checkpoint-91/          # Checkpoint epoch 1 (best)
â”‚
â”œâ”€â”€ config.json             # â­ Best model (tá»« checkpoint-91)
â”œâ”€â”€ model.safetensors       # â­ Best model weights
â”œâ”€â”€ tokenizer.json          # Tokenizer
â””â”€â”€ ...
```

**LÆ°u Ã½:** 
- Files á»Ÿ thÆ° má»¥c gá»‘c = **best model** (load_best_model_at_end)
- Subfolders = checkpoints cá»§a tá»«ng epoch (cÃ³ thá»ƒ bá»‹ xÃ³a)

---

## Äáº£m báº£o evaluate trÃªn best model:

### âœ… CÃ¡ch 1: DÃ¹ng config hiá»‡n táº¡i (KHUYáº¾N NGHá»Š)

```yaml
load_best_model_at_end: true
save_total_limit: 3
```

â†’ Trainer tá»± Ä‘á»™ng load best model khi káº¿t thÃºc

### âœ… CÃ¡ch 2: TÄƒng save_total_limit

```yaml
save_total_limit: 5  # Hoáº·c None (giá»¯ táº¥t cáº£)
```

â†’ Giá»¯ nhiá»u checkpoints hÆ¡n, giáº£m nguy cÆ¡ máº¥t best

### âœ… CÃ¡ch 3: Manually load best checkpoint

```python
# Náº¿u lo láº¯ng, cÃ³ thá»ƒ manually load:
from transformers import AutoModelForSequenceClassification

# Load tá»« thÆ° má»¥c gá»‘c (best model)
model = AutoModelForSequenceClassification.from_pretrained(
    "finetuned_visobert_absa_model"
)

# Hoáº·c load tá»« checkpoint cá»¥ thá»ƒ
model = AutoModelForSequenceClassification.from_pretrained(
    "finetuned_visobert_absa_model/checkpoint-91"
)
```

---

## Káº¿t luáº­n:

### â“ Checkpoint cÃ³ bá»‹ ghi Ä‘Ã¨ khÃ´ng?

**Tráº£ lá»i:** 
- âŒ **Checkpoint folders cÃ³ thá»ƒ bá»‹ XÃ“A** (do save_total_limit)
- âœ… **Best model LUÃ”N Ä‘Æ°á»£c giá»¯** (nhá» load_best_model_at_end)
- âœ… **Evaluate LUÃ”N trÃªn best model** (tá»± Ä‘á»™ng)

### ğŸ¯ Khuyáº¿n nghá»‹:

```yaml
save_total_limit: 3-5  # An toÃ n
load_best_model_at_end: true  # Báº®T BUá»˜C!
early_stopping_patience: 2  # Dá»«ng sá»›m náº¿u khÃ´ng cáº£i thiá»‡n
```

â†’ **Äáº£m báº£o:** Best model Ä‘Æ°á»£c giá»¯ láº¡i vÃ  evaluate Ä‘Ãºng!

---

## Test thá»±c táº¿:

Sau khi training, kiá»ƒm tra:

```bash
# 1. Xem cÃ¡c checkpoints cÃ²n láº¡i
dir finetuned_visobert_absa_model

# 2. Load model tá»« thÆ° má»¥c gá»‘c (best model)
python
>>> from transformers import AutoModelForSequenceClassification
>>> model = AutoModelForSequenceClassification.from_pretrained("finetuned_visobert_absa_model")
>>> # Model nÃ y lÃ  BEST MODEL (eval_loss tháº¥p nháº¥t)
```

---

## Náº¿u váº«n lo láº¯ng:

### Option 1: TÄƒng save_total_limit lÃªn 10
```yaml
save_total_limit: 10
```

### Option 2: KhÃ´ng giá»›i háº¡n (giá»¯ táº¥t cáº£)
```yaml
save_total_limit: null
```

### Option 3: Manually backup best checkpoint
```bash
# Sau khi training xong
cp -r finetuned_visobert_absa_model/checkpoint-91 backup_best_checkpoint/
```

---

**TÃ³m láº¡i:** Config hiá»‡n táº¡i ÄÃƒ AN TOÃ€N nhá» `load_best_model_at_end: true`! Trainer sáº½ tá»± Ä‘á»™ng load vÃ  save best model, Ä‘áº£m báº£o evaluate Ä‘Ãºng káº¿t quáº£ tá»‘t nháº¥t. ğŸ¯
