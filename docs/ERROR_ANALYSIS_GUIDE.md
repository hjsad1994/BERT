# ğŸ” ERROR ANALYSIS GUIDE

## ğŸ“– HÆ°á»›ng Dáº«n PhÃ¢n TÃ­ch Lá»—i Model

---

## ğŸ¯ Má»¤C ÄÃCH

Error Analysis giÃºp báº¡n:

1. âœ… **TÃ¬m patterns** trong cÃ¡c predictions sai
2. âœ… **PhÃ¡t hiá»‡n aspects yáº¿u** cáº§n cáº£i thiá»‡n
3. âœ… **PhÃ¢n tÃ­ch confusion** giá»¯a cÃ¡c sentiment classes
4. âœ… **TÃ¬m hard cases** (cÃ¢u khÃ³) Ä‘á»ƒ cáº£i thiá»‡n data
5. âœ… **Äá» xuáº¥t cáº£i thiá»‡n** cá»¥ thá»ƒ dá»±a trÃªn phÃ¢n tÃ­ch

---

## ğŸš€ CÃCH Sá»¬ Dá»¤NG

### BÆ°á»›c 1: Äáº£m báº£o cÃ³ predictions

Kiá»ƒm tra xem Ä‘Ã£ cÃ³ file `test_predictions.csv` chÆ°a:

```bash
# Náº¿u chÆ°a cÃ³, cháº¡y inference trÆ°á»›c
python analyze_results.py
```

Hoáº·c náº¿u Ä‘Ã£ train xong, file nÃ y sáº½ tá»± Ä‘á»™ng Ä‘Æ°á»£c táº¡o.

---

### BÆ°á»›c 2: Cháº¡y Error Analysis

```bash
python error_analysis.py
```

**Output:**
- Terminal sáº½ hiá»ƒn thá»‹ phÃ¢n tÃ­ch chi tiáº¿t
- CÃ¡c files Ä‘Æ°á»£c lÆ°u trong folder `error_analysis_results/`

---

### BÆ°á»›c 3: Xem káº¿t quáº£

```bash
cd error_analysis_results
```

**Files Ä‘Æ°á»£c táº¡o:**

```
error_analysis_results/
â”œâ”€â”€ aspect_error_analysis.csv          â† Error rate theo aspect
â”œâ”€â”€ sentiment_error_analysis.csv       â† Error rate theo sentiment
â”œâ”€â”€ confusion_patterns.csv             â† Confusion pairs chi tiáº¿t
â”œâ”€â”€ hard_cases.csv                     â† CÃ¡c cÃ¢u khÃ³ nháº¥t
â”œâ”€â”€ improvement_suggestions.txt        â† Äá» xuáº¥t cáº£i thiá»‡n
â”œâ”€â”€ error_analysis_report.txt          â† Report tá»•ng há»£p
â”œâ”€â”€ aspect_error_rates.png             â† Visualization
â”œâ”€â”€ confusion_matrix.png               â† Confusion matrix
â””â”€â”€ sentiment_error_rates.png          â† Error rates by sentiment
```

---

## ğŸ“Š PHÃ‚N TÃCH CHI TIáº¾T

### 1. Aspect Error Analysis

**File:** `aspect_error_analysis.csv`

**Ná»™i dung:**

```
Aspect          Total   Correct  Errors   Accuracy   Error Rate
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Performance     120     98       22       81.67%     18.33%  â† Yáº¿u!
Neutral         65      48       17       73.85%     26.15%  â† Ráº¥t yáº¿u!
Camera          95      88       7        92.63%     7.37%   â† Tá»‘t!
Battery         110     105      5        95.45%     4.55%   â† Ráº¥t tá»‘t!
```

**CÃ¡ch Ä‘á»c:**
- **Error Rate cao** (>15%): Aspect yáº¿u, cáº§n cáº£i thiá»‡n
- **Error Rate tháº¥p** (<10%): Aspect máº¡nh
- **Errors column**: Sá»‘ lÆ°á»£ng predictions sai

**Action:**
1. TÃ¬m aspects cÃ³ Error Rate > 15%
2. Xem `hard_cases.csv` Ä‘á»ƒ hiá»ƒu táº¡i sao sai
3. CÃ¢n nháº¯c:
   - Thu tháº­p thÃªm data cho aspects yáº¿u
   - Review labeling quality
   - ThÃªm keywords/features Ä‘áº·c trÆ°ng

---

### 2. Sentiment Error Analysis

**File:** `sentiment_error_analysis.csv`

**Ná»™i dung:**

```
Sentiment    Total   Correct  Errors   Accuracy   Error Rate
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Negative     666     625      41       93.84%     6.16%   â† Tá»‘t
Positive     426     398      28       93.43%     6.57%   â† Tá»‘t
Neutral      65      48       17       73.85%     26.15%  â† Yáº¿u!
```

**Váº¥n Ä‘á» phá»• biáº¿n:**

**Neutral class yáº¿u nháº¥t vÃ¬:**
- âŒ Imbalanced data (chá»‰ 5.7%)
- âŒ Subjective labeling
- âŒ Boundary khÃ´ng rÃµ rÃ ng

**ÄÃ£ Ã¡p dá»¥ng:**
- âœ… Focal Loss (Î³=2.0)
- âœ… Oversampling (305 â†’ 1,244)
- âœ… Class weights

**CÃ³ thá»ƒ cáº£i thiá»‡n thÃªm:**
- ğŸ”§ TÄƒng oversampling ratio (40% â†’ 50%)
- ğŸ”§ TÄƒng Focal Loss gamma (2.0 â†’ 3.0)
- ğŸ”§ SMOTE thay vÃ¬ random oversampling
- ğŸ”§ Thu tháº­p thÃªm neutral samples cháº¥t lÆ°á»£ng

---

### 3. Confusion Patterns

**File:** `confusion_patterns.csv`

**VÃ­ dá»¥:**

```
CONFUSION PAIRS (Nháº§m gÃ¬ thÃ nh gÃ¬):

True         â†’  Predicted    Count   % of Errors
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
neutral      â†’  positive     25      29.1%  â† Confusion nhiá»u nháº¥t!
neutral      â†’  negative     18      20.9%
positive     â†’  neutral      12      14.0%
negative     â†’  neutral      10      11.6%
positive     â†’  negative     8       9.3%
negative     â†’  positive     7       8.1%
```

**CÃ¡ch Ä‘á»c:**

**Pattern 1: `neutral â†’ positive` (29.1%)**
- Model cÃ³ **positive bias**
- CÃ¡c neutral samples bá»‹ nháº§m thÃ nh positive
- **NguyÃªn nhÃ¢n:** Data imbalance, positive class nhiá»u hÆ¡n
- **Giáº£i phÃ¡p:**
  - TÄƒng alpha weight cho neutral trong Focal Loss
  - Review cÃ¡c neutral samples cÃ³ tá»« tÃ­ch cá»±c
  - ThÃªm negative keywords cho neutral detection

**Pattern 2: `positive â†’ negative` hoáº·c `negative â†’ positive`**
- Confusion **NGHIÃŠM TRá»ŒNG** (ngÆ°á»£c hoÃ n toÃ n!)
- **NguyÃªn nhÃ¢n cÃ³ thá»ƒ:**
  - Sarcasm/irony ("Tá»‘t láº¯m! Pin háº¿t sau 2 tiáº¿ng")
  - Context phá»©c táº¡p
  - Labeling errors
- **Giáº£i phÃ¡p:**
  - Manual review cÃ¡c cases nÃ y
  - Check data quality
  - Xem xÃ©t thÃªm context features

---

### 4. Confusion by Aspect

**VÃ­ dá»¥:**

```
CONFUSION PATTERNS BY ASPECT:

Performance:
  â€¢ neutral â†’ positive: 8 cases
  â€¢ positive â†’ neutral: 5 cases
  â€¢ negative â†’ neutral: 3 cases

Camera:
  â€¢ neutral â†’ negative: 4 cases
  â€¢ positive â†’ neutral: 2 cases

Battery:
  â€¢ neutral â†’ positive: 6 cases
```

**Insights:**

**Performance aspect:**
- Neutral bá»‹ nháº§m positive nhiá»u nháº¥t
- â†’ CÃ³ thá»ƒ tá»« "mÆ°á»£t", "nhanh" gÃ¢y positive bias
- â†’ Cáº§n distinguish giá»¯a "mÆ°á»£t" (positive) vs "bÃ¬nh thÆ°á»ng" (neutral)

**Camera aspect:**
- Neutral bá»‹ nháº§m negative
- â†’ CÃ³ thá»ƒ tá»« "táº¡m", "ok" bá»‹ hiá»ƒu nháº§m
- â†’ Cáº§n balance positive/negative keywords

---

### 5. Hard Cases

**File:** `hard_cases.csv`

**Format:**

```csv
sentence,aspect,true_sentiment,predicted_sentiment,confusion_type
"Pin trÃ¢u nhÆ°ng khÃ´ng áº¥n tÆ°á»£ng",Battery,neutral,positive,neutral â†’ positive
"Camera táº¡m á»•n cho táº§m giÃ¡",Camera,neutral,negative,neutral â†’ negative
"MÃ¡y mÆ°á»£t nhÆ°ng khÃ´ng xuáº¥t sáº¯c",Performance,neutral,positive,neutral â†’ positive
```

**CÃ¡ch phÃ¢n tÃ­ch:**

#### Case 1: "Pin trÃ¢u nhÆ°ng khÃ´ng áº¥n tÆ°á»£ng"

**Aspect:** Battery  
**True:** Neutral  
**Predicted:** Positive  

**Táº¡i sao sai:**
- Tá»« "trÃ¢u" = positive keyword máº¡nh
- "KhÃ´ng áº¥n tÆ°á»£ng" = neutral/negative BUT model focus "trÃ¢u"
- â†’ Model thiÃªn vá» positive vÃ¬ "trÃ¢u" quÃ¡ máº¡nh

**CÃ¡ch fix:**
- ThÃªm training samples vá»›i pattern "X tá»‘t nhÆ°ng Y"
- Teach model vá» adversative conjunctions ("nhÆ°ng", "tuy nhiÃªn")
- Balance attention mechanism

#### Case 2: "Camera táº¡m á»•n cho táº§m giÃ¡"

**Aspect:** Camera  
**True:** Neutral  
**Predicted:** Negative  

**Táº¡i sao sai:**
- "Táº¡m á»•n" = neutral phrase NHÆ¯NG cÃ³ negative connotation
- "Cho táº§m giÃ¡" = qualifying phrase
- â†’ Model focus "táº¡m" = not good enough = negative

**CÃ¡ch fix:**
- Label clarification: "táº¡m á»•n" lÃ  neutral hay negative?
- Add more context-aware samples
- Teach model vá» qualifying phrases

---

## ğŸ’¡ Äá»€ XUáº¤T Cáº¢I THIá»†N

### Priority 1: Aspects Yáº¿u (High Error Rate)

**Náº¿u cÃ³ aspects error rate > 15%:**

```
ğŸ“ Performance (Error Rate: 18.33%)
   Action:
   1. Thu tháº­p thÃªm ~44 samples (2x sá»‘ errors)
   2. Review labeling quality
   3. ThÃªm keywords: "mÆ°á»£t", "lag", "giáº­t", "nhanh", "cháº­m"
   4. Check confusion patterns: neutral â†’ positive?
```

**Implementation:**

```python
# 1. Check current data
performance_samples = df[df['aspect'] == 'Performance']
print(f"Current: {len(performance_samples)} samples")

# 2. Check sentiment distribution
print(performance_samples['sentiment'].value_counts())

# 3. Identify if imbalanced â†’ apply oversampling
# 4. Collect more data tá»« specific sources
```

---

### Priority 2: Neutral Class

**Problem:** Error rate = 26.15%

**ÄÃ£ lÃ m:**
- âœ… Focal Loss (Î³=2.0)
- âœ… Oversampling (305 â†’ 1,244, 40% of majority)
- âœ… Alpha weights: [0.54, 0.35, 3.52]

**Cáº§n lÃ m thÃªm:**

#### Option A: TÄƒng Oversampling

```python
# config.yaml hoáº·c trong train.py
# Current: 40% of majority
target_neutral_count = int(majority_count * 0.4)

# New: 50% of majority
target_neutral_count = int(majority_count * 0.5)
```

#### Option B: TÄƒng Focal Loss Gamma

```python
# train.py, line ~316
# Current: gamma = 2.0
gamma = 2.0

# New: gamma = 3.0 (focus hÆ¡n vÃ o hard examples)
gamma = 3.0
```

#### Option C: SMOTE (Synthetic Oversampling)

```bash
pip install imbalanced-learn

# Táº¡o synthetic samples thay vÃ¬ duplicate
from imblearn.over_sampling import SMOTE
```

---

### Priority 3: Confusion Patterns

**Pattern:** `neutral â†’ positive` (29.1% of errors)

**Root cause:** Positive bias

**Solutions:**

#### 1. Äiá»u chá»‰nh Class Weights

```python
# train.py
# Current alpha (auto-calculated)
alpha = [0.54, 0.35, 3.52]  # [pos, neg, neutral]

# Manual adjustment: Boost neutral hÆ¡n ná»¯a
alpha = [0.54, 0.35, 4.50]  # TÄƒng neutral weight
```

#### 2. Threshold Adjustment

```python
# test_sentiment_smart.py
# Add confidence threshold adjustment for neutral

if predicted_sentiment == 'neutral':
    # Require higher confidence for neutral
    if confidence < 0.75:  # Instead of 0.70
        continue
```

#### 3. Balanced Sampling

```python
# Ensure equal representation during training
from torch.utils.data import WeightedRandomSampler

# Calculate sample weights
class_counts = Counter(train_df['sentiment'])
weights = [1.0/class_counts[s] for s in train_df['sentiment']]

sampler = WeightedRandomSampler(weights, len(weights))

# Use in DataLoader
train_loader = DataLoader(train_dataset, sampler=sampler, ...)
```

---

### Priority 4: Data Quality

**Action items:**

#### 1. Review Labeling Guidelines

```
Positive: RÃµ rÃ ng tá»‘t, khÃ´ng cÃ³ nhÆ°á»£c Ä‘iá»ƒm Ä‘Ã¡ng ká»ƒ
  âœ“ "Pin trÃ¢u láº¯m"
  âœ“ "Camera Ä‘áº¹p"
  
Negative: RÃµ rÃ ng tá»‡, khÃ´ng cÃ³ Æ°u Ä‘iá»ƒm Ä‘Ã¡ng ká»ƒ
  âœ“ "Pin tá»‡ quÃ¡"
  âœ“ "Camera má»"
  
Neutral: Trung bÃ¬nh HOáº¶C cÃ³ cáº£ Æ°u vÃ  nhÆ°á»£c Ä‘iá»ƒm
  âœ“ "Pin táº¡m á»•n"
  âœ“ "Camera bÃ¬nh thÆ°á»ng"
  âœ“ "Pin tá»‘t nhÆ°ng camera tá»‡" (náº¿u nÃ³i vá» cáº£ 2)
```

#### 2. Inter-Annotator Agreement

```python
# Check labeling consistency
# CÃ³ thá»ƒ dÃ¹ng Cohen's Kappa, Fleiss' Kappa

from sklearn.metrics import cohen_kappa_score

# If cÃ³ multiple annotators
kappa = cohen_kappa_score(annotator1_labels, annotator2_labels)
print(f"Kappa: {kappa:.3f}")

# Kappa > 0.8: Excellent
# Kappa 0.6-0.8: Good
# Kappa < 0.6: Need improvement
```

#### 3. Hard Cases Manual Review

```bash
# Má»Ÿ file hard_cases.csv
# Manual review tá»«ng case
# Re-label náº¿u cáº§n
# Add to training data vá»›i label má»›i
```

---

## ğŸ“ˆ MONITORING IMPROVEMENTS

### After Implementing Changes

**1. Re-train model:**

```bash
python train.py
```

**2. Re-run Error Analysis:**

```bash
python error_analysis.py
```

**3. Compare metrics:**

```python
# Before:
Neutral Error Rate: 26.15%
Confusion (neutral â†’ positive): 29.1%

# After (Expected):
Neutral Error Rate: 18-20%  â† Target
Confusion (neutral â†’ positive): 20-22%  â† Reduced
```

**4. Track progress:**

```bash
# Create comparison report
echo "Iteration 1: Neutral Error Rate = 26.15%" >> progress.txt
echo "Iteration 2: Neutral Error Rate = 20.30%" >> progress.txt
```

---

## ğŸ¯ SUCCESS METRICS

### Good Performance:

```
âœ“ Overall Accuracy: > 90%
âœ“ Per-aspect Error Rate: < 10%
âœ“ Neutral Class Error Rate: < 15%
âœ“ Confusion Rate: < 20% for any pair
```

### Current Status:

```
âœ“ Overall Accuracy: 91.1%  âœ…
âœ— Worst Aspect: 18.33%     âš ï¸  Need improvement
âœ— Neutral: 26.15%          âŒ  Need significant improvement
âœ“ Best Aspect: 4.55%       âœ…  Excellent
```

---

## ğŸ› ï¸ ADVANCED TECHNIQUES

### 1. Error-Driven Data Augmentation

```python
# Generate similar samples to hard cases
from nlpaug.augmenter.word import SynonymAug

aug = SynonymAug(aug_src='wordnet')

# For each hard case
for sentence in hard_cases['sentence']:
    # Generate variations
    augmented = aug.augment(sentence, n=3)
    
    # Label manually
    # Add to training data
```

### 2. Active Learning

```python
# Select most uncertain samples for human labeling
uncertainty = []

for sample in unlabeled_data:
    probs = model.predict_proba(sample)
    entropy = -sum(p * log(p) for p in probs)
    uncertainty.append((sample, entropy))

# Sort by entropy (high = uncertain)
most_uncertain = sorted(uncertainty, key=lambda x: -x[1])[:100]

# Send to human annotators
```

### 3. Ensemble Methods

```python
# Train multiple models vá»›i different seeds/configs
models = [
    train_model(seed=42),
    train_model(seed=123),
    train_model(seed=456),
]

# Ensemble predictions
def ensemble_predict(sentence, aspect):
    predictions = [m.predict(sentence, aspect) for m in models]
    # Majority voting hoáº·c averaging
    return most_common(predictions)
```

---

## ğŸ“š TÃ€I LIá»†U THAM KHáº¢O

1. **Error Analysis:**
   - Manning & SchÃ¼tze "Foundations of Statistical NLP"
   - Wu et al. "Errudite: Scalable Error Analysis" (ACL 2019)

2. **Class Imbalance:**
   - He & Garcia "Learning from Imbalanced Data" (2009)
   - Chawla et al. "SMOTE" (2002)

3. **Model Improvement:**
   - Howard & Ruder "Universal Language Model Fine-tuning" (2018)
   - Devlin et al. "BERT" (2018)

---

## ğŸ‰ Káº¾T LUáº¬N

**Error Analysis giÃºp:**

1. âœ… **Hiá»ƒu** model Ä‘ang sai á»Ÿ Ä‘Ã¢u
2. âœ… **PhÃ¡t hiá»‡n** patterns trong errors
3. âœ… **Æ¯u tiÃªn** aspects/classes cáº§n cáº£i thiá»‡n
4. âœ… **Äá» xuáº¥t** actions cá»¥ thá»ƒ
5. âœ… **Track** progress qua iterations

**Next Steps:**

```bash
# 1. Run Error Analysis
python error_analysis.py

# 2. Review results
cd error_analysis_results
cat improvement_suggestions.txt

# 3. Implement improvements
# - Adjust hyperparameters
# - Collect more data
# - Fix labeling issues

# 4. Re-train
python train.py

# 5. Compare
python error_analysis.py

# 6. Iterate until satisfied!
```

---

**ğŸš€ Good luck improving your model!**
