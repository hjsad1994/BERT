# Multi-Label Aspect Detection Configuration
# ========================================================================
# Aspect Detection: Detect which aspects are mentioned in the text
# Output: 11 binary predictions (one per aspect)
# Each aspect is detected independently: 0 = not mentioned, 1 = mentioned
# 
# Base Model: 5CD-AI/visobert-14gb-corpus
# - Pre-trained on 14GB Vietnamese corpus
# - We fine-tune this model for aspect detection task (binary classification)
# ========================================================================

paths:
  data_dir: "aspect-detection/data"
  train_file: "aspect-detection/data/train_multilabel.csv"
  validation_file: "aspect-detection/data/validation_multilabel.csv"
  test_file: "aspect-detection/data/test_multilabel.csv"
  output_dir: "aspect-detection/models/aspect_detection"
  evaluation_report: "aspect-detection/results/evaluation_report.txt"
  predictions_file: "aspect-detection/results/test_predictions.csv"

model:
  name: "5CD-AI/visobert-14gb-corpus"  # Pre-trained model on 14GB Vietnamese corpus
  # Note: Fine-tuned for aspect detection (binary classification: aspect present or not)
  num_labels: 11  # 11 aspects = 11 binary outputs (0 = not mentioned, 1 = mentioned)
  num_aspects: 11
  max_length: 256
  
  # Model architecture
  hidden_size: 512      # Dense layer
  dropout: 0.3

valid_aspects:
  - Battery
  - Camera
  - Performance
  - Display
  - Design
  - Packaging
  - Price
  - Shop_Service
  - Shipping
  - General
  - Others

# Aspect Detection: Binary labels
# 0 = aspect not mentioned in text
# 1 = aspect mentioned in text
aspect_labels:
  not_mentioned: 0
  mentioned: 1

training:
  # Batch size for multi-label (larger samples)
  # per_device_train_batch_size: 32
  # per_device_eval_batch_size: 64
  # gradient_accumulation_steps: 2  # Effective batch = 64
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 4  
  # Optimizer settings
  optim: "adamw_bnb_8bit"
  learning_rate: 2.0e-5
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8
  max_grad_norm: 1.0
  
  # Learning rate scheduler
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.06
  
  # Training duration
  num_train_epochs: 10  # More epochs for contrastive learning
  
  # Mixed precision (RTX 3070 Ampere)
  fp16: true
  fp16_opt_level: "O2"
  fp16_full_eval: false
  tf32: true
  
  # DataLoader settings
  dataloader_num_workers: 2  # Reduced for larger batches
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 4
  dataloader_persistent_workers: true
  
  # Memory optimization
  gradient_checkpointing: false
  auto_find_batch_size: false
  group_by_length: false
  
  # Evaluation & checkpointing
  evaluation_strategy: "epoch"
  save_strategy: "epoch"
  save_total_limit: 4
  load_best_model_at_end: true
  metric_for_best_model: "eval_f1"
  greater_is_better: true
  
  # Early stopping
  early_stopping_patience: 5
  early_stopping_threshold: 0.001
  
  # Logging
  logging_strategy: "steps"
  logging_steps: 50
  logging_first_step: true
  report_to: []
  
  # Misc
  disable_tqdm: false
  prediction_loss_only: false
  remove_unused_columns: true
  label_names: ["labels"]
  include_inputs_for_metrics: false

general:
  device: "auto"
  log_level: "info"

# ========================================================================
# REPRODUCIBILITY: Master Seed Configuration
# ========================================================================
# All random operations use seeds derived from master_seed for consistency
# Change master_seed to run different experiments (e.g., 42, 123, 456, 789, 2024)
# ========================================================================
reproducibility:
  master_seed: 42                 # Master seed for all operations
  
  # Data preprocessing seeds
  data_split_seed: 99              # Seed for train/val/test split
  oversampling_seed: 99            # Seed for aspect-wise oversampling
  shuffle_seed: 99               # Seed for data shuffling
  
  # Model training seeds
  training_seed: 99                # Seed for PyTorch/Transformers training
  dataloader_seed: 99             # Seed for DataLoader worker initialization
  
  # Notes:
  # - All seeds set to 42 by default for reproducibility
  # - For multiple runs, change master_seed: [42, 123, 456, 789, 2024]
  # - Recommended: Keep all seeds = master_seed unless specific reason

# Multi-label aspect detection settings
aspect_detection:
  # Loss settings
  use_bce_loss: true        # Binary Cross-Entropy Loss for binary classification
  use_focal_loss: false     # Can use Focal Loss for imbalanced binary data
  focal_gamma: 2.0           # Focusing parameter (if using Focal Loss)
  focal_alpha: "auto"       # Auto class weights from data
  
  # Data settings
  use_balanced_data: false  # Aspect detection usually doesn't need oversampling
  balance_method: "none"    # No balancing needed
  
  # Inference settings
  batch_prediction: true    # Predict all 11 aspects in one pass
  use_class_weights: true   # Apply class weights in loss (for imbalanced data)
  prediction_threshold: 0.5 # Default threshold for binary prediction
  
  # Per-aspect thresholds (auto-loaded from optimal_thresholds.json if exists)
  use_per_aspect_thresholds: true  # Use optimal thresholds per aspect (if optimal_thresholds.json exists)
  
  # Per-aspect thresholds (default values, will be overridden by optimal_thresholds.json if exists)
  # Run find_optimal_thresholds.py to find optimal thresholds for your model
  per_aspect_thresholds:
    Battery: 0.5
    Camera: 0.5
    Performance: 0.5
    Display: 0.5
    Design: 0.5
    Packaging: 0.5
    Price: 0.5
    Shop_Service: 0.5
    Shipping: 0.5
    General: 0.5
    Others: 0.5
