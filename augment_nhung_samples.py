"""
Data Augmentation cho cÃ¡c samples cÃ³ tá»« chuyá»ƒn Ã½ "nhÆ°ng"
Oversampling vÃ  táº¡o thÃªm variants Ä‘á»ƒ model há»c tá»‘t hÆ¡n
"""

import sys
import io
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

import pandas as pd
import os
import random

def augment_nhung_samples(train_file='data/train.csv', output_file='data/train_augmented_nhung.csv', oversample_factor=2):
    """
    Augment training data vá»›i oversampling cho samples cÃ³ 'nhÆ°ng'
    
    Args:
        train_file: File training data gá»‘c
        output_file: File output sau augmentation
        oversample_factor: Sá»‘ láº§n nhÃ¢n báº£n samples cÃ³ 'nhÆ°ng' (máº·c Ä‘á»‹nh: 3x)
    """
    print("="*80)
    print("ğŸ”„ DATA AUGMENTATION CHO SAMPLES CÃ“ 'NHÆ¯NG'")
    print("="*80)
    
    os.chdir('D:/BERT')
    
    # Load training data
    if not os.path.exists(train_file):
        print(f"âŒ File khÃ´ng tá»“n táº¡i: {train_file}")
        return
    
    df = pd.read_csv(train_file, encoding='utf-8-sig')
    
    print(f"\nğŸ“Š Training data gá»‘c: {len(df)} samples")
    
    # Find samples with "nhÆ°ng"
    nhung_samples = df[df['sentence'].str.contains('nhÆ°ng', case=False, na=False)]
    non_nhung_samples = df[~df['sentence'].str.contains('nhÆ°ng', case=False, na=False)]
    
    print(f"ğŸ“Š Samples cÃ³ 'nhÆ°ng': {len(nhung_samples)} ({len(nhung_samples)/len(df)*100:.1f}%)")
    print(f"ğŸ“Š Samples khÃ´ng cÃ³ 'nhÆ°ng': {len(non_nhung_samples)} ({len(non_nhung_samples)/len(df)*100:.1f}%)")
    
    # Oversample nhung samples
    print(f"\nğŸ”„ Äang oversample samples cÃ³ 'nhÆ°ng' (x{oversample_factor})...")
    
    oversampled_nhung = pd.concat([nhung_samples] * oversample_factor, ignore_index=True)
    
    # Combine
    augmented_df = pd.concat([non_nhung_samples, oversampled_nhung], ignore_index=True)
    
    # Shuffle
    augmented_df = augmented_df.sample(frac=1, random_state=42).reset_index(drop=True)
    
    print(f"âœ“ Augmented data: {len(augmented_df)} samples")
    print(f"âœ“ TÄƒng thÃªm: {len(augmented_df) - len(df)} samples (+{(len(augmented_df) - len(df))/len(df)*100:.1f}%)")
    
    # Save
    augmented_df.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"\nâœ“ Saved augmented data to: {output_file}")
    
    # Statistics
    print(f"\n{'='*80}")
    print("ğŸ“Š THá»NG KÃŠ AUGMENTED DATA:")
    print(f"{'='*80}")
    
    new_nhung_samples = augmented_df[augmented_df['sentence'].str.contains('nhÆ°ng', case=False, na=False)]
    print(f"   â€¢ Tá»•ng samples: {len(augmented_df)}")
    print(f"   â€¢ Samples cÃ³ 'nhÆ°ng': {len(new_nhung_samples)} ({len(new_nhung_samples)/len(augmented_df)*100:.1f}%)")
    print(f"   â€¢ Samples khÃ´ng cÃ³ 'nhÆ°ng': {len(augmented_df) - len(new_nhung_samples)} ({(len(augmented_df) - len(new_nhung_samples))/len(augmented_df)*100:.1f}%)")
    
    # Sentiment distribution
    print(f"\nğŸ“Š PHÃ‚N Bá» SENTIMENT:")
    sentiment_dist = augmented_df['sentiment'].value_counts()
    for sent, count in sentiment_dist.items():
        print(f"   â€¢ {sent:<10}: {count:>5} samples ({count/len(augmented_df)*100:.1f}%)")
    
    # Aspect distribution
    print(f"\nğŸ“Š TOP 10 ASPECTS:")
    aspect_dist = augmented_df['aspect'].value_counts().head(10)
    for asp, count in aspect_dist.items():
        print(f"   â€¢ {asp:<15}: {count:>5} samples ({count/len(augmented_df)*100:.1f}%)")
    
    print(f"\n{'='*80}")
    print("ğŸ¯ NEXT STEPS:")
    print(f"{'='*80}")
    print("\n1. Update config.yaml Ä‘á»ƒ sá»­ dá»¥ng augmented data:")
    print("   train_file: data/train_augmented_nhung.csv")
    print("\n2. Retrain model:")
    print("   python train.py")
    print("\n3. So sÃ¡nh performance:")
    print("   - TrÆ°á»›c: ~79.57% accuracy trÃªn cÃ¢u cÃ³ 'nhÆ°ng'")
    print("   - Má»¥c tiÃªu: TÄƒng lÃªn ~85-88% (gáº§n vá»›i overall 91.34%)")
    print(f"\n{'='*80}\n")
    
    return augmented_df


def create_advanced_augmentation(train_file='data/train.csv', output_file='data/train_augmented_nhung_advanced.csv'):
    """
    Advanced augmentation vá»›i cÃ¡c ká»¹ thuáº­t phá»©c táº¡p hÆ¡n
    """
    print("="*80)
    print("ğŸ”„ ADVANCED DATA AUGMENTATION CHO SAMPLES CÃ“ 'NHÆ¯NG'")
    print("="*80)
    
    os.chdir('D:/BERT')
    
    # Load training data
    if not os.path.exists(train_file):
        print(f"âŒ File khÃ´ng tá»“n táº¡i: {train_file}")
        return
    
    df = pd.read_csv(train_file, encoding='utf-8-sig')
    
    print(f"\nğŸ“Š Training data gá»‘c: {len(df)} samples")
    
    # Find samples with adversative conjunctions
    adversative_words = ['nhÆ°ng', 'tuy nhiÃªn', 'máº·c dÃ¹', 'song', 'nhÆ°ng mÃ ', 'tuy váº­y']
    
    pattern = '|'.join(adversative_words)
    adv_samples = df[df['sentence'].str.contains(pattern, case=False, na=False)]
    non_adv_samples = df[~df['sentence'].str.contains(pattern, case=False, na=False)]
    
    print(f"ğŸ“Š Samples cÃ³ tá»« chuyá»ƒn Ã½: {len(adv_samples)} ({len(adv_samples)/len(df)*100:.1f}%)")
    
    # Strategy 1: Oversample 2x
    oversampled = pd.concat([adv_samples] * 2, ignore_index=True)
    
    # Strategy 2: Add [ADV] marker (optional - comment out if not using)
    # for idx, row in oversampled.iterrows():
    #     for word in adversative_words:
    #         if word in row['sentence'].lower():
    #             oversampled.at[idx, 'sentence'] = row['sentence'].replace(word, f"[ADV] {word}")
    #             break
    
    # Combine
    augmented_df = pd.concat([non_adv_samples, oversampled], ignore_index=True)
    augmented_df = augmented_df.sample(frac=1, random_state=42).reset_index(drop=True)
    
    print(f"âœ“ Advanced augmented data: {len(augmented_df)} samples")
    print(f"âœ“ TÄƒng thÃªm: {len(augmented_df) - len(df)} samples (+{(len(augmented_df) - len(df))/len(df)*100:.1f}%)")
    
    # Save
    augmented_df.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"\nâœ“ Saved advanced augmented data to: {output_file}")
    
    return augmented_df


if __name__ == '__main__':
    # Basic augmentation (recommended)
    print("\nğŸ¯ OPTION 1: BASIC AUGMENTATION (Khuyáº¿n nghá»‹)")
    print("-"*80)
    augment_nhung_samples(oversample_factor=2)
    
    print("\n\n")
    
    # Advanced augmentation (experimental)
    print("ğŸ¯ OPTION 2: ADVANCED AUGMENTATION (Thá»­ nghiá»‡m)")
    print("-"*80)
    create_advanced_augmentation()
    
    print("\n\n")
    print("="*80)
    print("âœ… HOÃ€N Táº¤T DATA AUGMENTATION!")
    print("="*80)
    print("\nğŸ’¡ Chá»n má»™t trong hai file Ä‘á»ƒ train:")
    print("   1. data/train_augmented_nhung.csv (basic - khuyáº¿n nghá»‹)")
    print("   2. data/train_augmented_nhung_advanced.csv (advanced - thá»­ nghiá»‡m)")
    print("\nUpdate config.yaml rá»“i cháº¡y:")
    print("   python train.py")
    print("\n" + "="*80 + "\n")
