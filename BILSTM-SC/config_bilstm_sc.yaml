# BiLSTM + CNN Sentiment Classification Configuration
# ========================================================================
# Single Task Learning: Sentiment Classification Only
# 
# Task: Multi-class classification (3 classes: Positive, Negative, Neutral)
# Model: BiLSTM + Conv1D + GlobalPooling (NO pretrained embeddings)
# Embeddings: Trainable embeddings (random initialization)
# Architecture: Embedding → SpatialDropout → BiLSTM → Conv1D → Pooling → Dense
# ========================================================================

paths:
  data_dir: "data"
  train_file: "data/train_multilabel.csv"
  validation_file: "data/validation_multilabel.csv"
  test_file: "data/test_multilabel.csv"
  output_dir: "models/bilstm_sentiment_classification"
  evaluation_report: "results/evaluation_report_bilstm_sc.txt"
  predictions_file: "results/test_predictions_bilstm_sc.csv"

model:
  # Tokenizer (for text → token IDs, NOT for embeddings)
  tokenizer_name: "5CD-AI/visobert-14gb-corpus"  # Use tokenizer only, not embeddings
  
  # Trainable embeddings (NO pretrained model)
  vocab_size: 30000         # Vocabulary size (will be set from tokenizer)
  embedding_dim: 300        # Embedding dimension (trainable)
  padding_idx: 0            # Padding token index
  
  # BiLSTM architecture
  lstm_hidden_size: 256     # Hidden size of BiLSTM
  lstm_num_layers: 2        # Number of BiLSTM layers
  lstm_dropout: 0.3         # Dropout for BiLSTM (only if num_layers > 1)
  
  # Spatial Dropout
  spatial_dropout: 0.2      # Spatial dropout rate for embeddings
  
  # Conv1D layer
  conv_filters: 128         # Number of Conv1D filters
  conv_kernel_size: 3       # Kernel size for Conv1D
  
  # Dense layers
  dense_hidden_size: 256    # Hidden size of dense layer
  dense_dropout: 0.3        # Dropout for dense layer
  
  # Task settings
  num_aspects: 11           # 11 aspects
  num_sentiments: 3         # Positive, Negative, Neutral
  max_length: 256

aspect_names:
  - Battery
  - Camera
  - Performance
  - Display
  - Design
  - Packaging
  - Price
  - Shop_Service
  - Shipping
  - General
  - Others

sentiment_labels:
  - Positive
  - Negative
  - Neutral

training:
  # Batch sizes
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 64
  gradient_accumulation_steps: 2  # Effective batch = 64
  
  # Optimizer settings
  learning_rate: 3.0e-4     # Learning rate for pure BiLSTM with trainable embeddings
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Learning rate scheduler
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  
  # Training duration
  num_train_epochs: 40      # More epochs for BiLSTM
  
  # Mixed precision (RTX 3070)
  fp16: true
  
  # DataLoader settings
  dataloader_num_workers: 2
  dataloader_pin_memory: true
  
  # Evaluation & checkpointing
  evaluation_strategy: "epoch"
  save_strategy: "epoch"
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "eval_accuracy"
  greater_is_better: true
  
  # Early stopping
  early_stopping_patience: 7
  early_stopping_threshold: 0.001
  
  # Logging
  logging_strategy: "steps"
  logging_steps: 50
  logging_first_step: true
  
  # Device
  device: "cuda"  # or "cpu"

# Loss function settings
loss:
  # CrossEntropyLoss settings
  use_class_weight: true    # Use class weights for imbalance
  class_weight_auto: true   # Auto-calculate class weights from data

# Reproducibility
reproducibility:
  seed: 42
  deterministic: true

general:
  log_level: "info"
  save_predictions: true
  save_attention_weights: false  # Set to true to save attention weights
