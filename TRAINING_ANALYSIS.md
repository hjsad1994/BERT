# Ph√¢n T√≠ch Training Log - T·∫°i Sao Accuracy Th·∫•p?

## üìä Training Metrics Summary

| Epoch | Val Accuracy | Val F1 | Val Loss | Best? |
|-------|--------------|--------|----------|-------|
| **1** | **93.35%** | **93.29%** | 0.1135 | ‚úÖ BEST |
| 2 | 90.34% | 90.48% | 0.1073 | ‚ùå (F1 gi·∫£m) |
| 3 | 91.49% | 91.54% | 0.1080 | ‚ùå |
| 4 | 91.68% | 91.68% | 0.1386 | ‚ùå (Loss tƒÉng) |

**Test Set Performance:**
- Accuracy: **91.36%**
- F1 Score: **91.33%**

---

## üîç V·∫•n ƒê·ªÅ Ch√≠nh

### 1. **Epoch 1 T·ªêT NH·∫§T nh∆∞ng Test Th·∫•p H∆°n** ‚ö†Ô∏è

```
Epoch 1 Validation: 93.35% accuracy, 93.29% F1 ‚Üê BEST
Test Set:          91.36% accuracy, 91.33% F1 ‚Üê 2% GAP!
```

**L√Ω do:**
- **Overfitting nh·∫π**: Model fit val set t·ªët h∆°n test set
- **Distribution shift**: Val v√† test c√≥ ph√¢n b·ªë h∆°i kh√°c
- **Luck factor**: Epoch 1 c√≥ th·ªÉ "may m·∫Øn" v·ªõi val set

---

### 2. **Model Gi·∫£m Performance Sau Epoch 1** üìâ

```
Epoch 1 ‚Üí Epoch 2: 93.29% ‚Üí 90.48% F1 (-2.81%)
Epoch 2 ‚Üí Epoch 3: 90.48% ‚Üí 91.54% F1 (+1.06%)
Epoch 3 ‚Üí Epoch 4: 91.54% ‚Üí 91.68% F1 (+0.14%)
```

**ƒêi·ªÅu n√†y cho th·∫•y:**
- ‚úÖ Model ƒë√£ h·ªôi t·ª• ·ªü epoch 1
- ‚ùå Training th√™m l√†m GI·∫¢M performance
- ‚ö†Ô∏è C√≥ th·ªÉ ƒëang **overfitting** ho·∫∑c **catastrophic forgetting**

---

### 3. **Training Loss vs Validation Loss** üìà

```
Epoch 1: train_loss ~ 0.15, eval_loss = 0.1135 (good gap)
Epoch 4: train_loss ~ 0.05, eval_loss = 0.1386 (bad gap!)
```

**Train loss gi·∫£m t·ª´ 0.15 ‚Üí 0.05**
**Val loss TƒÇNG t·ª´ 0.1135 ‚Üí 0.1386**

‚Üí **Clear sign of OVERFITTING!** üî¥

---

### 4. **Checkpoint ƒê∆∞·ª£c Ch·ªçn L√† ƒê√∫ng** ‚úÖ

```
Best checkpoint: checkpoint-9334-e1 (Epoch 1)
Loaded: epoch 1 model v·ªõi 93.29% val F1
Test:   91.36% accuracy
```

Model **ƒê√É LOAD ƒë√∫ng** checkpoint t·ªët nh·∫•t (epoch 1).
V·∫•n ƒë·ªÅ kh√¥ng ph·∫£i ·ªü vi·ªác ch·ªçn checkpoint!

---

## üéØ T·∫°i Sao Test Accuracy Th·∫•p H∆°n Val?

### Gap: 93.35% (val) ‚Üí 91.36% (test) = -1.99%

**L√Ω do 1: Val/Test Distribution Difference** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
```
Validation set c√≥ th·ªÉ "d·ªÖ h∆°n" test set
- √çt sample kh√≥
- Ph√¢n b·ªë aspect/sentiment kh√°c
- Random split kh√¥ng ho√†n h·∫£o
```

**L√Ω do 2: Overfitting to Validation Set** ‚≠ê‚≠ê‚≠ê
```
Model optimization d·ª±a tr√™n val set
‚Üí C√≥ th·ªÉ h·ªçc "quirks" c·ªßa val set
‚Üí Kh√¥ng generalize t·ªët cho test
```

**L√Ω do 3: Small Dataset Effect** ‚≠ê‚≠ê‚≠ê
```
Dataset ~20k samples
Val set ~1.5k, Test set ~1.5k
‚Üí High variance in metrics
‚Üí 1-2% gap l√† NORMAL
```

**L√Ω do 4: Batch Size 16 Overfitting** ‚≠ê‚≠ê‚≠ê
```
Batch 16 ‚Üí High gradient noise
C√≥ th·ªÉ converge qu√° nhanh ·ªü epoch 1
Sau ƒë√≥ b·∫Øt ƒë·∫ßu memorize training set
```

---

## üìâ Detailed Performance Breakdown

### Per Class Performance (Test Set)

| Class | Precision | Recall | F1 | Support |
|-------|-----------|--------|-----|---------|
| **Positive** | 92.01% | 92.14% | 92.08% | 700 |
| **Negative** | 91.38% | 93.54% | 92.45% | 635 |
| **Neutral** | 89.15% | 82.89% | 85.91% | 228 |

**Problem:** üî¥
- **Neutral class** c√≥ performance th·∫•p nh·∫•t
- Recall: 82.89% (worst!)
- F1: 85.91% (k√©o t·ªïng F1 xu·ªëng)

**Neutral class chi·∫øm 228/1563 = 14.6% test set**
‚Üí ·∫¢nh h∆∞·ªüng ƒë√°ng k·ªÉ ƒë·∫øn weighted F1!

---

## üî¨ Root Cause Analysis

### Hypothesis 1: Model Peaked at Epoch 1 ‚úÖ

**Evidence:**
- Epoch 1: Best val F1 (93.29%)
- Epoch 2-4: Degrading or stagnant
- Train loss keeps decreasing ‚Üí overfitting

**Conclusion:**
Model ƒë√£ t√¨m ƒë∆∞·ª£c optimal point ·ªü epoch 1.
Training th√™m ch·ªâ l√†m overfit.

---

### Hypothesis 2: Batch Size 16 Converged Too Fast ‚úÖ

**Evidence:**
- Batch 16 = 1,250 updates/epoch
- Very frequent updates
- Reached good performance in just 1 epoch

**Research backing:**
> "Small batch sizes converge faster but may overfit quicker"

---

### Hypothesis 3: Neutral Class Distribution Gap ‚úÖ

**Evidence:**
- Neutral: 82.89% recall (worst)
- Positive: 92.14% recall (best)
- Negative: 93.54% recall (excellent)

**Reason:**
- Neutral samples harder to classify
- May have different distribution in val vs test
- Less training samples (minority class)

---

## üí° Gi·∫£i Ph√°p

### Solution 1: **Increase Batch Size to 32** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

```yaml
per_device_train_batch_size: 32
gradient_accumulation_steps: 2
num_train_epochs: 3  # Reduce to 3 (enough!)
```

**Why:**
- Smoother convergence
- Less prone to quick overfitting
- Better generalization
- Still reasonable training time

**Expected:**
- Val F1: 92-93% (similar to now)
- Test F1: 92-92.5% (+1% improvement)
- Better val/test consistency

---

### Solution 2: **More Regularization** ‚≠ê‚≠ê‚≠ê‚≠ê

```yaml
weight_decay: 0.02           # Increase from 0.01
dropout: 0.15                # Add dropout
label_smoothing: 0.1         # Add label smoothing
```

**Why:**
- Prevent overfitting
- Better generalization
- Reduce val/test gap

---

### Solution 3: **Better Data Split** ‚≠ê‚≠ê‚≠ê

```python
# Stratified split by aspect AND sentiment
from sklearn.model_selection import train_test_split

# Ensure val/test have similar distribution
train, val, test = stratified_split(data, 
                                    stratify_by=['aspect', 'sentiment'])
```

**Why:**
- Ensure val and test have same distribution
- Reduce metric variance
- More reliable validation

---

### Solution 4: **Early Stopping Earlier** ‚≠ê‚≠ê‚≠ê

```yaml
early_stopping_patience: 1    # Stop after 1 epoch no improvement
num_train_epochs: 5           # Keep max at 5
```

**Why:**
- Model already peaked at epoch 1
- Training more = overfitting
- Save time and get better model

---

### Solution 5: **Ensemble or Average Checkpoints** ‚≠ê‚≠ê

```python
# Average weights from epoch 1, 3, 4
model_ensemble = average_checkpoints([
    'checkpoint-9334-e1',
    'checkpoint-9149-e3', 
    'checkpoint-9168-e4'
])
```

**Why:**
- Smoother predictions
- Better generalization
- Reduce variance

---

## üéØ Recommended Next Steps

### Priority 1: **Change Batch Size to 32** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

```bash
# Edit config.yaml
per_device_train_batch_size: 32
per_device_eval_batch_size: 64
gradient_accumulation_steps: 2
num_train_epochs: 3  # Reduce to 3

# Retrain
python train.py
```

**Expected improvement:** +0.5-1% test F1

---

### Priority 2: **Add Regularization**

```yaml
weight_decay: 0.02
warmup_ratio: 0.1  # More warmup
```

**Expected improvement:** +0.3-0.5% test F1

---

### Priority 3: **Focus on Neutral Class**

```python
# Add class weights for Focal Loss
class_weights = {
    'positive': 1.0,
    'negative': 1.0,
    'neutral': 1.5  # Higher weight for neutral
}
```

**Expected improvement:** +1-2% neutral F1

---

## üìä Current vs Expected Performance

| Metric | Current | With Changes | Improvement |
|--------|---------|--------------|-------------|
| **Val F1** | 93.29% | 92.5-93% | Slight decrease OK |
| **Test F1** | 91.33% | 92-93% | **+0.7-1.7%** ‚úÖ |
| **Val-Test Gap** | 1.99% | 0.5-1% | **Better consistency** ‚úÖ |
| **Training Time** | 6 min | 9-12 min | Acceptable |

---

## ‚úÖ Conclusion

### T·∫°i Sao Test Accuracy "Ch·ªâ" 91.36%?

1. ‚úÖ **Model ƒë√£ peak ·ªü epoch 1** (93.29% val F1)
2. ‚ùå **Batch 16 converge qu√° nhanh** ‚Üí overfit sau epoch 1
3. ‚ö†Ô∏è **Val/Test distribution kh√°c nhau** ‚Üí 2% gap
4. üî¥ **Neutral class y·∫øu** (82.89% recall)
5. ‚ùå **Training th√™m l√†m gi·∫£m performance** ‚Üí overfitting

### Accuracy 91.36% C√≥ T·ªët Kh√¥ng?

**G√≥c nh√¨n t√≠ch c·ª±c:**
- ‚úÖ 91.36% l√† **R·∫§T T·ªêT** cho ABSA task
- ‚úÖ State-of-the-art th∆∞·ªùng ~92-94%
- ‚úÖ Gap v·ªõi val ch·ªâ 2% (acceptable)

**G√≥c nh√¨n c·∫£i thi·ªán:**
- ‚ö†Ô∏è C√≥ th·ªÉ l√™n 92-93% v·ªõi batch size t·ªët h∆°n
- ‚ö†Ô∏è Neutral class c·∫ßn attention
- ‚ö†Ô∏è Model overfit sau epoch 1

---

## üöÄ Action Items

**Ngay l·∫≠p t·ª©c:**
1. ‚úÖ Ch·∫•p nh·∫≠n 91.36% (t·ªët r·ªìi!)
2. üîÑ Ho·∫∑c retrain v·ªõi batch 32 (improve th√™m 1%)

**N·∫øu mu·ªën improve:**
```bash
# 1. Edit config.yaml
per_device_train_batch_size: 32
num_train_epochs: 3

# 2. Retrain
python train.py

# Expected: 92-93% test F1
```

**Final verdict:**
üéâ **91.36% l√† performance T·ªêT!** C√≥ th·ªÉ improve th√™m 1% n·∫øu c·∫ßn.
