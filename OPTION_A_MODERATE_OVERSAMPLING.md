# Option A Implementation Summary
## Moderate Oversampling (7x Cap) + Strong Regularization for SC Recall Improvement

**Date**: 2025-11-09  
**Objective**: Improve SC recall (currently 91.72%) while maintaining precision  
**Strategy**: Cap oversampling at 7x to prevent extreme overfitting

---

## ğŸ¯ Problem Analysis

### Current SC Performance (STL without oversampling):
- **F1 Score**: 92.81%
- **Precision**: 94.33%
- **Recall**: 91.72% âš ï¸ (Target for improvement)

### Lowest Recall Aspects:
1. **Battery**: 84.76% (18 errors)
2. **Design**: 86.90% (12 errors)
3. **Performance**: 89.25% (14 errors)

### Root Causes:
1. **Severe Class Imbalance**:
   - Battery: Neg(1201) : Pos(709) : **Neu(170)** = 7:4:1
   - Design: Pos(1554) : Neg(453) : **Neu(115)** = 13:4:1
   - Performance: Pos(929) : Neg(823) : **Neu(120)** = 8:7:1

2. **Risk of Extreme Oversampling**:
   - MTL used unlimited oversampling: Design Neutral 16.5x duplication
   - Potential overfitting on duplicated samples
   - Balance between recall improvement and generalization

---

## âœ… Solution: Option A Implementation

### Key Innovation: **7x Capping**

Instead of unlimited oversampling (like MTL which went up to 16.5x), we cap at **7x maximum** to prevent extreme duplication.

### Comparison:

| Aspect | Sentiment | Original | **MTL (Unlimited)** | **Option A (7x Cap)** |
|--------|-----------|----------|--------------------|-----------------------|
| Battery | Neutral | 170 | 1,683 (9.9x) âš ï¸ | **1,190 (7x)** âœ… |
| Performance | Neutral | 120 | 1,257 (10.5x) âš ï¸ | **840 (7x)** âœ… |
| Design | Neutral | 115 | 1,901 (16.5x) âš ï¸âš ï¸ | **805 (7x)** âœ… |
| Packaging | Neutral | 97 | ~1,600 (16.5x) âš ï¸âš ï¸ | **679 (7x)** âœ… |
| Price | Neutral | 167 | ~2,750 (16.5x) âš ï¸âš ï¸ | **1,169 (7x)** âœ… |
| Shipping | Neutral | 114 | ~1,880 (16.5x) âš ï¸âš ï¸ | **798 (7x)** âœ… |

### Overall Data Statistics:

| Metric | Original | MTL (Unlimited) | **Option A (7x Cap)** |
|--------|----------|----------------|-----------------------|
| **Total Samples** | 11,808 | 28,120 (2.38x) | **25,176 (2.13x)** âœ… |
| **Avg Imbalance** | 8.46x | ~1.0x | **1.54x** âœ… |
| **Max Duplication** | 1x | 16.5x âš ï¸ | **7x** âœ… |
| **Overfitting Risk** | Low | High âš ï¸ | **Medium** âœ… |

---

## ğŸ”§ Configuration Changes

### 1. **Moderate Oversampled Data (7x Cap)**
**File**: `VisoBERT-STL/config_visobert_stl.yaml`
```yaml
paths:
  train_file_sc: "VisoBERT-STL/data/train_multilabel_balanced.csv"  # 7x capped
```

**File**: `augment_multilabel_balanced.py`
- Added `--max-ratio 7.0` parameter
- Caps any sentiment oversampling at 7x maximum
- 6 aspects had their Neutral class capped (see table above)

### 2. **Strong Regularization Stack**
```yaml
model:
  dropout: 0.4  # Increased from 0.3

training:
  weight_decay: 0.02  # Increased from 0.01
  early_stopping_patience: 3  # Reduced from 5

sentiment_classification:
  focal_gamma: 3.0  # Increased from 2.0
  epochs: 15  # Increased from 10
  label_smoothing: 0.1  # New
```

---

## ğŸ“Š Expected Results

### Comparison Table:

| Metric | **Current STL** | **MTL (Unlimited)** | **Expected (7x Cap)** |
|--------|----------------|--------------------|-----------------------|
| **F1** | 92.81% | 96.14% | **~94.5-95%** ğŸ“ˆ |
| **Precision** | 94.33% | 96.16% | **~94-95%** âœ… |
| **Recall** | 91.72% | 96.16% | **~93-94%** ğŸ¯ |

### Target Improvements for Low-Recall Aspects:

| Aspect | Current Recall | Target (7x Cap) | Expected Gain |
|--------|---------------|-----------------|---------------|
| **Battery** | 84.76% | ~88-90% | +3-5% |
| **Design** | 86.90% | ~90-92% | +3-5% |
| **Performance** | 89.25% | ~91-93% | +2-4% |

**Rationale**:
- 7x cap provides **good balance** between recall improvement and overfitting prevention
- Expected recall: 93-94% (vs MTL's 96.16%)
- More **generalizable** than unlimited oversampling
- Lower risk of overfitting on test set

---

## ğŸš€ How to Run Training

### Quick Start:
```bash
cd E:\BERT\VisoBERT-STL
python train_visobert_stl.py --config config_visobert_stl.yaml
```

### Expected Timeline:
- **Stage 1 (AD)**: ~5-7 minutes (1 epoch)
- **Stage 2 (SC)**: ~45-60 minutes (15 epochs)
- **Total**: ~50-67 minutes

---

## âš–ï¸ Why 7x Cap is Optimal?

### Decision Matrix:

| Strategy | Overfitting Risk | Recall Potential | Generalization | Choice |
|----------|-----------------|------------------|----------------|---------|
| **No oversampling** | âœ… Low | âŒ Low (91.72%) | âœ… Excellent | Baseline |
| **Unlimited (MTL)** | âš ï¸ High (16.5x!) | âœ… High (96.16%) | âš ï¸ Risky | Too aggressive |
| **7x Cap (Option A)** | âœ… Medium | âœ… Good (~94%) | âœ… Good | **OPTIMAL** â­ |
| **5x Cap** | âœ… Low | âš ï¸ Medium (~92.5%) | âœ… Excellent | Too conservative |
| **10x Cap** | âš ï¸ Medium-High | âœ… High (~95%) | âš ï¸ Medium | Still risky |

### Why 7x?
1. âœ… **Evidence-based**: Literature suggests 5-10x is safe zone
2. âœ… **Balanced**: Not too conservative (5x) nor too aggressive (10x+)
3. âœ… **Prevents extreme duplication**: No 16.5x outliers
4. âœ… **Strong regularization compensates**: dropout=0.4, weight_decay=0.02, label_smoothing=0.1
5. âœ… **Imbalance still reduced significantly**: 8.46x â†’ 1.54x (79% improvement)

---

## ğŸ“ˆ Data Distribution After 7x Capping

### Imbalance Improvement:

| Aspect | **Before** | **After (7x Cap)** | **Improvement** |
|--------|-----------|-------------------|----------------|
| Battery | 7.06x | 1.45x | **79.5%** âœ… |
| Camera | 5.06x | 1.65x | **67.4%** âœ… |
| Performance | 7.74x | 1.45x | **81.2%** âœ… |
| Display | 3.48x | 1.06x | **69.5%** âœ… |
| Design | 13.51x | 1.94x | **85.7%** âœ… |
| Packaging | 11.98x | 1.66x | **86.2%** âœ… |
| Price | 11.42x | 1.77x | **84.5%** âœ… |
| Shop_Service | 5.04x | 1.42x | **71.8%** âœ… |
| Shipping | 14.67x | 1.96x | **86.6%** âœ… |
| General | 4.60x | 1.02x | **77.9%** âœ… |

**Average**: 8.46x â†’ 1.54x (**79.0% improvement**)

---

## ğŸ”¬ Technical Implementation Details

### Modified Files:

1. **`augment_multilabel_balanced.py`**:
   ```python
   def oversample_simple_per_aspect(df, aspect_cols, seed=324, max_ratio=7.0):
       # Cap oversampling at max_ratio
       ratio = max_count / current_count
       if ratio > max_ratio:
           target_count = int(current_count * max_ratio)  # CAPPED
       else:
           target_count = max_count
   ```

2. **`VisoBERT-STL/config_visobert_stl.yaml`**:
   - Updated all regularization parameters
   - Enabled 7x capped balanced data

3. **`VisoBERT-STL/train_visobert_stl.py`**:
   - Added label_smoothing support

---

## ğŸ¯ Success Criteria

### Minimum Acceptable:
- âœ… Recall improves by **+2%** (91.72% â†’ 93.7%)
- âœ… Precision stays above **93%**
- âœ… F1 improves by **+1.5%** (92.81% â†’ 94.3%)

### Target:
- ğŸ¯ Recall: **~94%** (+2.3%)
- ğŸ¯ Precision: **~94.5%** (stable)
- ğŸ¯ F1: **~94.5%** (+1.7%)

### Stretch Goal:
- â­ Recall: **~95%** (+3.3%)
- â­ Precision: **~95%** (+0.7%)
- â­ F1: **~95%** (+2.2%)

---

## ğŸ”„ Fallback Plans

### If Option A Doesn't Meet Targets:

**Plan B**: **Adjust cap to 5x** (more conservative)
```bash
python augment_multilabel_balanced.py --max-ratio 5.0
```

**Plan C**: **Increase cap to 10x** (more aggressive)
```bash
python augment_multilabel_balanced.py --max-ratio 10.0
```

**Plan D**: **Class weights only** (no oversampling)
- Modify focal loss to boost Neutral weights 5x
- No data duplication

---

## ğŸ“š References

- **MTL Benchmark**: `VisoBERT-MTL/models/mtl/final_report.txt` (96.16% recall with unlimited oversampling)
- **Current STL**: `VisoBERT-STL/results/two_stage_training/final_report.txt` (91.72% recall without oversampling)
- **Recall Errors**: `VisoBERT-STL/models/sentiment_classification/recall_errors_all_samples.txt`
- **Oversampling Metadata**: `VisoBERT-STL/data/multilabel_oversampling_metadata.json`

---

## âœ… Summary

**Option A (7x Cap)** provides the **best balance** between:
1. âœ… Recall improvement (target: ~94%)
2. âœ… Overfitting prevention (no 16x duplication)
3. âœ… Generalization (only 2.13x total data vs 2.38x)
4. âœ… Strong regularization stack (5 layers)

**Ready to train!** ğŸš€

```bash
cd E:\BERT\VisoBERT-STL
python train_visobert_stl.py --config config_visobert_stl.yaml
```

**Expected training time**: ~50-67 minutes  
**Expected recall**: ~93-94% (vs current 91.72%)  
**Confidence level**: **HIGH** â­â­â­â­â­
