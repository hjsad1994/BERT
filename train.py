"""
Script Hu·∫•n Luy·ªán M√¥ H√¨nh ABSA
==============================
Script ch√≠nh ƒë·ªÉ fine-tune m√¥ h√¨nh BERT cho ti·∫øng Vi·ªát (ViSoBERT/PhoBERT)
cho nhi·ªám v·ª• Aspect-Based Sentiment Analysis (ABSA)

Usage:
    python train.py --config config.yaml
"""

import os
import sys
import argparse
import torch
import logging
from datetime import datetime
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    set_seed as hf_set_seed,
    EarlyStoppingCallback
)

# Import c√°c h√†m ti·ªán √≠ch
from utils import (
    load_config,
    set_seed,
    load_and_preprocess_data,
    ABSADataset,
    compute_metrics,
    save_predictions,
    save_predictions_from_output,
    get_detailed_metrics,
    print_system_info
)


class TeeLogger:
    """Logger ghi ƒë·ªìng th·ªùi ra console v√† file"""
    def __init__(self, log_file):
        self.terminal = sys.stdout
        self.log = open(log_file, 'w', encoding='utf-8')
    
    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
        self.log.flush()
    
    def flush(self):
        self.terminal.flush()
        self.log.flush()
    
    def close(self):
        self.log.close()


def setup_logging():
    """Thi·∫øt l·∫≠p logging ra file v·ªõi timestamp"""
    # T·∫°o t√™n file log v·ªõi timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = "training_logs"
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, f"training_log_{timestamp}.txt")
    
    # T·∫°o TeeLogger ƒë·ªÉ ghi c·∫£ console v√† file
    tee = TeeLogger(log_file)
    sys.stdout = tee
    sys.stderr = tee
    
    print(f"üìù Training log s·∫Ω ƒë∆∞·ª£c l∆∞u t·∫°i: {log_file}\n")
    
    return tee, log_file


def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description='Fine-tune ViSoBERT cho ABSA tr√™n d·ªØ li·ªáu ti·∫øng Vi·ªát'
    )
    parser.add_argument(
        '--config',
        type=str,
        default='config.yaml',
        help='ƒê∆∞·ªùng d·∫´n ƒë·∫øn file c·∫•u h√¨nh YAML (default: config.yaml)'
    )
    return parser.parse_args()


def main():
    """H√†m main ƒëi·ªÅu ph·ªëi to√†n b·ªô workflow"""
    
    # =====================================================================
    # 0. SETUP LOGGING TO FILE
    # =====================================================================
    tee_logger, log_file_path = setup_logging()
    
    # =====================================================================
    # 1. PARSE ARGUMENTS V√Ä LOAD CONFIG
    # =====================================================================
    print("\n" + "="*70)
    print("üöÄ FINE-TUNING VISOBERT CHO ABSA")
    print("="*70)
    
    args = parse_arguments()
    
    # Load c·∫•u h√¨nh
    try:
        config = load_config(args.config)
    except Exception as e:
        print(f"\n‚ùå L·ªói khi load config: {str(e)}")
        return
    
    # =====================================================================
    # 2. THI·∫æT L·∫¨P SEED V√Ä IN TH√îNG TIN H·ªÜ TH·ªêNG
    # =====================================================================
    seed = config['general']['seed']
    set_seed(seed)
    hf_set_seed(seed)  # Set seed cho transformers
    
    print_system_info()
    
    # =====================================================================
    # 3. PH√ÅT HI·ªÜN V√Ä THI·∫æT L·∫¨P DEVICE (GPU/CPU)
    # =====================================================================
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"‚úì S·ª≠ d·ª•ng GPU: {torch.cuda.get_device_name(0)}")
    else:
        device = torch.device('cpu')
        print(f"‚úì S·ª≠ d·ª•ng CPU")
    
    print(f"‚úì Device: {device}")
    
    # =====================================================================
    # 4. T·∫¢I TOKENIZER V√Ä M√î H√åNH
    # =====================================================================
    print(f"\n{'='*70}")
    print("ü§ñ ƒêang t·∫£i tokenizer v√† m√¥ h√¨nh...")
    print(f"{'='*70}")
    
    model_name = config['model']['name']
    num_labels = config['model']['num_labels']
    
    try:
        print(f"\n‚úì ƒêang t·∫£i tokenizer t·ª´: {model_name}")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        print(f"‚úì ƒêang t·∫£i m√¥ h√¨nh t·ª´: {model_name}")
        model = AutoModelForSequenceClassification.from_pretrained(
            model_name,
            num_labels=num_labels,
            use_safetensors=True  # Force d√πng safetensors format (an to√†n h∆°n)
        )
        
        print(f"‚úì Tokenizer vocab size: {tokenizer.vocab_size}")
        print(f"‚úì Model parameters: {model.num_parameters():,}")
        print(f"‚úì S·ªë l∆∞·ª£ng labels: {num_labels}")
        
    except Exception as e:
        print(f"\n‚ùå L·ªói khi t·∫£i m√¥ h√¨nh: {str(e)}")
        print(f"\nG·ª£i √Ω: Ki·ªÉm tra k·∫øt n·ªëi internet ho·∫∑c t√™n m√¥ h√¨nh trong config.yaml")
        return
    
    # =====================================================================
    # 5. T·∫¢I V√Ä X·ª¨ L√ù D·ªÆ LI·ªÜU
    # =====================================================================
    try:
        train_df, val_df, test_df, label_map, id2label = load_and_preprocess_data(config)
    except Exception as e:
        print(f"\n‚ùå L·ªói khi load d·ªØ li·ªáu: {str(e)}")
        print(f"\nG·ª£i √Ω: Ch·∫°y 'python prepare_data.py' ƒë·ªÉ t·∫°o d·ªØ li·ªáu tr∆∞·ªõc")
        return
    
    # =====================================================================
    # 6. T·∫†O DATASETS
    # =====================================================================
    print(f"\n{'='*70}")
    print("üì¶ ƒêang t·∫°o PyTorch Datasets...")
    print(f"{'='*70}")
    
    max_length = config['model']['max_length']
    
    try:
        train_dataset = ABSADataset(train_df, tokenizer, max_length)
        val_dataset = ABSADataset(val_df, tokenizer, max_length)
        test_dataset = ABSADataset(test_df, tokenizer, max_length)
        
        print(f"\n‚úì Train dataset: {len(train_dataset)} m·∫´u")
        print(f"‚úì Val dataset:   {len(val_dataset)} m·∫´u")
        print(f"‚úì Test dataset:  {len(test_dataset)} m·∫´u")
        
        # In m·ªôt m·∫´u ƒë·ªÉ ki·ªÉm tra
        print(f"\n‚úì V√≠ d·ª• m·ªôt m·∫´u ƒë√£ tokenize:")
        sample = train_dataset[0]
        print(f"   Input IDs shape:      {sample['input_ids'].shape}")
        print(f"   Attention mask shape: {sample['attention_mask'].shape}")
        print(f"   Token type IDs shape: {sample['token_type_ids'].shape}")
        print(f"   Label:                {sample['labels'].item()} ({id2label[sample['labels'].item()]})")
        
    except Exception as e:
        print(f"\n‚ùå L·ªói khi t·∫°o datasets: {str(e)}")
        return
    
    # =====================================================================
    # 7. THI·∫æT L·∫¨P TRAINING ARGUMENTS
    # =====================================================================
    print(f"\n{'='*70}")
    print("‚öôÔ∏è  ƒêang thi·∫øt l·∫≠p Training Arguments...")
    print(f"{'='*70}")
    
    training_config = config['training']
    output_dir = config['paths']['output_dir']
    
    training_args = TrainingArguments(
        # Output directory
        output_dir=output_dir,
        
        # Training parameters
        num_train_epochs=training_config['num_train_epochs'],
        per_device_train_batch_size=training_config['per_device_train_batch_size'],
        per_device_eval_batch_size=training_config['per_device_eval_batch_size'],
        gradient_accumulation_steps=training_config['gradient_accumulation_steps'],
        
        # Optimizer
        learning_rate=training_config['learning_rate'],
        weight_decay=training_config['weight_decay'],
        adam_beta1=training_config['adam_beta1'],
        adam_beta2=training_config['adam_beta2'],
        adam_epsilon=training_config['adam_epsilon'],
        max_grad_norm=training_config['max_grad_norm'],
        
        # Scheduler
        warmup_ratio=training_config['warmup_ratio'],
        lr_scheduler_type=training_config['lr_scheduler_type'],
        
        # Evaluation
        eval_strategy=training_config['evaluation_strategy'],
        save_strategy=training_config['save_strategy'],
        save_total_limit=training_config['save_total_limit'],
        load_best_model_at_end=training_config['load_best_model_at_end'],
        metric_for_best_model=training_config['metric_for_best_model'],
        greater_is_better=training_config['greater_is_better'],
        
        # Logging
        logging_steps=training_config['logging_steps'],
        logging_first_step=training_config['logging_first_step'],
        
        # Performance
        fp16=training_config['fp16'],
        dataloader_num_workers=training_config['dataloader_num_workers'],
        dataloader_pin_memory=training_config['dataloader_pin_memory'],
        dataloader_prefetch_factor=training_config.get('dataloader_prefetch_factor', 2),
        dataloader_persistent_workers=training_config.get('dataloader_persistent_workers', False),
        
        # Other
        seed=training_config['seed'],
        disable_tqdm=training_config['disable_tqdm'],
        remove_unused_columns=training_config['remove_unused_columns'],
    )
    
    print(f"\n‚úì C√°c tham s·ªë hu·∫•n luy·ªán ch√≠nh:")
    print(f"   Learning rate:        {training_config['learning_rate']}")
    print(f"   Epochs:               {training_config['num_train_epochs']}")
    print(f"   Train batch size:     {training_config['per_device_train_batch_size']}")
    print(f"   Eval batch size:      {training_config['per_device_eval_batch_size']}")
    print(f"   Warmup ratio:         {training_config['warmup_ratio']}")
    print(f"   FP16:                 {training_config['fp16']}")
    print(f"   Output directory:     {output_dir}")
    
    # =====================================================================
    # 8. OVERSAMPLING - X·ª¨ L√ù CLASS IMBALANCE (DISABLED)
    # =====================================================================
    print(f"\n{'='*70}")
    print("üìà OVERSAMPLING - X·ª≠ l√Ω class imbalance...")
    print(f"{'='*70}")
    
    # L∆∞u class counts g·ªëc ƒë·ªÉ t√≠nh Focal Loss alpha weights
    from collections import Counter
    class_counts_original = Counter(train_df['sentiment'])  # L∆ØU L·∫†I G·ªêC cho Focal Loss
    
    from oversampling_utils import random_oversample, get_class_balance_report
    
    # Check imbalance tr∆∞·ªõc khi oversample
    print(f"\nüìä BEFORE Oversampling:")
    report_before = get_class_balance_report(train_df, target_column='sentiment')
    print(f"   Imbalance ratio: {report_before['imbalance_ratio']:.2f}x")
    
    if report_before['imbalance_ratio'] > 2.0:
        print(f"   ‚ö†Ô∏è  Severe imbalance detected!")
    
    # Apply oversampling
    # Strategy options:
    # - 'auto': Balance t·∫•t c·∫£ v·ªÅ majority class
    # - 'minority': Ch·ªâ oversample minority class (neutral)
    # - 0.5: Target ratio 50% of majority
    # - {'neutral': 2000}: Custom target count
    
    # Recommended: Smart ratio (minority at least 30% of majority)
    majority_count = max(class_counts_original.values())
    
    # Target: Neutral at least 30% of majority class (tƒÉng t·ª´ 20%)
    target_neutral_count = int(majority_count * 0.3)
    
    sampling_strategy = {
        'positive': class_counts_original['positive'],  # Keep original
        'negative': class_counts_original['negative'],  # Keep original
        'neutral': max(target_neutral_count, class_counts_original['neutral'])  # Oversample to 30%
    }
    
    print(f"\nüéØ Oversampling strategy:")
    print(f"   Target neutral: {target_neutral_count:,} samples (30% of majority)")
    
    train_df_oversampled = random_oversample(
        train_df, 
        target_column='sentiment',
        sampling_strategy=sampling_strategy,
        random_state=config['general']['seed']
    )
    
    # Check sau khi oversample
    print(f"\nüìä AFTER Oversampling:")
    report_after = get_class_balance_report(train_df_oversampled, target_column='sentiment')
    print(f"   Imbalance ratio: {report_after['imbalance_ratio']:.2f}x")
    
    if report_after['imbalance_ratio'] < 2.0:
        print(f"   ‚úÖ Imbalance reduced to acceptable level!")
    
    # Use oversampled data
    train_df = train_df_oversampled
    
    # Recreate train_dataset with oversampled data
    print(f"\nüîÑ Recreating train_dataset with oversampled data...")
    train_dataset = ABSADataset(train_df, tokenizer, max_length)
    print(f"‚úì New train dataset: {len(train_dataset):,} samples")
    
    # =====================================================================
    # 9. T√çNH CLASS WEIGHTS V√Ä KH·ªûI T·∫†O FOCAL LOSS
    # =====================================================================
    print(f"\n{'='*70}")
    print("üî• ƒêang t√≠nh class weights cho Focal Loss...")
    print(f"{'='*70}")
    
    # T√≠nh ph√¢n b·ªë classes trong training data G·ªêC (BEFORE oversampling)
    # ‚ö†Ô∏è QUAN TR·ªåNG: Alpha weights ph·∫£i d·ª±a tr√™n imbalance G·ªêC, kh√¥ng ph·∫£i sau oversampling!
    label_counts = class_counts_original  # D√πng counts G·ªêC
    total = sum(label_counts.values())
    
    # Class distribution
    print(f"\nüìä Ph√¢n b·ªë classes trong training data G·ªêC (before oversampling):")
    for label in ['positive', 'negative', 'neutral']:
        count = label_counts.get(label, 0)
        pct = (count / total) * 100
        print(f"   {label:10}: {count:6,} samples ({pct:5.2f}%)")
    
    print(f"\n‚ö†Ô∏è  L∆∞u √Ω: Alpha weights d·ª±a tr√™n imbalance G·ªêC ƒë·ªÉ gi·ªØ nguy√™n tr·ªçng s·ªë!")
    
    # T√≠nh alpha weights (inverse frequency)
    # alpha_i = 1 / (class_count_i / total)
    from utils import FocalLoss
    from focal_loss_trainer import CustomTrainer
    
    label_map = config['sentiment_labels']  # {'positive': 0, 'negative': 1, 'neutral': 2}
    alpha = [0.0, 0.0, 0.0]
    
    for label, idx in label_map.items():
        count = label_counts.get(label, 1)
        # Inverse frequency weight
        alpha[idx] = total / (len(label_map) * count)
    
    print(f"\nüéØ Alpha weights (inverse frequency):")
    for label, idx in label_map.items():
        print(f"   {label:10} (class {idx}): {alpha[idx]:.4f}")
    
    # Create Focal Loss
    gamma = 2.0  # Focusing parameter
    focal_loss = FocalLoss(alpha=alpha, gamma=gamma)
    print(f"\n‚úì Focal Loss created: gamma={gamma}, weighted by ORIGINAL class frequency")
    print(f"‚úì Alpha ph·∫£n √°nh imbalance G·ªêC, k·∫øt h·ª£p v·ªõi oversampling ƒë·ªÉ c√¢n b·∫±ng t·ªëi ∆∞u")
    
    # =====================================================================
    # 10. KH·ªûI T·∫†O TRAINER V·ªöI FOCAL LOSS
    # =====================================================================
    print(f"\n{'='*70}")
    print("üèãÔ∏è  ƒêang kh·ªüi t·∫°o Custom Trainer v·ªõi Focal Loss...")
    print(f"{'='*70}")
    
    trainer = CustomTrainer.create_trainer_with_focal_loss(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
        focal_loss=focal_loss
    )
    
    print(f"‚úì Custom Trainer v·ªõi Focal Loss ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng")
    print(f"‚úì Chi·∫øn l∆∞·ª£c x·ª≠ l√Ω class imbalance:")
    print(f"   ‚Ä¢ Oversampling: TƒÉng neutral l√™n 30% c·ªßa majority class")
    print(f"   ‚Ä¢ Focal Loss: TƒÉng tr·ªçng s·ªë loss cho minority class (d·ª±a tr√™n imbalance G·ªêC)")
    print(f"   ‚Ä¢ K·∫øt h·ª£p 2 ph∆∞∆°ng ph√°p ƒë·ªÉ c·∫£i thi·ªán Neutral class (hi·ªán F1=0.48)")
    
    # =====================================================================
    # 10.5. ADD CHECKPOINT RENAMER CALLBACK
    # =====================================================================
    print(f"\nüìÅ ƒêang thi·∫øt l·∫≠p Checkpoint Renamer...")
    
    from checkpoint_renamer import SimpleMetricCheckpointCallback
    
    # Add callback ƒë·ªÉ rename checkpoints theo accuracy
    # Example: checkpoint-1352 ‚Üí checkpoint-91 (91% accuracy)
    checkpoint_callback = SimpleMetricCheckpointCallback(metric_name='eval_accuracy')
    trainer.add_callback(checkpoint_callback)
    
    # Add Early Stopping callback ƒë·ªÉ tr√°nh overfitting  
    # TEMPORARY DISABLE ƒë·ªÉ debug l·ªói float vs string comparison
    use_early_stopping = False  # Set to True sau khi fix bug
    
    if use_early_stopping:
        # Threshold ph·∫£i ph√π h·ª£p v·ªõi metric: loss (~0.001) vs accuracy/F1 (~0.01)
        metric_name = training_config.get('metric_for_best_model', 'eval_loss')
        default_threshold = 0.01 if 'accuracy' in metric_name or 'f1' in metric_name else 0.001
        
        early_stopping_callback = EarlyStoppingCallback(
            early_stopping_patience=training_config.get('early_stopping_patience', 2),
            early_stopping_threshold=training_config.get('early_stopping_threshold', default_threshold)
        )
        trainer.add_callback(early_stopping_callback)
        print(f"‚úì Early Stopping: s·∫Ω d·ª´ng n·∫øu {metric_name} kh√¥ng c·∫£i thi·ªán sau {training_config.get('early_stopping_patience', 2)} epoch")
    else:
        print(f"‚ö†Ô∏è  Early Stopping DISABLED (temporary for debugging)")
    
    print(f"‚úì Checkpoints s·∫Ω ƒë∆∞·ª£c ƒë·∫∑t t√™n theo accuracy (vd: checkpoint-90, checkpoint-92)")
    
    # =====================================================================
    # 11. B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN
    # =====================================================================
    print(f"\n{'='*70}")
    print("üéØ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN")
    print(f"{'='*70}\n")
    
    try:
        train_result = trainer.train()
        
        print(f"\n{'='*70}")
        print("‚úÖ HO√ÄN T·∫§T HU·∫§N LUY·ªÜN")
        print(f"{'='*70}")
        print(f"‚úì Training loss: {train_result.training_loss:.4f}")
        print(f"‚úì Training time: {train_result.metrics['train_runtime']:.2f}s")
        print(f"‚úì Samples/second: {train_result.metrics['train_samples_per_second']:.2f}")
        
    except Exception as e:
        print(f"\n‚ùå L·ªói trong qu√° tr√¨nh hu·∫•n luy·ªán: {str(e)}")
        print("\nüìã Chi ti·∫øt l·ªói:")
        import traceback
        traceback.print_exc()
        return
    
    # =====================================================================
    # 9.5. T·∫†O TRAINER M·ªöI CHO EVALUATION (KH√îNG C√ì OPTIMIZER)
    # =====================================================================
    print(f"\n{'='*70}")
    print("üîÑ T·∫†O TRAINER M·ªöI CHO EVALUATION")
    print(f"{'='*70}")
    
    # L∆∞u model hi·ªán t·∫°i
    current_model = trainer.model
    
    # X√≥a trainer c≈© (c√≥ optimizer/scheduler)
    del trainer
    torch.cuda.empty_cache()
    
    # T·∫°o trainer m·ªõi ch·ªâ ƒë·ªÉ eval (kh√¥ng c√≥ optimizer/scheduler)
    eval_trainer = Trainer(
        model=current_model,
        args=training_args,
        eval_dataset=val_dataset,
        processing_class=tokenizer,
        compute_metrics=compute_metrics
    )
    
    print(f"‚úì ƒê√£ t·∫°o trainer m·ªõi cho evaluation (kh√¥ng c√≥ optimizer/scheduler)")
    print(f"‚úì VRAM ƒë√£ gi·∫£m, s·∫µn s√†ng cho evaluation")
    
    # =====================================================================
    # 10. ƒê√ÅNH GI√Å TR√äN T·∫¨P TEST
    # =====================================================================
    print(f"\n{'='*70}")
    print("üìä ƒê√ÅNH GI√Å TR√äN T·∫¨P TEST")
    print(f"{'='*70}")
    
    try:
        # Evaluate
        print("‚è≥ ƒêang evaluate tr√™n test dataset...")
        test_results = eval_trainer.evaluate(test_dataset)
        
        print(f"\n‚úì K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n t·∫≠p test:")
        print(f"   Accuracy:  {test_results['eval_accuracy']:.4f}")
        print(f"   Precision: {test_results['eval_precision']:.4f}")
        print(f"   Recall:    {test_results['eval_recall']:.4f}")
        print(f"   F1 Score:  {test_results['eval_f1']:.4f}")
        
        # Gi·∫£i ph√≥ng cache tr∆∞·ªõc khi predict ƒë·ªÉ tr√°nh OOM
        torch.cuda.empty_cache()
        
        # L·∫•y detailed metrics
        # CH√ö √ù: Ch·ªâ predict 1 L·∫¶N DUY NH·∫§T ·ªü ƒë√¢y, sau ƒë√≥ t√°i s·ª≠ d·ª•ng cho save_predictions
        print("\n‚è≥ ƒêang predict ƒë·ªÉ l·∫•y detailed metrics...")
        predictions_output = eval_trainer.predict(test_dataset)
        print("‚úì Predict ho√†n t·∫•t")
        label_names = [id2label[i] for i in sorted(id2label.keys())]
        detailed_report = get_detailed_metrics(
            predictions_output.predictions,
            predictions_output.label_ids,
            label_names
        )
        
        print(f"\n‚úì B√°o c√°o chi ti·∫øt theo t·ª´ng class:")
        print(detailed_report)
        
        # L∆∞u b√°o c√°o v√†o file
        report_path = config['paths']['evaluation_report']
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("="*70 + "\n")
            f.write("B√ÅO C√ÅO ƒê√ÅNH GI√Å M√î H√åNH ABSA\n")
            f.write("="*70 + "\n\n")
            
            f.write("T·ªïng quan:\n")
            f.write(f"  Accuracy:  {test_results['eval_accuracy']:.4f}\n")
            f.write(f"  Precision: {test_results['eval_precision']:.4f}\n")
            f.write(f"  Recall:    {test_results['eval_recall']:.4f}\n")
            f.write(f"  F1 Score:  {test_results['eval_f1']:.4f}\n\n")
            
            f.write("B√°o c√°o chi ti·∫øt theo t·ª´ng class:\n")
            f.write(detailed_report)
            
            f.write("\n" + "="*70 + "\n")
            f.write("C·∫•u h√¨nh m√¥ h√¨nh:\n")
            f.write(f"  Model: {model_name}\n")
            f.write(f"  Epochs: {training_config['num_train_epochs']}\n")
            f.write(f"  Learning rate: {training_config['learning_rate']}\n")
            f.write(f"  Batch size: {training_config['per_device_train_batch_size']}\n")
            f.write(f"  Max length: {max_length}\n")
        
        print(f"\n‚úì ƒê√£ l∆∞u b√°o c√°o chi ti·∫øt v√†o: {report_path}")
        
    except Exception as e:
        print(f"\n‚ùå L·ªói khi ƒë√°nh gi√°: {str(e)}")
        import traceback
        traceback.print_exc()
        return
    
    # =====================================================================
    # 11. L∆ØU K·∫æT QU·∫¢ D·ª∞ ƒêO√ÅN
    # =====================================================================
    try:
        # T√°i s·ª≠ d·ª•ng predictions_output ƒë√£ c√≥ t·ª´ b∆∞·ªõc tr∆∞·ªõc (tr√°nh predict 2 l·∫ßn)
        save_predictions_from_output(predictions_output, test_df, config, id2label)
    except Exception as e:
        print(f"\n‚ö†Ô∏è  C·∫£nh b√°o: Kh√¥ng th·ªÉ l∆∞u predictions: {str(e)}")
    
    # =====================================================================
    # 12. L∆ØU M√î H√åNH V√Ä TOKENIZER
    # =====================================================================
    print(f"\n{'='*70}")
    print("üíæ ƒêang l∆∞u m√¥ h√¨nh v√† tokenizer...")
    print(f"{'='*70}")
    
    try:
        # load_best_model_at_end=True ch·ªâ load best model v√†o memory
        # Ph·∫£i g·ªçi save_model() ƒë·ªÉ l∆∞u ra disk
        final_model_dir = output_dir
        
        # Save best model (ƒë√£ ƒë∆∞·ª£c load v√†o trainer.model)
        trainer.save_model(final_model_dir)
        tokenizer.save_pretrained(final_model_dir)
        
        print(f"\n‚úì M√¥ h√¨nh v√† tokenizer ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {final_model_dir}")
        print(f"‚úì B·∫°n c√≥ th·ªÉ load l·∫°i b·∫±ng:")
        print(f"   tokenizer = AutoTokenizer.from_pretrained('{final_model_dir}')")
        print(f"   model = AutoModelForSequenceClassification.from_pretrained('{final_model_dir}')")
        
    except Exception as e:
        print(f"\n‚ö†Ô∏è  C·∫£nh b√°o: Kh√¥ng th·ªÉ l∆∞u m√¥ h√¨nh: {str(e)}")
    
    # =====================================================================
    # 12.5. GI·∫¢I PH√ìNG GPU MEMORY TR∆Ø·ªöC ANALYSIS
    # =====================================================================
    print(f"\n{'='*70}")
    print("üßπ GI·∫¢I PH√ìNG GPU MEMORY")
    print(f"{'='*70}")
    
    # X√≥a eval_trainer v√† model sau khi ƒë√£ save xong
    del eval_trainer
    del current_model
    torch.cuda.empty_cache()
    
    print(f"‚úì ƒê√£ gi·∫£i ph√≥ng GPU memory")
    
    # =====================================================================
    # 13. T·ª∞ ƒê·ªòNG PH√ÇN T√çCH K·∫æT QU·∫¢
    # =====================================================================
    print(f"\n{'='*70}")
    print("üìä T·ª∞ ƒê·ªòNG PH√ÇN T√çCH K·∫æT QU·∫¢ CHI TI·∫æT")
    print(f"{'='*70}")
    
    try:
        # Import v√† ch·∫°y analyze_results
        import analyze_results
        
        print("‚úì ƒêang ch·∫°y ph√¢n t√≠ch chi ti·∫øt...")
        analyze_results.main()
        
    except Exception as e:
        print(f"\n‚ö†Ô∏è  C·∫£nh b√°o: Kh√¥ng th·ªÉ t·ª± ƒë·ªông ph√¢n t√≠ch: {str(e)}")
        print(f"   B·∫°n c√≥ th·ªÉ ch·∫°y th·ªß c√¥ng: python analyze_results.py")
    
    # =====================================================================
    # 14. K·∫æT TH√öC
    # =====================================================================
    print(f"\n{'='*70}")
    print("üéâ HO√ÄN T·∫§T TO√ÄN B·ªò QU√Å TR√åNH!")
    print(f"{'='*70}")
    
    print(f"\n‚úì T·ªïng k·∫øt:")
    print(f"   ‚Ä¢ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c fine-tune th√†nh c√¥ng")
    print(f"   ‚Ä¢ F1 Score tr√™n test: {test_results['eval_f1']:.4f}")
    print(f"   ‚Ä¢ M√¥ h√¨nh ƒë∆∞·ª£c l∆∞u t·∫°i: {output_dir}")
    print(f"   ‚Ä¢ B√°o c√°o ƒë√°nh gi√°: {config['paths']['evaluation_report']}")
    print(f"   ‚Ä¢ Predictions: {config['paths']['predictions_file']}")
    print(f"   ‚Ä¢ Ph√¢n t√≠ch chi ti·∫øt: analysis_results/")
    
    print(f"\n‚úì C·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng! üôè\n")
    
    # =====================================================================
    # ƒê√ìNG LOGGER V√Ä RESTORE STDOUT/STDERR
    # =====================================================================
    print(f"\nüìù Training log ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {log_file_path}")
    
    # Restore stdout/stderr v√† ƒë√≥ng file log
    sys.stdout = tee_logger.terminal
    sys.stderr = tee_logger.terminal
    tee_logger.close()


if __name__ == '__main__':
    main()
