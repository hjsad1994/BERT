"""
Script Huáº¥n Luyá»‡n MÃ´ HÃ¬nh ABSA
==============================
Script chÃ­nh Ä‘á»ƒ fine-tune mÃ´ hÃ¬nh BERT cho tiáº¿ng Viá»‡t (ViSoBERT/PhoBERT)
cho nhiá»‡m vá»¥ Aspect-Based Sentiment Analysis (ABSA)

Usage:
    python train.py --config config.yaml
"""

import os
import sys
import argparse
import torch
import logging
from datetime import datetime
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    set_seed as hf_set_seed,
    EarlyStoppingCallback
)

# Import cÃ¡c hÃ m tiá»‡n Ã­ch
from utils import (
    load_config,
    set_seed,
    load_and_preprocess_data,
    ABSADataset,
    compute_metrics,
    save_predictions,
    save_predictions_from_output,
    get_detailed_metrics,
    print_system_info
)


class TeeLogger:
    """Logger ghi Ä‘á»“ng thá»i ra console vÃ  file"""
    def __init__(self, log_file):
        self.terminal = sys.stdout
        self.log = open(log_file, 'w', encoding='utf-8')
    
    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
        self.log.flush()
    
    def flush(self):
        self.terminal.flush()
        self.log.flush()
    
    def close(self):
        self.log.close()


def setup_logging():
    """Thiáº¿t láº­p logging ra file vá»›i timestamp"""
    # Táº¡o tÃªn file log vá»›i timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = "training_logs"
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, f"training_log_{timestamp}.txt")
    
    # Táº¡o TeeLogger Ä‘á»ƒ ghi cáº£ console vÃ  file
    tee = TeeLogger(log_file)
    sys.stdout = tee
    sys.stderr = tee
    
    print(f"ğŸ“ Training log sáº½ Ä‘Æ°á»£c lÆ°u táº¡i: {log_file}\n")
    
    return tee, log_file


def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description='Fine-tune ViSoBERT cho ABSA trÃªn dá»¯ liá»‡u tiáº¿ng Viá»‡t'
    )
    parser.add_argument(
        '--config',
        type=str,
        default='config.yaml',
        help='ÄÆ°á»ng dáº«n Ä‘áº¿n file cáº¥u hÃ¬nh YAML (default: config.yaml)'
    )
    return parser.parse_args()


def main():
    """HÃ m main Ä‘iá»u phá»‘i toÃ n bá»™ workflow"""
    
    # =====================================================================
    # 0. SETUP LOGGING TO FILE
    # =====================================================================
    tee_logger, log_file_path = setup_logging()
    
    # =====================================================================
    # 1. PARSE ARGUMENTS VÃ€ LOAD CONFIG
    # =====================================================================
    print("\n" + "="*70)
    print("ğŸš€ FINE-TUNING VISOBERT CHO ABSA")
    print("="*70)
    
    args = parse_arguments()
    
    # Load cáº¥u hÃ¬nh
    try:
        config = load_config(args.config)
    except Exception as e:
        print(f"\nâŒ Lá»—i khi load config: {str(e)}")
        return
    
    # =====================================================================
    # 2. THIáº¾T Láº¬P SEED VÃ€ IN THÃ”NG TIN Há»† THá»NG
    # =====================================================================
    seed = config['general']['seed']
    set_seed(seed)
    hf_set_seed(seed)  # Set seed cho transformers
    
    print_system_info()
    
    # =====================================================================
    # 3. PHÃT HIá»†N VÃ€ THIáº¾T Láº¬P DEVICE (GPU/CPU)
    # =====================================================================
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"âœ“ Sá»­ dá»¥ng GPU: {torch.cuda.get_device_name(0)}")
    else:
        device = torch.device('cpu')
        print(f"âœ“ Sá»­ dá»¥ng CPU")
    
    print(f"âœ“ Device: {device}")
    
    # =====================================================================
    # 4. Táº¢I TOKENIZER VÃ€ MÃ” HÃŒNH
    # =====================================================================
    print(f"\n{'='*70}")
    print("ğŸ¤– Äang táº£i tokenizer vÃ  mÃ´ hÃ¬nh...")
    print(f"{'='*70}")
    
    model_name = config['model']['name']
    num_labels = config['model']['num_labels']
    
    try:
        print(f"\nâœ“ Äang táº£i tokenizer tá»«: {model_name}")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        print(f"âœ“ Äang táº£i mÃ´ hÃ¬nh tá»«: {model_name}")
        model = AutoModelForSequenceClassification.from_pretrained(
            model_name,
            num_labels=num_labels,
            use_safetensors=True  # Force dÃ¹ng safetensors format (an toÃ n hÆ¡n)
        )
        
        print(f"âœ“ Tokenizer vocab size: {tokenizer.vocab_size}")
        print(f"âœ“ Model parameters: {model.num_parameters():,}")
        print(f"âœ“ Sá»‘ lÆ°á»£ng labels: {num_labels}")
        
    except Exception as e:
        print(f"\nâŒ Lá»—i khi táº£i mÃ´ hÃ¬nh: {str(e)}")
        print(f"\nGá»£i Ã½: Kiá»ƒm tra káº¿t ná»‘i internet hoáº·c tÃªn mÃ´ hÃ¬nh trong config.yaml")
        return
    
    # =====================================================================
    # 5. Táº¢I VÃ€ Xá»¬ LÃ Dá»® LIá»†U
    # =====================================================================
    try:
        train_df, val_df, test_df, label_map, id2label = load_and_preprocess_data(config)
    except Exception as e:
        print(f"\nâŒ Lá»—i khi load dá»¯ liá»‡u: {str(e)}")
        print(f"\nGá»£i Ã½: Cháº¡y 'python prepare_data.py' Ä‘á»ƒ táº¡o dá»¯ liá»‡u trÆ°á»›c")
        return
    
    # =====================================================================
    # 6. Táº O DATASETS
    # =====================================================================
    print(f"\n{'='*70}")
    print("ğŸ“¦ Äang táº¡o PyTorch Datasets...")
    print(f"{'='*70}")
    
    max_length = config['model']['max_length']
    
    try:
        train_dataset = ABSADataset(train_df, tokenizer, max_length)
        val_dataset = ABSADataset(val_df, tokenizer, max_length)
        test_dataset = ABSADataset(test_df, tokenizer, max_length)
        
        print(f"\nâœ“ Train dataset: {len(train_dataset)} máº«u")
        print(f"âœ“ Val dataset:   {len(val_dataset)} máº«u")
        print(f"âœ“ Test dataset:  {len(test_dataset)} máº«u")
        
        # In má»™t máº«u Ä‘á»ƒ kiá»ƒm tra
        print(f"\nâœ“ VÃ­ dá»¥ má»™t máº«u Ä‘Ã£ tokenize:")
        sample = train_dataset[0]
        print(f"   Input IDs shape:      {sample['input_ids'].shape}")
        print(f"   Attention mask shape: {sample['attention_mask'].shape}")
        print(f"   Token type IDs shape: {sample['token_type_ids'].shape}")
        print(f"   Label:                {sample['labels'].item()} ({id2label[sample['labels'].item()]})")
        
    except Exception as e:
        print(f"\nâŒ Lá»—i khi táº¡o datasets: {str(e)}")
        return
    
    # =====================================================================
    # 7. THIáº¾T Láº¬P TRAINING ARGUMENTS
    # =====================================================================
    print(f"\n{'='*70}")
    print("âš™ï¸  Äang thiáº¿t láº­p Training Arguments...")
    print(f"{'='*70}")
    
    training_config = config['training']
    output_dir = config['paths']['output_dir']
    
    training_args = TrainingArguments(
        # Output directory
        output_dir=output_dir,
        
        # Training parameters
        num_train_epochs=training_config['num_train_epochs'],
        per_device_train_batch_size=training_config['per_device_train_batch_size'],
        per_device_eval_batch_size=training_config['per_device_eval_batch_size'],
        gradient_accumulation_steps=training_config['gradient_accumulation_steps'],
        
        # Optimizer
        learning_rate=training_config['learning_rate'],
        weight_decay=training_config['weight_decay'],
        adam_beta1=training_config['adam_beta1'],
        adam_beta2=training_config['adam_beta2'],
        adam_epsilon=training_config['adam_epsilon'],
        max_grad_norm=training_config['max_grad_norm'],
        
        # Scheduler
        warmup_ratio=training_config['warmup_ratio'],
        lr_scheduler_type=training_config['lr_scheduler_type'],
        
        # Evaluation
        eval_strategy=training_config['evaluation_strategy'],
        save_strategy=training_config['save_strategy'],
        save_total_limit=training_config['save_total_limit'],
        load_best_model_at_end=training_config['load_best_model_at_end'],
        metric_for_best_model=training_config['metric_for_best_model'],
        greater_is_better=training_config['greater_is_better'],
        
        # Logging
        logging_steps=training_config['logging_steps'],
        logging_first_step=training_config['logging_first_step'],
        
        # Performance
        fp16=training_config['fp16'],
        dataloader_num_workers=training_config['dataloader_num_workers'],
        dataloader_pin_memory=training_config['dataloader_pin_memory'],
        dataloader_prefetch_factor=training_config.get('dataloader_prefetch_factor', 2),
        dataloader_persistent_workers=training_config.get('dataloader_persistent_workers', False),
        
        # Other
        seed=training_config['seed'],
        disable_tqdm=training_config['disable_tqdm'],
        remove_unused_columns=training_config['remove_unused_columns'],
    )
    
    print(f"\nâœ“ CÃ¡c tham sá»‘ huáº¥n luyá»‡n chÃ­nh:")
    print(f"   Learning rate:        {training_config['learning_rate']}")
    print(f"   Epochs:               {training_config['num_train_epochs']}")
    print(f"   Train batch size:     {training_config['per_device_train_batch_size']}")
    print(f"   Eval batch size:      {training_config['per_device_eval_batch_size']}")
    print(f"   Warmup ratio:         {training_config['warmup_ratio']}")
    print(f"   FP16:                 {training_config['fp16']}")
    print(f"   Output directory:     {output_dir}")
    
    # =====================================================================
    # 8. OVERSAMPLING - Xá»¬ LÃ CLASS IMBALANCE (DISABLED)
    # =====================================================================
    print(f"\n{'='*70}")
    print("ğŸ“ˆ OVERSAMPLING - Xá»­ lÃ½ class imbalance...")
    print(f"{'='*70}")
    
    # LÆ°u class counts gá»‘c Ä‘á»ƒ tÃ­nh Focal Loss alpha weights
    from collections import Counter
    class_counts_original = Counter(train_df['sentiment'])  # LÆ¯U Láº I Gá»C cho Focal Loss
    
    from oversampling_utils import random_oversample, get_class_balance_report
    
    # Check imbalance trÆ°á»›c khi oversample
    print(f"\nğŸ“Š BEFORE Oversampling:")
    report_before = get_class_balance_report(train_df, target_column='sentiment')
    print(f"   Imbalance ratio: {report_before['imbalance_ratio']:.2f}x")
    
    if report_before['imbalance_ratio'] > 2.0:
        print(f"   âš ï¸  Severe imbalance detected!")
    
    # Apply oversampling
    # Strategy options:
    # - 'auto': Balance táº¥t cáº£ vá» majority class
    # - 'minority': Chá»‰ oversample minority class (neutral)
    # - 0.5: Target ratio 50% of majority
    # - {'neutral': 2000}: Custom target count
    
    # Recommended: Smart ratio (minority at least 30% of majority)
    majority_count = max(class_counts_original.values())
    
    # Target: Neutral at least 30% of majority class (tÄƒng tá»« 20%)
    target_neutral_count = int(majority_count * 0.3)
    
    sampling_strategy = {
        'positive': class_counts_original['positive'],  # Keep original
        'negative': class_counts_original['negative'],  # Keep original
        'neutral': max(target_neutral_count, class_counts_original['neutral'])  # Oversample to 30%
    }
    
    print(f"\nğŸ¯ Oversampling strategy:")
    print(f"   Target neutral: {target_neutral_count:,} samples (30% of majority)")
    
    train_df_oversampled = random_oversample(
        train_df, 
        target_column='sentiment',
        sampling_strategy=sampling_strategy,
        random_state=config['general']['seed']
    )
    
    # Check sau khi oversample
    print(f"\nğŸ“Š AFTER Oversampling:")
    report_after = get_class_balance_report(train_df_oversampled, target_column='sentiment')
    print(f"   Imbalance ratio: {report_after['imbalance_ratio']:.2f}x")
    
    if report_after['imbalance_ratio'] < 2.0:
        print(f"   âœ… Imbalance reduced to acceptable level!")
    
    # Use oversampled data
    train_df = train_df_oversampled
    
    # Recreate train_dataset with oversampled data
    print(f"\nğŸ”„ Recreating train_dataset with oversampled data...")
    train_dataset = ABSADataset(train_df, tokenizer, max_length)
    print(f"âœ“ New train dataset: {len(train_dataset):,} samples")
    
    # =====================================================================
    # 9. TÃNH CLASS WEIGHTS VÃ€ KHá»I Táº O FOCAL LOSS
    # =====================================================================
    print(f"\n{'='*70}")
    print("ğŸ”¥ Äang tÃ­nh class weights cho Focal Loss...")
    print(f"{'='*70}")
    
    # TÃ­nh phÃ¢n bá»‘ classes trong training data Gá»C (BEFORE oversampling)
    # âš ï¸ QUAN TRá»ŒNG: Alpha weights pháº£i dá»±a trÃªn imbalance Gá»C, khÃ´ng pháº£i sau oversampling!
    label_counts = class_counts_original  # DÃ¹ng counts Gá»C
    total = sum(label_counts.values())
    
    # Class distribution
    print(f"\nğŸ“Š PhÃ¢n bá»‘ classes trong training data Gá»C (before oversampling):")
    for label in ['positive', 'negative', 'neutral']:
        count = label_counts.get(label, 0)
        pct = (count / total) * 100
        print(f"   {label:10}: {count:6,} samples ({pct:5.2f}%)")
    
    print(f"\nâš ï¸  LÆ°u Ã½: Alpha weights dá»±a trÃªn imbalance Gá»C Ä‘á»ƒ giá»¯ nguyÃªn trá»ng sá»‘!")
    
    # TÃ­nh alpha weights (inverse frequency)
    # alpha_i = 1 / (class_count_i / total)
    from utils import FocalLoss
    from focal_loss_trainer import CustomTrainer
    
    label_map = config['sentiment_labels']  # {'positive': 0, 'negative': 1, 'neutral': 2}
    alpha = [0.0, 0.0, 0.0]
    
    for label, idx in label_map.items():
        count = label_counts.get(label, 1)
        # Inverse frequency weight
        alpha[idx] = total / (len(label_map) * count)
    
    print(f"\nğŸ¯ Alpha weights (inverse frequency):")
    for label, idx in label_map.items():
        print(f"   {label:10} (class {idx}): {alpha[idx]:.4f}")
    
    # Create Focal Loss
    gamma = 2.0  # Focusing parameter
    focal_loss = FocalLoss(alpha=alpha, gamma=gamma)
    print(f"\nâœ“ Focal Loss created: gamma={gamma}, weighted by ORIGINAL class frequency")
    print(f"âœ“ Alpha pháº£n Ã¡nh imbalance Gá»C, káº¿t há»£p vá»›i oversampling Ä‘á»ƒ cÃ¢n báº±ng tá»‘i Æ°u")
    
    # =====================================================================
    # 10. KHá»I Táº O TRAINER Vá»šI FOCAL LOSS
    # =====================================================================
    print(f"\n{'='*70}")
    print("ğŸ‹ï¸  Äang khá»Ÿi táº¡o Custom Trainer vá»›i Focal Loss...")
    print(f"{'='*70}")
    
    trainer = CustomTrainer.create_trainer_with_focal_loss(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
        focal_loss=focal_loss
    )
    
    print(f"âœ“ Custom Trainer vá»›i Focal Loss Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o thÃ nh cÃ´ng")
    print(f"âœ“ Chiáº¿n lÆ°á»£c xá»­ lÃ½ class imbalance:")
    print(f"   â€¢ Oversampling: TÄƒng neutral lÃªn 30% cá»§a majority class")
    print(f"   â€¢ Focal Loss: TÄƒng trá»ng sá»‘ loss cho minority class (dá»±a trÃªn imbalance Gá»C)")
    print(f"   â€¢ Káº¿t há»£p 2 phÆ°Æ¡ng phÃ¡p Ä‘á»ƒ cáº£i thiá»‡n Neutral class (hiá»‡n F1=0.48)")
    
    # =====================================================================
    # 10.5. ADD CHECKPOINT RENAMER CALLBACK
    # =====================================================================
    print(f"\nğŸ“ Äang thiáº¿t láº­p Checkpoint Renamer...")
    
    from checkpoint_renamer import SimpleMetricCheckpointCallback
    
    # Add callback Ä‘á»ƒ rename checkpoints theo accuracy
    # Example: checkpoint-1352 â†’ checkpoint-91 (91% accuracy)
    checkpoint_callback = SimpleMetricCheckpointCallback(metric_name='eval_accuracy')
    trainer.add_callback(checkpoint_callback)
    
    # Add Early Stopping callback Ä‘á»ƒ trÃ¡nh overfitting  
    # TEMPORARY DISABLE Ä‘á»ƒ debug lá»—i float vs string comparison
    use_early_stopping = False  # Set to True sau khi fix bug
    
    if use_early_stopping:
        # Threshold pháº£i phÃ¹ há»£p vá»›i metric: loss (~0.001) vs accuracy/F1 (~0.01)
        metric_name = training_config.get('metric_for_best_model', 'eval_loss')
        default_threshold = 0.01 if 'accuracy' in metric_name or 'f1' in metric_name else 0.001
        
        early_stopping_callback = EarlyStoppingCallback(
            early_stopping_patience=training_config.get('early_stopping_patience', 2),
            early_stopping_threshold=training_config.get('early_stopping_threshold', default_threshold)
        )
        trainer.add_callback(early_stopping_callback)
        print(f"âœ“ Early Stopping: sáº½ dá»«ng náº¿u {metric_name} khÃ´ng cáº£i thiá»‡n sau {training_config.get('early_stopping_patience', 2)} epoch")
    else:
        print(f"âš ï¸  Early Stopping DISABLED (temporary for debugging)")
    
    print(f"âœ“ Checkpoints sáº½ Ä‘Æ°á»£c Ä‘áº·t tÃªn theo accuracy (vd: checkpoint-90, checkpoint-92)")
    
    # =====================================================================
    # 11. Báº®T Äáº¦U HUáº¤N LUYá»†N
    # =====================================================================
    print(f"\n{'='*70}")
    print("ğŸ¯ Báº®T Äáº¦U HUáº¤N LUYá»†N")
    print(f"{'='*70}\n")
    
    try:
        train_result = trainer.train()
        
        print(f"\n{'='*70}")
        print("âœ… HOÃ€N Táº¤T HUáº¤N LUYá»†N")
        print(f"{'='*70}")
        print(f"âœ“ Training loss: {train_result.training_loss:.4f}")
        print(f"âœ“ Training time: {train_result.metrics['train_runtime']:.2f}s")
        print(f"âœ“ Samples/second: {train_result.metrics['train_samples_per_second']:.2f}")
        
    except Exception as e:
        print(f"\nâŒ Lá»—i trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n: {str(e)}")
        print("\nğŸ“‹ Chi tiáº¿t lá»—i:")
        import traceback
        traceback.print_exc()
        return
    
    # =====================================================================
    # 9.5. Táº O TRAINER Má»šI CHO EVALUATION (KHÃ”NG CÃ“ OPTIMIZER)
    # =====================================================================
    print(f"\n{'='*70}")
    print("ğŸ”„ Táº O TRAINER Má»šI CHO EVALUATION")
    print(f"{'='*70}")
    
    # LÆ°u model hiá»‡n táº¡i
    current_model = trainer.model
    
    # XÃ³a trainer cÅ© (cÃ³ optimizer/scheduler)
    del trainer
    torch.cuda.empty_cache()
    
    # Táº¡o trainer má»›i chá»‰ Ä‘á»ƒ eval (khÃ´ng cÃ³ optimizer/scheduler)
    eval_trainer = Trainer(
        model=current_model,
        args=training_args,
        eval_dataset=val_dataset,
        processing_class=tokenizer,
        compute_metrics=compute_metrics
    )
    
    print(f"âœ“ ÄÃ£ táº¡o trainer má»›i cho evaluation (khÃ´ng cÃ³ optimizer/scheduler)")
    print(f"âœ“ VRAM Ä‘Ã£ giáº£m, sáºµn sÃ ng cho evaluation")
    
    # =====================================================================
    # 10. ÄÃNH GIÃ TRÃŠN Táº¬P TEST
    # =====================================================================
    print(f"\n{'='*70}")
    print("ğŸ“Š ÄÃNH GIÃ TRÃŠN Táº¬P TEST")
    print(f"{'='*70}")
    
    try:
        # Evaluate
        print("â³ Äang evaluate trÃªn test dataset...")
        test_results = eval_trainer.evaluate(test_dataset)
        
        print(f"\nâœ“ Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn táº­p test:")
        print(f"   Accuracy:  {test_results['eval_accuracy']:.4f}")
        print(f"   Precision: {test_results['eval_precision']:.4f}")
        print(f"   Recall:    {test_results['eval_recall']:.4f}")
        print(f"   F1 Score:  {test_results['eval_f1']:.4f}")
        
        # Giáº£i phÃ³ng cache trÆ°á»›c khi predict Ä‘á»ƒ trÃ¡nh OOM
        torch.cuda.empty_cache()
        
        # Láº¥y detailed metrics
        # CHÃš Ã: Chá»‰ predict 1 Láº¦N DUY NHáº¤T á»Ÿ Ä‘Ã¢y, sau Ä‘Ã³ tÃ¡i sá»­ dá»¥ng cho save_predictions
        print("\nâ³ Äang predict Ä‘á»ƒ láº¥y detailed metrics...")
        predictions_output = eval_trainer.predict(test_dataset)
        print("âœ“ Predict hoÃ n táº¥t")
        label_names = [id2label[i] for i in sorted(id2label.keys())]
        detailed_report = get_detailed_metrics(
            predictions_output.predictions,
            predictions_output.label_ids,
            label_names
        )
        
        print(f"\nâœ“ BÃ¡o cÃ¡o chi tiáº¿t theo tá»«ng class:")
        print(detailed_report)
        
        # LÆ°u bÃ¡o cÃ¡o vÃ o file
        report_path = config['paths']['evaluation_report']
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("="*70 + "\n")
            f.write("BÃO CÃO ÄÃNH GIÃ MÃ” HÃŒNH ABSA\n")
            f.write("="*70 + "\n\n")
            
            f.write("Tá»•ng quan:\n")
            f.write(f"  Accuracy:  {test_results['eval_accuracy']:.4f}\n")
            f.write(f"  Precision: {test_results['eval_precision']:.4f}\n")
            f.write(f"  Recall:    {test_results['eval_recall']:.4f}\n")
            f.write(f"  F1 Score:  {test_results['eval_f1']:.4f}\n\n")
            
            f.write("BÃ¡o cÃ¡o chi tiáº¿t theo tá»«ng class:\n")
            f.write(detailed_report)
            
            f.write("\n" + "="*70 + "\n")
            f.write("Cáº¥u hÃ¬nh mÃ´ hÃ¬nh:\n")
            f.write(f"  Model: {model_name}\n")
            f.write(f"  Epochs: {training_config['num_train_epochs']}\n")
            f.write(f"  Learning rate: {training_config['learning_rate']}\n")
            f.write(f"  Batch size: {training_config['per_device_train_batch_size']}\n")
            f.write(f"  Max length: {max_length}\n")
        
        print(f"\nâœ“ ÄÃ£ lÆ°u bÃ¡o cÃ¡o chi tiáº¿t vÃ o: {report_path}")
        
    except Exception as e:
        print(f"\nâŒ Lá»—i khi Ä‘Ã¡nh giÃ¡: {str(e)}")
        import traceback
        traceback.print_exc()
        return
    
    # =====================================================================
    # 11. LÆ¯U Káº¾T QUáº¢ Dá»° ÄOÃN
    # =====================================================================
    try:
        # TÃ¡i sá»­ dá»¥ng predictions_output Ä‘Ã£ cÃ³ tá»« bÆ°á»›c trÆ°á»›c (trÃ¡nh predict 2 láº§n)
        save_predictions_from_output(predictions_output, test_df, config, id2label)
    except Exception as e:
        print(f"\nâš ï¸  Cáº£nh bÃ¡o: KhÃ´ng thá»ƒ lÆ°u predictions: {str(e)}")
    
    # =====================================================================
    # 12. LÆ¯U MÃ” HÃŒNH VÃ€ TOKENIZER
    # =====================================================================
    print(f"\n{'='*70}")
    print("ğŸ’¾ Äang lÆ°u mÃ´ hÃ¬nh vÃ  tokenizer...")
    print(f"{'='*70}")
    
    try:
        # load_best_model_at_end=True chá»‰ load best model vÃ o memory
        # Pháº£i gá»i save_model() Ä‘á»ƒ lÆ°u ra disk
        final_model_dir = output_dir
        
        # Save best model (Ä‘Ã£ Ä‘Æ°á»£c load vÃ o trainer.model)
        trainer.save_model(final_model_dir)
        tokenizer.save_pretrained(final_model_dir)
        
        print(f"\nâœ“ MÃ´ hÃ¬nh vÃ  tokenizer Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {final_model_dir}")
        print(f"âœ“ Báº¡n cÃ³ thá»ƒ load láº¡i báº±ng:")
        print(f"   tokenizer = AutoTokenizer.from_pretrained('{final_model_dir}')")
        print(f"   model = AutoModelForSequenceClassification.from_pretrained('{final_model_dir}')")
        
    except Exception as e:
        print(f"\nâš ï¸  Cáº£nh bÃ¡o: KhÃ´ng thá»ƒ lÆ°u mÃ´ hÃ¬nh: {str(e)}")
    
    # =====================================================================
    # 12.5. GIáº¢I PHÃ“NG GPU MEMORY TRÆ¯á»šC ANALYSIS
    # =====================================================================
    print(f"\n{'='*70}")
    print("ğŸ§¹ GIáº¢I PHÃ“NG GPU MEMORY")
    print(f"{'='*70}")
    
    # XÃ³a eval_trainer vÃ  model sau khi Ä‘Ã£ save xong
    del eval_trainer
    del current_model
    torch.cuda.empty_cache()
    
    print(f"âœ“ ÄÃ£ giáº£i phÃ³ng GPU memory")
    
    # =====================================================================
    # 13. Tá»° Äá»˜NG PHÃ‚N TÃCH Káº¾T QUáº¢
    # =====================================================================
    print(f"\n{'='*70}")
    print("ğŸ“Š Tá»° Äá»˜NG PHÃ‚N TÃCH Káº¾T QUáº¢ CHI TIáº¾T")
    print(f"{'='*70}")
    
    try:
        # Import vÃ  cháº¡y analyze_results
        import analyze_results
        
        print("âœ“ Äang cháº¡y phÃ¢n tÃ­ch chi tiáº¿t...")
        analyze_results.main()
        
    except Exception as e:
        print(f"\nâš ï¸  Cáº£nh bÃ¡o: KhÃ´ng thá»ƒ tá»± Ä‘á»™ng phÃ¢n tÃ­ch: {str(e)}")
        print(f"   Báº¡n cÃ³ thá»ƒ cháº¡y thá»§ cÃ´ng: python analyze_results.py")
    
    # =====================================================================
    # 14. Káº¾T THÃšC
    # =====================================================================
    print(f"\n{'='*70}")
    print("ğŸ‰ HOÃ€N Táº¤T TOÃ€N Bá»˜ QUÃ TRÃŒNH!")
    print(f"{'='*70}")
    
    print(f"\nâœ“ Tá»•ng káº¿t:")
    print(f"   â€¢ MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c fine-tune thÃ nh cÃ´ng")
    print(f"   â€¢ F1 Score trÃªn test: {test_results['eval_f1']:.4f}")
    print(f"   â€¢ MÃ´ hÃ¬nh Ä‘Æ°á»£c lÆ°u táº¡i: {output_dir}")
    print(f"   â€¢ BÃ¡o cÃ¡o Ä‘Ã¡nh giÃ¡: {config['paths']['evaluation_report']}")
    print(f"   â€¢ Predictions: {config['paths']['predictions_file']}")
    print(f"   â€¢ PhÃ¢n tÃ­ch chi tiáº¿t: analysis_results/")
    
    print(f"\nâœ“ Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng! ğŸ™\n")
    
    # =====================================================================
    # ÄÃ“NG LOGGER VÃ€ RESTORE STDOUT/STDERR
    # =====================================================================
    print(f"\nğŸ“ Training log Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {log_file_path}")
    
    # Restore stdout/stderr vÃ  Ä‘Ã³ng file log
    sys.stdout = tee_logger.terminal
    sys.stderr = tee_logger.terminal
    tee_logger.close()


if __name__ == '__main__':
    main()
